{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":45867,"databundleVersionId":6924515,"sourceType":"competition"},{"sourceId":6755371,"sourceType":"datasetVersion","datasetId":3888697},{"sourceId":6917177,"sourceType":"datasetVersion","datasetId":3889865},{"sourceId":6984590,"sourceType":"datasetVersion","datasetId":4014175},{"sourceId":7018060,"sourceType":"datasetVersion","datasetId":4035314},{"sourceId":7029230,"sourceType":"datasetVersion","datasetId":4027203},{"sourceId":7156628,"sourceType":"datasetVersion","datasetId":4133077},{"sourceId":153993382,"sourceType":"kernelVersion"}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction \n\n**This is a basic CNN Model training notebook**\n\nIt is based on: \n- Thumbnail images\n- Basic data transformation (using Albumentation):\n    - resizing images to 512x512\n    - normalizing pixel values\n- CNN Architecture\n\n\n**Todos:**\n\n- Learn about Dataset & DataLoader\n- add augmentations (albumentation)\n- gem pooling","metadata":{}},{"cell_type":"code","source":"\n\nimport sys\nsys.path.append('/kaggle/input/dummypy')\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T14:59:03.859609Z","iopub.execute_input":"2023-12-11T14:59:03.859916Z","iopub.status.idle":"2023-12-11T14:59:03.887475Z","shell.execute_reply.started":"2023-12-11T14:59:03.859891Z","shell.execute_reply":"2023-12-11T14:59:03.886648Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from main import print_hello_world","metadata":{"execution":{"iopub.status.busy":"2023-12-11T14:59:39.526379Z","iopub.execute_input":"2023-12-11T14:59:39.526870Z","iopub.status.idle":"2023-12-11T14:59:39.531092Z","shell.execute_reply.started":"2023-12-11T14:59:39.526847Z","shell.execute_reply":"2023-12-11T14:59:39.530260Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"print_hello_world()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T14:59:46.828257Z","iopub.execute_input":"2023-12-11T14:59:46.828643Z","iopub.status.idle":"2023-12-11T14:59:46.833682Z","shell.execute_reply.started":"2023-12-11T14:59:46.828617Z","shell.execute_reply":"2023-12-11T14:59:46.832360Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"hello world!\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --quiet torch_optimizer\nimport torch_optimizer as torch_optimizer\n","metadata":{"execution":{"iopub.status.busy":"2023-12-09T16:25:13.747841Z","iopub.execute_input":"2023-12-09T16:25:13.749025Z","iopub.status.idle":"2023-12-09T16:25:25.635245Z","shell.execute_reply.started":"2023-12-09T16:25:13.748978Z","shell.execute_reply":"2023-12-09T16:25:25.633632Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install --quiet mlflow dagshub\nimport mlflow.pytorch \nfrom mlflow import MlflowClient\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:34:21.370204Z","iopub.execute_input":"2023-12-11T11:34:21.370648Z","iopub.status.idle":"2023-12-11T11:34:44.870180Z","shell.execute_reply.started":"2023-12-11T11:34:21.370613Z","shell.execute_reply":"2023-12-11T11:34:44.868885Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nydata-profiling 4.3.1 requires dacite>=1.8, but you have dacite 1.6.0 which is incompatible.\nydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"\n\nimport os\nimport gc\nimport cv2\nimport datetime\nimport math\nimport copy\nimport time\nimport random\nimport glob\nfrom matplotlib import pyplot as plt\nfrom skimage import io\n\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.cuda import amp\nimport torchvision\n\nimport optuna\nfrom optuna.trial import TrialState\n\n# Utils\nimport joblib\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\n\nfrom PIL import Image\nfrom joblib import Parallel, delayed\nfrom tqdm.auto import tqdm\n\n# Sklearn Imports\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import balanced_accuracy_score, confusion_matrix, f1_score\nfrom torch.utils.data.sampler import WeightedRandomSampler\n\n# For Image Models\nimport timm\n\nimport dagshub\nfrom getpass import getpass\n\n# Albumentations for augmentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\nb_ = Fore.BLUE\nsr_ = Style.RESET_ALL\n\nimport warnings\n# warnings.filterwarnings(\"ignore\")\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:34:44.872794Z","iopub.execute_input":"2023-12-11T11:34:44.873481Z","iopub.status.idle":"2023-12-11T11:35:03.987540Z","shell.execute_reply.started":"2023-12-11T11:34:44.873434Z","shell.execute_reply":"2023-12-11T11:35:03.986164Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from cancer_utils_tiles import get_class_weights,UBCModel, get_optimizer, fetch_scheduler, EarlyStopping, print_logged_info, get_or_create_experiment_id\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:35:03.989590Z","iopub.execute_input":"2023-12-11T11:35:03.990584Z","iopub.status.idle":"2023-12-11T11:35:16.934130Z","shell.execute_reply.started":"2023-12-11T11:35:03.990537Z","shell.execute_reply":"2023-12-11T11:35:16.932478Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"os.environ[\"MLFLOW_TRACKING_USERNAME\"]=\"Niggl0n\"\nos.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"7a3590e8c5558d4598dacc7810befa70a4baac9e\"\nos.environ['MLFLOW_TRACKING_PROJECTNAME'] = \"UBC_Cancer_Classification\"\n#dagshub.auth.add_app_token(\"7a3590e8c5558d4598dacc7810befa70a4baac9e\")\nmlflow.set_tracking_uri(f'https://dagshub.com/' + os.environ['MLFLOW_TRACKING_USERNAME'] + '/' + os.environ['MLFLOW_TRACKING_PROJECTNAME'] + '.mlflow')","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:35:16.936786Z","iopub.execute_input":"2023-12-11T11:35:16.937160Z","iopub.status.idle":"2023-12-11T11:35:16.944217Z","shell.execute_reply.started":"2023-12-11T11:35:16.937130Z","shell.execute_reply":"2023-12-11T11:35:16.942967Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"CONFIG = {\n    \"is_submission\": False,\n    \"weighted_loss\": True,\n    \"datetime_now\": datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"), \n    \"n_fold\":5, \n    \"test_fold\": 0,\n    \"seed\": 42,\n    \"img_size\": 512,\n    \"model_name\": \"tf_efficientnet_b0_ns\",   # \"tf_efficientnet_b0_ns\", # \"tf_efficientnetv2_s_in21ft1k\"\n    \"checkpoint_path\": \"/kaggle/input/tf-efficientnet-b0-aa-827b6e33-pth/tf_efficientnet_b0_aa-827b6e33.pth\",\n    \"num_classes\": 5,\n    \"train_batch_size\": 8,\n    \"valid_batch_size\": 8,\n    \"n_tiles\": 10,\n    \"n_tiles_test\": 10,\n    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n    \"num_epochs\": 15,\n    \"early_stopping\": True,\n    \"patience\": 6,\n    \"optimizer\": 'adam',\n    \"scheduler\": 'CosineAnnealingLR',\n    \"min_lr\": 1e-6,\n    \"T_max\": 10,\n    \"momentum\": 0.9,\n    \"weight_decay\": 1e-4,\n}","metadata":{"execution":{"iopub.status.busy":"2023-12-09T16:26:10.703814Z","iopub.execute_input":"2023-12-09T16:26:10.704126Z","iopub.status.idle":"2023-12-09T16:26:10.723231Z","shell.execute_reply.started":"2023-12-09T16:26:10.704100Z","shell.execute_reply":"2023-12-09T16:26:10.722068Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## 1. Data Preparation","metadata":{}},{"cell_type":"code","source":"ROOT_DIR = '/kaggle/input/UBC-OCEAN'\nTRAIN_DIR = '/kaggle/input/tiles-of-cancer-2048px-scale-0-25'\nINFER_DIR = '/kaggle/input/ubc-ocean-tiles-w-masks-2048px-scale-0-25'\ndf_orig = pd.read_csv(\"/kaggle/input/UBC-OCEAN/train.csv\")\ndf_orig = df_orig.rename(columns={\"label\":\"subtype\"})\ndf_train = pd.read_csv(\"/kaggle/input/df-tiles-025x-cancer-tissue-binary-labels/tiles_labelled_binary_cancer.csv\", index_col=\"Unnamed: 0\")\ndisplay(df_orig.sample(5))\ndisplay(df_train.sample(5))","metadata":{"execution":{"iopub.status.busy":"2023-12-09T16:26:10.724165Z","iopub.execute_input":"2023-12-09T16:26:10.724519Z","iopub.status.idle":"2023-12-09T16:26:10.967468Z","shell.execute_reply.started":"2023-12-09T16:26:10.724491Z","shell.execute_reply":"2023-12-09T16:26:10.966488Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"     image_id subtype  image_width  image_height  is_tma\n331     39208      MC        61530         28220   False\n58       6359      CC        39280         37289   False\n317     38349      CC        45540         42755   False\n463     54949    HGSC        37903         28174   False\n426     51032      EC        91965         40838   False","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>subtype</th>\n      <th>image_width</th>\n      <th>image_height</th>\n      <th>is_tma</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>331</th>\n      <td>39208</td>\n      <td>MC</td>\n      <td>61530</td>\n      <td>28220</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>6359</td>\n      <td>CC</td>\n      <td>39280</td>\n      <td>37289</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>317</th>\n      <td>38349</td>\n      <td>CC</td>\n      <td>45540</td>\n      <td>42755</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>463</th>\n      <td>54949</td>\n      <td>HGSC</td>\n      <td>37903</td>\n      <td>28174</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>426</th>\n      <td>51032</td>\n      <td>EC</td>\n      <td>91965</td>\n      <td>40838</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"                                              image_path  \\\n42402  /kaggle/input/ubc-ocean-tiles-w-masks-2048px-s...   \n30867  /kaggle/input/ubc-ocean-tiles-w-masks-2048px-s...   \n12840  /kaggle/input/ubc-ocean-tiles-w-masks-2048px-s...   \n21100  /kaggle/input/ubc-ocean-tiles-w-masks-2048px-s...   \n25482  /kaggle/input/ubc-ocean-tiles-w-masks-2048px-s...   \n\n                                               mask_path  cancer_ratio  \\\n42402  /kaggle/input/ubc-ocean-tiles-w-masks-2048px-s...      0.000000   \n30867  /kaggle/input/ubc-ocean-tiles-w-masks-2048px-s...      0.000000   \n12840  /kaggle/input/ubc-ocean-tiles-w-masks-2048px-s...      0.929276   \n21100  /kaggle/input/ubc-ocean-tiles-w-masks-2048px-s...      0.000000   \n25482  /kaggle/input/ubc-ocean-tiles-w-masks-2048px-s...      0.000000   \n\n       non_cancer_ratio       label  \n42402          0.000057  non-cancer  \n30867          0.000000     unknown  \n12840          0.000000      cancer  \n21100          0.000000     unknown  \n25482          0.000000     unknown  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_path</th>\n      <th>mask_path</th>\n      <th>cancer_ratio</th>\n      <th>non_cancer_ratio</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>42402</th>\n      <td>/kaggle/input/ubc-ocean-tiles-w-masks-2048px-s...</td>\n      <td>/kaggle/input/ubc-ocean-tiles-w-masks-2048px-s...</td>\n      <td>0.000000</td>\n      <td>0.000057</td>\n      <td>non-cancer</td>\n    </tr>\n    <tr>\n      <th>30867</th>\n      <td>/kaggle/input/ubc-ocean-tiles-w-masks-2048px-s...</td>\n      <td>/kaggle/input/ubc-ocean-tiles-w-masks-2048px-s...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>unknown</td>\n    </tr>\n    <tr>\n      <th>12840</th>\n      <td>/kaggle/input/ubc-ocean-tiles-w-masks-2048px-s...</td>\n      <td>/kaggle/input/ubc-ocean-tiles-w-masks-2048px-s...</td>\n      <td>0.929276</td>\n      <td>0.000000</td>\n      <td>cancer</td>\n    </tr>\n    <tr>\n      <th>21100</th>\n      <td>/kaggle/input/ubc-ocean-tiles-w-masks-2048px-s...</td>\n      <td>/kaggle/input/ubc-ocean-tiles-w-masks-2048px-s...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>unknown</td>\n    </tr>\n    <tr>\n      <th>25482</th>\n      <td>/kaggle/input/ubc-ocean-tiles-w-masks-2048px-s...</td>\n      <td>/kaggle/input/ubc-ocean-tiles-w-masks-2048px-s...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>unknown</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_train[\"image_id\"] = df_train[\"image_path\"].map(lambda x: int(x.split('/')[-2]))\ndf_train = df_train[~df_train[\"image_path\"].str.lower().str.contains(\"tma\")]\ndf_train = df_train[df_train[\"label\"]!=\"unknown\"].reset_index(drop=True)\ndf_train = pd.merge(df_train, df_orig, on=\"image_id\", how=\"left\")\ndf_train[\"group\"] = df_train[\"label\"] + \"_\" + df_train[\"subtype\"] \ndf_masks = df_train.copy()\n\nencoder = LabelEncoder()\ndf_train['target_label'] = encoder.fit_transform(df_train['label'])\nwith open(\"label_encoder_\"+ CONFIG[\"datetime_now\"] +\".pkl\", \"wb\") as fp:\n    joblib.dump(encoder, fp)\n    \n# use stratified K Fold for crossvalidation \nskf = StratifiedKFold(n_splits=CONFIG['n_fold'], shuffle=True, random_state=CONFIG[\"seed\"],)\n\nfor fold, ( _, val_) in enumerate(skf.split(X=df_train, y=df_train.target_label,  groups=df_train.group)):\n    df_train.loc[val_ , \"kfold\"] = int(fold)\ndisplay(df_train.head())\n\n# separate train and test dataset\ndf_test = df_train[df_train[\"kfold\"]==CONFIG[\"test_fold\"]].reset_index(drop=True)\ndf_train = df_train[df_train[\"kfold\"]!=CONFIG[\"test_fold\"]].reset_index(drop=True)\nprint(f\"Shape df_train: {df_train.shape}, Shape df_test: {df_test.shape} \")","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:24:17.488392Z","iopub.execute_input":"2023-12-09T13:24:17.489257Z","iopub.status.idle":"2023-12-09T13:24:17.670313Z","shell.execute_reply.started":"2023-12-09T13:24:17.489222Z","shell.execute_reply":"2023-12-09T13:24:17.669276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef create_dataframe_from_directory(directory_path):\n    image_paths = []\n    image_ids = []\n\n    for folder in os.listdir(directory_path):\n        folder_path = os.path.join(directory_path, folder)\n        if os.path.isdir(folder_path):\n            for filename in os.listdir(folder_path):\n                if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n                    file_path = os.path.join(folder_path, filename)\n                    image_paths.append(file_path)\n                    image_ids.append(folder)\n\n    df = pd.DataFrame({\n        'image_path': image_paths,\n        'image_id': image_ids\n    })\n\n    return df\n\ndirectory_path = \"/kaggle/input/tiles-of-cancer-2048px-scale-0-25\"\ndf_images = create_dataframe_from_directory(directory_path)\ndf_images.head() ","metadata":{"execution":{"iopub.status.busy":"2023-12-09T16:44:31.546773Z","iopub.execute_input":"2023-12-09T16:44:31.547154Z","iopub.status.idle":"2023-12-09T16:44:56.023035Z","shell.execute_reply.started":"2023-12-09T16:44:31.547126Z","shell.execute_reply":"2023-12-09T16:44:56.022304Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                          image_path image_id\n0  /kaggle/input/tiles-of-cancer-2048px-scale-0-2...     8805\n1  /kaggle/input/tiles-of-cancer-2048px-scale-0-2...     8805\n2  /kaggle/input/tiles-of-cancer-2048px-scale-0-2...     8805\n3  /kaggle/input/tiles-of-cancer-2048px-scale-0-2...     8805\n4  /kaggle/input/tiles-of-cancer-2048px-scale-0-2...     8805","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_path</th>\n      <th>image_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/tiles-of-cancer-2048px-scale-0-2...</td>\n      <td>8805</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/tiles-of-cancer-2048px-scale-0-2...</td>\n      <td>8805</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/tiles-of-cancer-2048px-scale-0-2...</td>\n      <td>8805</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/tiles-of-cancer-2048px-scale-0-2...</td>\n      <td>8805</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/tiles-of-cancer-2048px-scale-0-2...</td>\n      <td>8805</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"\ndf_images.iloc[1004,0], df_images.iloc[1004,1]","metadata":{"execution":{"iopub.status.busy":"2023-12-09T16:45:47.034431Z","iopub.execute_input":"2023-12-09T16:45:47.034840Z","iopub.status.idle":"2023-12-09T16:45:47.042511Z","shell.execute_reply.started":"2023-12-09T16:45:47.034812Z","shell.execute_reply":"2023-12-09T16:45:47.041317Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"('/kaggle/input/tiles-of-cancer-2048px-scale-0-25/46444/000151_8-10.png',\n '46444')"},"metadata":{}}]},{"cell_type":"code","source":"img_color_mean=[0.8661704276539922, 0.7663107094675368, 0.8574260897185548]\nimg_color_std=[0.08670629753900036, 0.11646580094195522, 0.07164169171856792]\n\ndata_transforms = {\n    \"train\": A.Compose([\n        A.Resize(512, 512),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        # A.RandomBrightnessContrast(p=0.75),\n        A.ShiftScaleRotate(p=0.75),\n        A.OneOf([\n        A.GaussNoise(var_limit=[10, 50]),\n        A.GaussianBlur(),\n        A.MotionBlur(),\n        ], p=0.4),\n        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n        A.CoarseDropout(max_holes=5, max_width=int(512* 0.1), max_height=int(512* 0.1),\n        mask_fill_value=0, p=0.5),\n        A.Normalize(img_color_mean, img_color_std), \n        ToTensorV2()], p=1.),\n    \n    \"valid\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.Normalize(img_color_mean, img_color_std), \n        ToTensorV2()], p=1.)\n}\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-09T12:47:57.110161Z","iopub.execute_input":"2023-12-09T12:47:57.110528Z","iopub.status.idle":"2023-12-09T12:47:57.121938Z","shell.execute_reply.started":"2023-12-09T12:47:57.110494Z","shell.execute_reply":"2023-12-09T12:47:57.120923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Training","metadata":{}},{"cell_type":"code","source":"class InferBinaryTilesDataset(Dataset):\n    def __init__(\n        self,\n        df_data,\n        transforms = None,\n        mode: str = 'valid',\n        labels_lut = None,\n        # train_val_split: float = 0.90,\n        tissue_label_th: float = 0.2\n    ):\n\n        self.transforms = transforms\n        self.mode = mode\n        self.train_val_split = train_val_split\n\n        self.data = df_data\n        self.img_paths =  self.data.image_path.values.tolist()\n        \n    def __getitem__(self, idx: int) -> tuple:\n        img_path = self.img_paths[idx]\n        assert os.path.isfile(img_path), f\"missing: {img_path}\"\n        tile = cv2.imread(img_path)\n        tile = cv2.cvtColor(tile, cv2.COLOR_BGR2RGB)\n        # augmentation\n        if self.transforms:\n            tile = self.transforms(image=tile)[\"image\"]\n        #print(f\"img dim: {img.shape}\")\n        return {\n            \"image\": tile,\n            \"image_path\": img_path,\n               }\n    \n    def __len__(self) -> int:\n        return len(self.img_paths)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def infer_on_holdout_set(model, CONFIG, df_test, val_size=1.0):\n    model.eval()\n    test_dataset = InferBinaryTilesDataset(df_test, transforms=data_transforms[\"valid\"], mode=\"test\", train_val_split=1.0)\n    test_loader = DataLoader(test_dataset, batch_size=CONFIG['valid_batch_size'], num_workers=2, shuffle=False, pin_memory=True)\n    print(f\"Test-Dataset Size: {len(test_dataset)}\")\n\n    preds = []\n    probs_list = []\n    image_path_list = []\n    test_acc = 0.0\n\n    with torch.no_grad():\n        bar = tqdm(enumerate(test_loader), total=len(test_loader))\n        for step, data in bar: \n            # print(step)\n            images = data['image'].to(CONFIG[\"device\"], dtype=torch.float)\n            image_paths = data['label'].to(CONFIG[\"device\"], dtype=torch.float)\n\n            outputs = model(images)        \n            probs = torch.sigmoid(outputs)\n            predicted = (probs > 0.5).int()\n            preds.append(predicted.detach().cpu().numpy() )\n            probs_list.append(probs.detach().cpu().numpy() )\n            image_path_list.append(image_paths.detach().cpu().numpy() )\n    pred_labels = encoder.inverse_transform(preds)\n    return df_test\n","metadata":{"execution":{"iopub.status.busy":"2023-12-09T12:47:57.147807Z","iopub.execute_input":"2023-12-09T12:47:57.148134Z","iopub.status.idle":"2023-12-09T12:47:57.162363Z","shell.execute_reply.started":"2023-12-09T12:47:57.148102Z","shell.execute_reply":"2023-12-09T12:47:57.161598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6):\n        super(GeM, self).__init__()\n        self.p = nn.Parameter(torch.ones(1)*p)\n        self.eps = eps\n\n    def forward(self, x):\n        return self.gem(x, p=self.p, eps=self.eps)\n        \n    def gem(self, x, p=3, eps=1e-6):\n        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n        \n    def __repr__(self):\n        return self.__class__.__name__ + \\\n                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n                ', ' + 'eps=' + str(self.eps) + ')'\n\n\nclass UBCBinaryModel(nn.Module):\n    '''\n    EfficientNet B0 fine-tune.\n    '''\n    def __init__(self, model_name, pretrained=False, checkpoint_path=None):\n        '''\n        Fine tune for EfficientNetB0\n        Args\n            learnable_modules : tuple - Names of the modules to fine-tune.\n        Return\n            \n        '''\n        super(UBCBinaryModel, self).__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained, checkpoint_path=checkpoint_path)\n\n        in_features = self.model.classifier.in_features\n        self.model.classifier = nn.Identity()\n        self.model.global_pool = nn.Identity()\n        self.pooling = GeM()\n        self.linear = nn.Linear(in_features, 1)\n        \n\n    def forward(self, images):\n        \"\"\"\n        Forward function for the fine-tuned model\n        Args\n            x: \n        Return\n            result\n        \"\"\"\n        features = self.model(images)\n        pooled_features = self.pooling(features).flatten(1)\n        logits = self.linear(pooled_features)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2023-12-09T12:47:57.4693Z","iopub.execute_input":"2023-12-09T12:47:57.469662Z","iopub.status.idle":"2023-12-09T12:47:57.482774Z","shell.execute_reply.started":"2023-12-09T12:47:57.469633Z","shell.execute_reply":"2023-12-09T12:47:57.481844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Shape df_train: {df_train.shape}, Shape df_test: {df_test.shape}\")\n\nmodel = UBCBinaryModel(CONFIG['model_name'], pretrained=False , checkpoint_path=CONFIG[\"checkpoint_path\"])\n# model.load_state_dict(torch.load(CONFIG[\"checkpoint_path\"]))\nmodel.load_state_dict(torch.load(save_model_path))\nmodel.to(CONFIG['device']);\n\nencoder","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Infer on Holdout Set:\")\ndf_test = test_on_holdout(model, CONFIG, df_test, val_size=1)\ndf_test_file_path = \"df_test_results.csv\"\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:24:30.205509Z","iopub.execute_input":"2023-12-09T13:24:30.205878Z","iopub.status.idle":"2023-12-09T13:24:38.714828Z","shell.execute_reply.started":"2023-12-09T13:24:30.205847Z","shell.execute_reply":"2023-12-09T13:24:38.713077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nmodel = UBCModel(CONFIG['model_name'], CONFIG['num_classes'], pretrained=False , checkpoint_path=None)\nmodel.load_state_dict(torch.load(\"/kaggle/input/effnet-version-28/best_model_checkpoint2023-11-21_15-47-39.pth\"))\nmodel.to(CONFIG['device']);\ndf_test = test_on_holdout(model, CONFIG, df_test, TRAIN_DIR, val_size=1, n_tiles=CONFIG[\"n_tiles_test\"])\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-11-24T19:58:16.905615Z","iopub.execute_input":"2023-11-24T19:58:16.906004Z","iopub.status.idle":"2023-11-24T19:58:18.453133Z","shell.execute_reply.started":"2023-11-24T19:58:16.905973Z","shell.execute_reply":"2023-11-24T19:58:18.452382Z"},"trusted":true},"execution_count":null,"outputs":[]}]}