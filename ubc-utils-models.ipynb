{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":45867,"databundleVersionId":6924515,"sourceType":"competition"}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Standard library imports\nimport copy\nimport datetime\nimport gc\nimport glob\nimport heapq\nimport joblib\nimport math\nimport os\nimport random\nimport time\nfrom collections import defaultdict\nfrom itertools import chain\n\n# Related third-party imports\nimport albumentations as A\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nfrom albumentations.pytorch import ToTensorV2\nfrom colorama import Fore, Back, Style\nfrom joblib import Parallel, delayed\nfrom matplotlib import pyplot as plt\nfrom PIL import Image\nfrom skimage import io\nfrom sklearn.metrics import balanced_accuracy_score, confusion_matrix, f1_score, accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.cuda import amp\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm, tqdm_notebook\n\n# Local application/library specific imports\n# (Your local imports here, if any)\n\n# Set up for colored terminal text\nb_ = Fore.BLUE\nsr_ = Style.RESET_ALL\n\n# Disable warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Additional note: The following import seems to be from a very specific context or not typically used.\n# You might want to review if it's necessary or correct:\n# from joblib.externals.loky.backend.context import get_context\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-12T15:50:40.977296Z","iopub.execute_input":"2023-12-12T15:50:40.977703Z","iopub.status.idle":"2023-12-12T15:50:50.415640Z","shell.execute_reply.started":"2023-12-12T15:50:40.977669Z","shell.execute_reply":"2023-12-12T15:50:50.414494Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# 3. Model Architecture","metadata":{}},{"cell_type":"code","source":"class GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6):\n        super(GeM, self).__init__()\n        self.p = nn.Parameter(torch.ones(1)*p)\n        self.eps = eps\n\n    def forward(self, x):\n        return self.gem(x, p=self.p, eps=self.eps)\n        \n    def gem(self, x, p=3, eps=1e-6):\n        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n        \n    def __repr__(self):\n        return self.__class__.__name__ + \\\n                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n                ', ' + 'eps=' + str(self.eps) + ')'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UBCModel(nn.Module):\n    '''\n    EfficientNet B0 fine-tune.\n    '''\n    def __init__(self, model_name, num_classes, pretrained=False, checkpoint_path=None):\n        '''\n        Fine tune for EfficientNetB0\n        Args\n            n_classes : int - Number of classification categories.\n            learnable_modules : tuple - Names of the modules to fine-tune.\n        Return\n            \n        '''\n        super(UBCModel, self).__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained, checkpoint_path=checkpoint_path)\n\n        in_features = self.model.classifier.in_features\n        self.model.classifier = nn.Identity()\n        self.model.global_pool = nn.Identity()\n        self.pooling = GeM()\n        self.linear = nn.Linear(in_features, num_classes)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, images):\n        \"\"\"\n        Forward function for the fine-tuned model\n        Args\n            x: \n        Return\n            result\n        \"\"\"\n        features = self.model(images)\n        pooled_features = self.pooling(features).flatten(1)\n        output = self.linear(pooled_features)\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-11-08T09:01:23.860451Z","iopub.execute_input":"2023-11-08T09:01:23.861541Z","iopub.status.idle":"2023-11-08T09:01:23.873475Z","shell.execute_reply.started":"2023-11-08T09:01:23.861465Z","shell.execute_reply":"2023-11-08T09:01:23.872397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This model is used for binary classification on cancerous vs non-cancerous tiles \nclass UBCBinaryModel(nn.Module):\n    '''\n    EfficientNet B0 fine-tune.\n    '''\n    def __init__(self, model_name, pretrained=False, checkpoint_path=None):\n        '''\n        Fine tune for EfficientNetB0\n        Args\n            learnable_modules : tuple - Names of the modules to fine-tune.\n        Return\n            \n        '''\n        super(UBCBinaryModel, self).__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained, checkpoint_path=checkpoint_path)\n\n        in_features = self.model.classifier.in_features\n        self.model.classifier = nn.Identity()\n        self.model.global_pool = nn.Identity()\n        self.pooling = GeM()\n        self.linear = nn.Linear(in_features, 1)\n        \n\n    def forward(self, images):\n        features = self.model(images)\n        pooled_features = self.pooling(features).flatten(1)\n        logits = self.linear(pooled_features)\n        return logits","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=5, verbose=False, delta=0, path='checkpoint.pth', trace_func=print):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = float('inf')\n        self.delta = delta\n        self.path = path\n        self.trace_func = trace_func\n\n    def __call__(self, val_loss, model):\n        score = -val_loss\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Saves model when validation loss decreases.'''\n        if self.verbose:\n            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...')\n        torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss","metadata":{"execution":{"iopub.status.busy":"2023-11-08T09:01:23.943061Z","iopub.execute_input":"2023-11-08T09:01:23.94359Z","iopub.status.idle":"2023-11-08T09:01:23.9543Z","shell.execute_reply.started":"2023-11-08T09:01:23.943557Z","shell.execute_reply":"2023-11-08T09:01:23.95259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Training","metadata":{}},{"cell_type":"code","source":"def fetch_scheduler(optimizer, CONFIG):\n    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], \n                                                   eta_min=CONFIG['min_lr'], verbose=False)\n    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n        CONFIG['T_0'] = 10\n        CONFIG['T_mult'] = 2\n        CONFIG['min_lr'] = 1e-6\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CONFIG['T_0'], T_mult=CONFIG['T_mult'],\n                                                             eta_min=CONFIG['min_lr'], verbose=False)\n    elif CONFIG['scheduler'] == 'ReduceLROnPlateau':\n        scheduler =  ReduceLROnPlateau(optimizer, mode='min', factor=kwargs.get('factor', 0.1), patience=kwargs.get('patience', 5), verbose=False)\n    elif CONFIG['scheduler'] == 'LambdaLR':\n        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda)\n    elif CONFIG['scheduler'] == None:\n        return None\n    return scheduler\n\ndef get_optimizer(optimizer_name, model, CONFIG):\n    if optimizer_name.lower() == \"adam\":\n        CONFIG['learning_rate'] = 1e-4\n        CONFIG['weight_decay'] = 1e-5\n        CONFIG['betas'] = (0.9, 0.999)\n        CONFIG['eps'] = 1e-8\n        optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'], betas=CONFIG['betas'], eps=CONFIG['eps'],  weight_decay=CONFIG['weight_decay'])\n    elif optimizer_name.lower() == \"sgd\":\n        CONFIG['learning_rate'] = 1e-3\n        CONFIG['weight_decay'] = 1e-3\n        CONFIG['momentum'] = 1e-3\n        optimizer = optim.SGD(model.parameters(), lr=CONFIG['learning_rate'], momentum=CONFIG['momentum'], weight_decay=CONFIG['weight_decay'])\n    elif optimizer_name.lower() == \"radam\":\n        CONFIG['learning_rate'] = 1e-4\n        CONFIG['weight_decay'] = 0\n        CONFIG['betas'] = (0.9, 0.999)\n        CONFIG['eps'] = 1e-8\n        optimizer = torch_optimizer.RAdam(\n            model.parameters(),\n            lr= CONFIG['learning_rate'],\n            betas=CONFIG['betas'],\n            eps=CONFIG['eps'],\n            weight_decay=CONFIG['weight_decay'],\n        )\n    elif optimizer_name.lower() == \"rmsprop\":\n        CONFIG['learning_rate'] = 0.256\n        CONFIG['alpha'] = 0.9\n        CONFIG['momentum'] = 0.9\n        CONFIG['weight_decay'] = 1e-5\n        optimizer = optim.RMSprop(model.parameters(), lr=CONFIG['learning_rate'], alpha=CONFIG['learning_rate'], \n                                  momentum=CONFIG['learning_rate'], weight_decay=CONFIG['learning_rate'])\n    else:\n        raise ValueError(\"Invalid Optimizer given!\")\n    return optimizer\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-08T09:01:24.025798Z","iopub.execute_input":"2023-11-08T09:01:24.026205Z","iopub.status.idle":"2023-11-08T09:01:24.039156Z","shell.execute_reply.started":"2023-11-08T09:01:24.026177Z","shell.execute_reply":"2023-11-08T09:01:24.037687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}