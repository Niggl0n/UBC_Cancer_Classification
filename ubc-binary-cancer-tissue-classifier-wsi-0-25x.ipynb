{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":45867,"databundleVersionId":6924515,"sourceType":"competition"},{"sourceId":6755371,"sourceType":"datasetVersion","datasetId":3888697},{"sourceId":6917177,"sourceType":"datasetVersion","datasetId":3889865},{"sourceId":6984590,"sourceType":"datasetVersion","datasetId":4014175},{"sourceId":7018060,"sourceType":"datasetVersion","datasetId":4035314},{"sourceId":7029230,"sourceType":"datasetVersion","datasetId":4027203},{"sourceId":7156628,"sourceType":"datasetVersion","datasetId":4133077},{"sourceId":154268140,"sourceType":"kernelVersion"}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction \n\n**This is a basic CNN Model training notebook**\n\nIt is based on: \n- Thumbnail images\n- Basic data transformation (using Albumentation):\n    - resizing images to 512x512\n    - normalizing pixel values\n- CNN Architecture\n\n\n**Todos:**\n\n- Learn about Dataset & DataLoader\n- add augmentations (albumentation)\n- gem pooling","metadata":{}},{"cell_type":"code","source":"!pip install --quiet torch_optimizer\nimport torch_optimizer as torch_optimizer\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T10:26:02.837942Z","iopub.execute_input":"2023-12-10T10:26:02.838819Z","iopub.status.idle":"2023-12-10T10:26:15.037335Z","shell.execute_reply.started":"2023-12-10T10:26:02.838784Z","shell.execute_reply":"2023-12-10T10:26:15.036063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --quiet mlflow dagshub\nimport mlflow.pytorch \nfrom mlflow import MlflowClient\nimport dagshub\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:30:40.774255Z","iopub.execute_input":"2023-12-11T11:30:40.774834Z","iopub.status.idle":"2023-12-11T11:31:02.298315Z","shell.execute_reply.started":"2023-12-11T11:30:40.774805Z","shell.execute_reply":"2023-12-11T11:31:02.297324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nimport os\nimport gc\nimport cv2\nimport datetime\nimport math\nimport copy\nimport time\nimport random\nimport glob\nfrom matplotlib import pyplot as plt\nfrom skimage import io\n\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.cuda import amp\nimport torchvision\n\nimport optuna\nfrom optuna.trial import TrialState\n\n# Utils\nimport joblib\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\n\nfrom PIL import Image\nfrom joblib import Parallel, delayed\nfrom tqdm.auto import tqdm\n\n# Sklearn Imports\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import balanced_accuracy_score, confusion_matrix, f1_score\nfrom torch.utils.data.sampler import WeightedRandomSampler\n\n# For Image Models\nimport timm\n\nfrom getpass import getpass\n\n# Albumentations for augmentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\nb_ = Fore.BLUE\nsr_ = Style.RESET_ALL\n\nimport warnings\n# warnings.filterwarnings(\"ignore\")\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:31:02.300315Z","iopub.execute_input":"2023-12-11T11:31:02.300773Z","iopub.status.idle":"2023-12-11T11:31:17.202423Z","shell.execute_reply.started":"2023-12-11T11:31:02.300744Z","shell.execute_reply":"2023-12-11T11:31:17.201659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from cancer_utils_tiles import get_class_weights,UBCModel, get_optimizer, fetch_scheduler, EarlyStopping, print_logged_info, get_or_create_experiment_id\n","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:31:17.203473Z","iopub.execute_input":"2023-12-11T11:31:17.204008Z","iopub.status.idle":"2023-12-11T11:31:29.668798Z","shell.execute_reply.started":"2023-12-11T11:31:17.203981Z","shell.execute_reply":"2023-12-11T11:31:29.667718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ[\"MLFLOW_TRACKING_USERNAME\"]=\"Niggl0n\"\nos.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"7a3590e8c5558d4598dacc7810befa70a4baac9e\"\nos.environ['MLFLOW_TRACKING_PROJECTNAME'] = \"UBC_Cancer_Classification\"\n#dagshub.auth.add_app_token(\"7a3590e8c5558d4598dacc7810befa70a4baac9e\")\nmlflow.set_tracking_uri(f'https://dagshub.com/' + os.environ['MLFLOW_TRACKING_USERNAME'] + '/' + os.environ['MLFLOW_TRACKING_PROJECTNAME'] + '.mlflow')","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:31:29.671279Z","iopub.execute_input":"2023-12-11T11:31:29.671577Z","iopub.status.idle":"2023-12-11T11:31:29.676961Z","shell.execute_reply.started":"2023-12-11T11:31:29.671551Z","shell.execute_reply":"2023-12-11T11:31:29.676155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = {\n    \"is_submission\": False,\n    \"weighted_loss\": True,\n    \"datetime_now\": datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"), \n    \"n_fold\":5, \n    \"test_fold\": 0,\n    \"valid_fold\": 2,\n    \"seed\": 42,\n    \"img_size\": 512,\n    \"model_name\": \"tf_efficientnet_b0_ns\",   # \"tf_efficientnet_b0_ns\", # \"tf_efficientnetv2_s_in21ft1k\"\n    \"checkpoint_path\": \"/kaggle/input/tf-efficientnet-b0-aa-827b6e33-pth/tf_efficientnet_b0_aa-827b6e33.pth\",\n    \"num_classes\": 5,\n    \"train_batch_size\": 8,\n    \"valid_batch_size\": 8,\n    \"n_tiles\": 10,\n    \"n_tiles_test\": 10,\n    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n    \"num_epochs\": 15,\n    \"early_stopping\": True,\n    \"patience\": 6,\n    \"optimizer\": 'adam',\n    \"scheduler\": 'CosineAnnealingLR',\n    \"min_lr\": 1e-6,\n    \"T_max\": 10,\n    \"momentum\": 0.9,\n    \"weight_decay\": 1e-4,\n}","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:31:29.678248Z","iopub.execute_input":"2023-12-11T11:31:29.678501Z","iopub.status.idle":"2023-12-11T11:31:29.696931Z","shell.execute_reply.started":"2023-12-11T11:31:29.678479Z","shell.execute_reply":"2023-12-11T11:31:29.696047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Data Preparation","metadata":{}},{"cell_type":"code","source":"ROOT_DIR = '/kaggle/input/UBC-OCEAN'\nTRAIN_DIR = '/kaggle/input/ubc-ocean-tiles-w-masks-2048px-scale-0-25'\ndf_orig = pd.read_csv(\"/kaggle/input/UBC-OCEAN/train.csv\")\ndf_orig = df_orig.rename(columns={\"label\":\"subtype\"})\ndf_train = pd.read_csv(\"/kaggle/input/df-tiles-025x-cancer-tissue-binary-labels/tiles_labelled_binary_cancer.csv\", index_col=\"Unnamed: 0\")\ndisplay(df_orig.sample(5))\ndisplay(df_train.sample(5))","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:31:29.698034Z","iopub.execute_input":"2023-12-11T11:31:29.698314Z","iopub.status.idle":"2023-12-11T11:31:29.992633Z","shell.execute_reply.started":"2023-12-11T11:31:29.698292Z","shell.execute_reply":"2023-12-11T11:31:29.991768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=6, shuffle=True, random_state=CONFIG[\"seed\"],)\nfor fold, ( _, val_) in enumerate(skf.split(X=df_orig, y=df_orig.subtype)):\n    df_orig.loc[val_ , \"kfold\"] = int(fold)\ndisplay(df_orig.head())","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:31:29.993996Z","iopub.execute_input":"2023-12-11T11:31:29.994367Z","iopub.status.idle":"2023-12-11T11:31:30.014893Z","shell.execute_reply.started":"2023-12-11T11:31:29.994333Z","shell.execute_reply":"2023-12-11T11:31:30.014052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[\"image_id\"] = df_train[\"image_path\"].map(lambda x: int(x.split('/')[-2]))\ndf_train = df_train[~df_train[\"image_path\"].str.lower().str.contains(\"tma\")]\ndf_train = df_train[df_train[\"label\"]!=\"unknown\"].reset_index(drop=True)\ndf_train = pd.merge(df_train, df_orig, on=\"image_id\", how=\"left\")\ndf_train[\"group\"] = df_train[\"label\"] + \"_\" + df_train[\"subtype\"] \n\nencoder = LabelEncoder()\ndf_train['target_label'] = encoder.fit_transform(df_train['label'])\nwith open(\"label_encoder_\"+ CONFIG[\"datetime_now\"] +\".pkl\", \"wb\") as fp:\n    joblib.dump(encoder, fp)\n    \n# use stratified K Fold for crossvalidation \n\n#for fold, ( _, val_) in enumerate(skf.split(X=df_train, y=df_train.target_label,  groups=df_train.group)):\n#    df_train.loc[val_ , \"kfold\"] = int(fold)\n#display(df_train.head())\n\n# separate train and test dataset\ndf_test = df_train[df_train[\"kfold\"]==CONFIG[\"test_fold\"]].reset_index(drop=True)\ndf_valid = df_train[df_train[\"kfold\"]==CONFIG[\"valid_fold\"]].reset_index(drop=True)\ndf_train = df_train[~df_train[\"kfold\"].isin([CONFIG[\"valid_fold\"],CONFIG[\"test_fold\"]])].reset_index(drop=True)\nprint(f\"Shape df_train: {df_train.shape}, Shape df_test: {df_valid.shape}, Shape df_test: {df_test.shape} \")","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:31:30.015830Z","iopub.execute_input":"2023-12-11T11:31:30.016070Z","iopub.status.idle":"2023-12-11T11:31:30.171541Z","shell.execute_reply.started":"2023-12-11T11:31:30.016049Z","shell.execute_reply":"2023-12-11T11:31:30.170647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df_train.subtype.value_counts())\ndisplay(df_valid.subtype.value_counts())\ndisplay(df_test.subtype.value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:31:30.172781Z","iopub.execute_input":"2023-12-11T11:31:30.173141Z","iopub.status.idle":"2023-12-11T11:31:30.189637Z","shell.execute_reply.started":"2023-12-11T11:31:30.173108Z","shell.execute_reply":"2023-12-11T11:31:30.188658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"unique_ids = df_train[\"image_id\"].unique()\ntrain_test_split = 0.2\nsampled_ids = np.random.choice(unique_ids, size=int(len(unique_ids) * train_test_split), replace=False)\nlen(unique_ids), len(sampled_ids)\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-12-09T16:54:21.741829Z","iopub.execute_input":"2023-12-09T16:54:21.742704Z","iopub.status.idle":"2023-12-09T16:54:21.749843Z","shell.execute_reply.started":"2023-12-09T16:54:21.742672Z","shell.execute_reply":"2023-12-09T16:54:21.748976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Todos: \n# - weighted based on (non) cancer ratio and/or label \n# - transformations -> histogram matching\n\nclass LabelledTilesDataset(Dataset):\n    def __init__(\n        self,\n        df_data,\n        # path_img_dir: str =  '',\n        transforms = None,\n        mode: str = 'train',\n        labels_lut = None,\n        #white_thr: int = 225,\n        #thr_max_bg: float = 0.2,\n        train_val_split: float = 0.90,\n        tissue_label_th: float = 0.2\n        # n_tiles: int = 1,\n        # tma_weight: float = 1.0,\n    ):\n        # assert os.path.isdir(path_img_dir)\n        #self.path_img_dir = path_img_dir\n        self.transforms = transforms\n        self.mode = mode\n        #self.white_thr = white_thr\n        #self.thr_max_bg = thr_max_bg\n        # self.train_val_split = train_val_split\n        #self.n_tiles = n_tiles\n        #self.tma_weight = tma_weight\n\n        self.data = df_data\n        self.data[\"max_ratio\"] = self.data[[\"cancer_ratio\", \"non_cancer_ratio\"]].max(axis=1)\n        self.data = self.data[self.data[\"max_ratio\"]>tissue_label_th]\n        self.data = self.data.sample(frac=1, random_state=42).reset_index(drop=True)\n\n        self.labels_unique = sorted(self.data[\"label\"].unique())\n        self.labels_lut = labels_lut or {lb: i for i, lb in enumerate(self.labels_unique)}\n\n        # split dataset   # No splitting needed --> commented\n        #assert 0.0 <= self.train_val_split <= 1.0\n        #frac = int(self.train_val_split * len(self.data))\n        # self.data = self.data[:frac] if mode in [\"train\", \"test\"] else self.data[frac:]  \n        self.labels =  np.array(self.data.target_label.values.tolist())\n        self.img_paths =  self.data.image_path.values.tolist()\n        \n        \n        \n        # set sample weights \n        # self.sample_weights = [self.tma_weight if is_tma == True else 1 for is_tma in self.data[\"is_tma\"]] \n        self.sample_weights =  np.array((self.data[\"target_label\"] + 1) * self.data[\"max_ratio\"])\n        \n    def __getitem__(self, idx: int) -> tuple:\n        #nth_iteration = idx//len(self.data)\n        #if self.mode==\"train\":\n        #    random.seed()\n        #else:\n        #    random.seed(CONFIG[\"seed\"]+nth_iteration)\n        #random.shuffle(self.img_dirs[idx])\n        #for img_path in self.img_paths[idx]:\n        img_path = self.img_paths[idx]\n        assert os.path.isfile(img_path), f\"missing: {img_path}\"\n        tile = cv2.imread(img_path)\n        tile = cv2.cvtColor(tile, cv2.COLOR_BGR2RGB)\n        #black_bg = np.sum(tile, axis=2) == 0\n        #tile[black_bg, :] = 255\n        #mask_bg = np.mean(tile, axis=2) > self.white_thr\n        #if np.sum(mask_bg) < (np.prod(mask_bg.shape) * self.thr_max_bg):\n            #self.img_paths.append(img_path)\n            #print(f\"Idx: {idx}, Path: {img_path}, len img_pths: {len(self.img_paths)}, nunique img_paths: {len(set(self.img_paths))}\")\n        #    break\n\n        # augmentation\n        if self.transforms:\n            tile = self.transforms(image=tile)[\"image\"]\n        #print(f\"img dim: {img.shape}\")\n        return {\n            \"image\": tile,\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n               }\n    def __len__(self) -> int:\n        return len(self.img_paths)\n    \n    def get_sample_weights(self):\n        return torch.from_numpy(self.sample_weights).double()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T10:47:33.165074Z","iopub.execute_input":"2023-12-10T10:47:33.165471Z","iopub.status.idle":"2023-12-10T10:47:33.179266Z","shell.execute_reply.started":"2023-12-10T10:47:33.165439Z","shell.execute_reply":"2023-12-10T10:47:33.178154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_color_mean=[0.8661704276539922, 0.7663107094675368, 0.8574260897185548]\nimg_color_std=[0.08670629753900036, 0.11646580094195522, 0.07164169171856792]\n\ndata_transforms = {\n    \"train\": A.Compose([\n        A.Resize(512, 512),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        # A.RandomBrightnessContrast(p=0.75),\n        A.ShiftScaleRotate(p=0.75),\n        A.OneOf([\n        A.GaussNoise(var_limit=[10, 50]),\n        A.GaussianBlur(),\n        A.MotionBlur(),\n        ], p=0.4),\n        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n        A.CoarseDropout(max_holes=5, max_width=int(512* 0.1), max_height=int(512* 0.1),\n        mask_fill_value=0, p=0.5),\n        A.Normalize(img_color_mean, img_color_std), \n        ToTensorV2()], p=1.),\n    \n    \"valid\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.Normalize(img_color_mean, img_color_std), \n        ToTensorV2()], p=1.)\n}\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T10:24:47.447007Z","iopub.execute_input":"2023-12-10T10:24:47.447377Z","iopub.status.idle":"2023-12-10T10:24:47.457763Z","shell.execute_reply.started":"2023-12-10T10:24:47.447345Z","shell.execute_reply":"2023-12-10T10:24:47.456826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Training","metadata":{}},{"cell_type":"code","source":"def train_one_epoch(model, train_loader, optimizer, criterion, device, writer, epoch, scheduler=None):\n    if torch.cuda.is_available():\n        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n    model.train()\n    train_loss = 0.0\n    bar = tqdm(enumerate(train_loader), total=len(train_loader))\n    for step, data in bar:\n        images = data['image'].to(device, dtype=torch.float)\n        labels = data['label'].to(device, dtype=torch.float).unsqueeze(1)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * images.size(0)\n        writer.add_scalar('loss/train_batch', loss.item(), epoch * len(train_loader) + step)\n    \n    # Update learning rate using the scheduler\n    if scheduler:\n        scheduler.step()\n    train_loss /= len(train_loader.dataset)\n    # Log the average training loss for the epoch to TensorBoard\n    writer.add_scalar('loss/train_epoch', train_loss, epoch)\n    # gc.collect()\n    return train_loss\n\ndef validate_one_epoch(model, valid_loader, criterion, device, writer, epoch):\n    model.eval()\n    valid_loss = 0.0\n    valid_acc = 0.0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        bar_val = tqdm(enumerate(valid_loader), total=len(valid_loader))\n        for step, data in bar_val:\n            images = data['image'].to(device, dtype=torch.float)\n            labels = data['label'].to(device, dtype=torch.float).unsqueeze(1)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n        \n            valid_loss += loss.item() * images.size(0)\n\n            # Applying sigmoid to the model outputs\n            probs = torch.sigmoid(outputs)\n            # Thresholding to get binary predictions\n            predicted = (probs > 0.5).float()\n\n            acc = torch.sum(predicted == labels).item() / images.size(0)\n            valid_acc += acc\n            \n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n        \n            writer.add_scalar('loss/valid_batch', loss.item(), epoch * len(valid_loader) + step)\n            writer.add_scalar('acc/valid_batch', acc, epoch * len(valid_loader) + step)\n    valid_loss /= len(valid_loader.dataset)\n    valid_acc /= len(valid_loader)\n    bal_acc = balanced_accuracy_score(all_labels, all_preds)\n    # At the end of your validation loop:\n    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n    micro_f1 = f1_score(all_labels, all_preds, average='micro')\n    weighted_f1 = f1_score(all_labels, all_preds, average='weighted')\n\n    # Logging to TensorBoard\n    writer.add_scalar('loss/val_epoch', valid_loss, epoch)\n    writer.add_scalar('acc/val_epoch', valid_acc, epoch)\n    writer.add_scalar('balanced_acc/val_epoch', bal_acc, epoch)\n    writer.add_scalar('F1/macro', macro_f1, epoch)\n    writer.add_scalar('F1/micro', micro_f1, epoch)\n    writer.add_scalar('F1/weighted', weighted_f1, epoch)\n    # in order to put multiple lines within one graph\n    #writer.add_scalars('run_14h', {'xsinx':i*np.sin(i/r),\n    #                        'xcosx':i*np.cos(i/r),\n    #                        'tanx': np.tan(i/r)}, i)\n    return valid_loss, valid_acc, bal_acc, weighted_f1\n\ndef train_model(model, train_loader, valid_loader, optimizer, criterion, device, num_epochs, scheduler, save_model_path=None):\n    model_name = \"model_epochs\" + str(CONFIG[\"num_epochs\"]) + \"_bs\"+str(CONFIG[\"train_batch_size\"] )+ \"_opt\" +CONFIG[\"optimizer\"]+ \"_sched\" + CONFIG[\"scheduler\"] + \"_lr\"+str(CONFIG[\"learning_rate\"])+ \"_wd\" + str(CONFIG[\"weight_decay\"])\n    print(f\"Training model: {model_name}\")\n    datetime_now =  datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n    if not save_model_path:\n        save_model_path = 'best_model_checkpoint' + datetime_now + '.pth'\n    print(f\"Path for saving model: {save_model_path}\")\n    # Initialize TensorBoard writer\n    writer = SummaryWriter('logs/fit/' + model_name)\n    early_stopping = EarlyStopping(patience=CONFIG[\"patience\"], verbose=True, path=save_model_path)\n    \n    for epoch in range(num_epochs):\n        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, writer, epoch, scheduler)\n        valid_loss, valid_acc, bal_acc, weighted_f1 = validate_one_epoch(model, valid_loader, criterion, device, writer, epoch)\n        print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {train_loss:.4f}, Validation loss: {valid_loss:.4f}, Validation acc: {valid_acc:.4f}, Balanced acc: {bal_acc:.4f}, Weighted F1-Score: {weighted_f1:.4f}\")\n\n        if CONFIG[\"early_stopping\"]:\n            early_stopping(valid_loss, model)\n            if early_stopping.early_stop:\n                print(\"Early stopping\")\n                break\n        writer.close()\n\n        mlflow.log_metrics({\n            'epoch': epoch,\n            'train_loss': train_loss,\n            'valid_loss': valid_loss,\n            'valid_acc': valid_acc,\n            'balanced_acc': bal_acc,\n            'weighted_f1': weighted_f1\n        }, step=epoch)\n    return train_loss, valid_loss, valid_acc, save_model_path\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T10:24:51.623656Z","iopub.execute_input":"2023-12-10T10:24:51.624703Z","iopub.status.idle":"2023-12-10T10:24:51.648970Z","shell.execute_reply.started":"2023-12-10T10:24:51.624656Z","shell.execute_reply":"2023-12-10T10:24:51.647951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_on_holdout(model, CONFIG, df_test, val_size=1.0):\n    if not CONFIG[\"is_submission\"]:\n        model.eval()\n        test_dataset = LabelledTilesDataset(df_test, transforms=data_transforms[\"valid\"], mode=\"test\", train_val_split=1.0)\n        test_loader = DataLoader(test_dataset, batch_size=CONFIG['valid_batch_size'], \n                                  num_workers=2, shuffle=False, pin_memory=True)\n        print(f\"Test-Dataset Size: {len(test_dataset)}\")\n\n        preds = []\n        labels_list = []\n        test_acc = 0.0\n\n        with torch.no_grad():\n            bar = tqdm(enumerate(test_loader), total=len(test_loader))\n            for step, data in bar: \n                # print(step)\n                images = data['image'].to(CONFIG[\"device\"], dtype=torch.float)\n                labels = data['label'].to(CONFIG[\"device\"], dtype=torch.float).unsqueeze(1)\n\n                outputs = model(images)        \n                probs = torch.sigmoid(outputs)\n                predicted = (probs > 0.5).int()\n                preds.append(predicted.detach().cpu().numpy() )\n                labels_list.append(labels.detach().cpu().numpy() )\n                acc = torch.sum(predicted == labels).item() / images.size(0)\n                test_acc  += acc       \n                \n        test_acc /= len(test_loader) \n        preds = np.concatenate(preds).astype(int).flatten()\n        labels_list = np.concatenate(labels_list).flatten()\n        pred_labels = encoder.inverse_transform(preds)\n        \n        # Calculate Balanced Accuracy\n        bal_acc = balanced_accuracy_score(labels_list, preds)\n        # Calculate Confusion Matrix\n        conf_matrix = confusion_matrix(labels_list, preds)\n        macro_f1 = f1_score(labels_list, preds, average='macro')\n\n    \n        print(f\"Test Accuracy: {test_acc}\")\n        print(f\"Balanced Accuracy: {bal_acc}\")\n        print(f\"Confusion Matrix: {conf_matrix}\")\n        mlflow.log_metrics({\n            'test_acc': test_acc,\n            'test_balanced_acc': bal_acc,\n            'test_f1_score': macro_f1,\n        })\n        return df_test\n    else:\n        print(\"Skip validation on training set due to submission!\")\n        return None","metadata":{"execution":{"iopub.status.busy":"2023-12-10T10:41:31.338381Z","iopub.execute_input":"2023-12-10T10:41:31.338765Z","iopub.status.idle":"2023-12-10T10:41:31.352275Z","shell.execute_reply.started":"2023-12-10T10:41:31.338734Z","shell.execute_reply":"2023-12-10T10:41:31.351251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetch_scheduler(optimizer, CONFIG):\n\n    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n        CONFIG['T_max'] = 10\n        CONFIG['min_lr'] = 1e-6\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], \n                                                   eta_min=CONFIG['min_lr'], verbose=False)\n    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n        CONFIG['T_0'] = 10\n        CONFIG['T_mult'] = 2\n        CONFIG['min_lr'] = 1e-6\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CONFIG['T_0'], T_mult=CONFIG['T_mult'],\n                                                             eta_min=CONFIG['min_lr'], verbose=False)\n    elif CONFIG['scheduler'] == 'ReduceLROnPlateau':\n        scheduler =  ReduceLROnPlateau(optimizer, mode='min', factor=kwargs.get('factor', 0.1), patience=kwargs.get('patience', 5), verbose=False)\n    elif CONFIG['scheduler'] == 'LambdaLR':\n        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda)\n    elif CONFIG['scheduler'] == None:\n        return None\n    return scheduler\n\ndef get_optimizer(optimizer_name, model):\n    if optimizer_name.lower() == \"adam\":\n        CONFIG['learning_rate'] = 1e-5\n        CONFIG['weight_decay'] = 1e-6\n        CONFIG['betas'] = (0.9, 0.999)\n        CONFIG['eps'] = 1e-8\n        optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'], betas=CONFIG['betas'], eps=CONFIG['eps'],  weight_decay=CONFIG['weight_decay'])\n    elif optimizer_name.lower() == \"sgd\":\n        CONFIG['learning_rate'] = 1e-3\n        CONFIG['weight_decay'] = 1e-3\n        CONFIG['momentum'] = 1e-3\n        optimizer = optim.SGD(model.parameters(), lr=CONFIG['learning_rate'], momentum=CONFIG['momentum'], weight_decay=CONFIG['weight_decay'])\n    elif optimizer_name.lower() == \"radam\":\n        CONFIG['learning_rate'] = 1e-4\n        CONFIG['weight_decay'] = 0\n        CONFIG['betas'] = (0.9, 0.999)\n        CONFIG['eps'] = 1e-8\n        optimizer = torch_optimizer.RAdam(\n            model.parameters(),\n            lr= CONFIG['learning_rate'],\n            betas=CONFIG['betas'],\n            eps=CONFIG['eps'],\n            weight_decay=CONFIG['weight_decay'],\n        )\n    elif optimizer_name.lower() == \"rmsprop\":\n        CONFIG['learning_rate'] = 0.256\n        CONFIG['alpha'] = 0.9\n        CONFIG['momentum'] = 0.9\n        CONFIG['weight_decay'] = 1e-5\n        optimizer = optim.RMSprop(model.parameters(), lr=CONFIG['learning_rate'], alpha=CONFIG['learning_rate'], \n                                  momentum=CONFIG['learning_rate'], weight_decay=CONFIG['learning_rate'])\n    else:\n        raise ValueError(\"Invalid Optimizer given!\")\n    return optimizer","metadata":{"execution":{"iopub.status.busy":"2023-12-10T10:25:00.330274Z","iopub.execute_input":"2023-12-10T10:25:00.330700Z","iopub.status.idle":"2023-12-10T10:25:00.346903Z","shell.execute_reply.started":"2023-12-10T10:25:00.330664Z","shell.execute_reply":"2023-12-10T10:25:00.345775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dataloaders_binary(df_train, df_valid, CONFIG, data_transforms,train_val_split=0.9, apply_sampler=False, sample_fac=1, tissue_label_th=0.2):\n    # df_train = df[df[\"kfold\"]!=fold].reset_index(drop=True)\n    train_dataset = LabelledTilesDataset(df_train, transforms=data_transforms[\"train\"], mode=\"train\", train_val_split=train_val_split, tissue_label_th=tissue_label_th)\n    if apply_sampler:\n        samples_weights = train_dataset.get_sample_weights()\n        train_sampler = WeightedRandomSampler(samples_weights, len(samples_weights)*sample_fac)\n        train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], num_workers=2, sampler=train_sampler, shuffle=False, pin_memory=True)\n    else:\n        train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], num_workers=2, sampler=None, shuffle=True, pin_memory=True)\n\n    \n    valid_dataset = LabelledTilesDataset(df_valid, transforms=data_transforms[\"valid\"], mode=\"valid\", train_val_split=train_val_split, tissue_label_th=tissue_label_th)\n    if apply_sampler:\n        samples_weights = valid_dataset.get_sample_weights()\n        valid_sampler = WeightedRandomSampler(samples_weights, len(samples_weights)*sample_fac)\n    else:\n        valid_sampler=None\n    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], num_workers=2, sampler=valid_sampler, shuffle=False, pin_memory=True)\n    print(f\"Len Train Dataset: {len(train_dataset)}, Len Validation Dataset: {len(valid_dataset)}\" )\n    return train_loader, valid_loader","metadata":{"execution":{"iopub.status.busy":"2023-12-10T10:49:58.756753Z","iopub.execute_input":"2023-12-10T10:49:58.757674Z","iopub.status.idle":"2023-12-10T10:49:58.766989Z","shell.execute_reply.started":"2023-12-10T10:49:58.757637Z","shell.execute_reply":"2023-12-10T10:49:58.765810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\"\"\"\ndef get_or_create_experiment_id(name):\n    exp = mlflow.get_experiment_by_name(name)\n    if exp is None:\n        exp_id = mlflow.create_experiment(name)\n        return exp_id\n    return exp.experiment_id\n\"\"\"\nmlflow_experiment_id = get_or_create_experiment_id(\"UBC_binary_tissue_classification\")\nmlflow_experiment_id","metadata":{"execution":{"iopub.status.busy":"2023-12-10T10:29:44.134508Z","iopub.execute_input":"2023-12-10T10:29:44.135377Z","iopub.status.idle":"2023-12-10T10:29:44.314175Z","shell.execute_reply.started":"2023-12-10T10:29:44.135342Z","shell.execute_reply":"2023-12-10T10:29:44.313383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6):\n        super(GeM, self).__init__()\n        self.p = nn.Parameter(torch.ones(1)*p)\n        self.eps = eps\n\n    def forward(self, x):\n        return self.gem(x, p=self.p, eps=self.eps)\n        \n    def gem(self, x, p=3, eps=1e-6):\n        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n        \n    def __repr__(self):\n        return self.__class__.__name__ + \\\n                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n                ', ' + 'eps=' + str(self.eps) + ')'\n\n\nclass UBCBinaryModel(nn.Module):\n    '''\n    EfficientNet B0 fine-tune.\n    '''\n    def __init__(self, model_name, pretrained=False, checkpoint_path=None):\n        '''\n        Fine tune for EfficientNetB0\n        Args\n            learnable_modules : tuple - Names of the modules to fine-tune.\n        Return\n            \n        '''\n        super(UBCBinaryModel, self).__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained, checkpoint_path=checkpoint_path)\n\n        in_features = self.model.classifier.in_features\n        self.model.classifier = nn.Identity()\n        self.model.global_pool = nn.Identity()\n        self.pooling = GeM()\n        self.linear = nn.Linear(in_features, 1)\n        \n\n    def forward(self, images):\n        \"\"\"\n        Forward function for the fine-tuned model\n        Args\n            x: \n        Return\n            result\n        \"\"\"\n        features = self.model(images)\n        pooled_features = self.pooling(features).flatten(1)\n        logits = self.linear(pooled_features)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2023-12-10T10:29:44.327240Z","iopub.execute_input":"2023-12-10T10:29:44.327523Z","iopub.status.idle":"2023-12-10T10:29:44.338775Z","shell.execute_reply.started":"2023-12-10T10:29:44.327498Z","shell.execute_reply":"2023-12-10T10:29:44.337736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CONFIG[\"weighted_loss\"]:\n    pos_weight = 3 # df_train[df_train[\"target_label\"]==0].shape[0] / df_train[df_train[\"target_label\"]==1].shape[0]\n    pos_weight = torch.tensor([pos_weight]).to(CONFIG['device'], dtype=torch.float)  # 'weight' should be a float\n    print(f\"Class weights: {pos_weight}\")\n    CONFIG[\"pos_weight\"] = pos_weight\nelse:\n    pos_weight=None\ncriterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T10:51:58.041566Z","iopub.execute_input":"2023-12-10T10:51:58.042471Z","iopub.status.idle":"2023-12-10T10:51:58.051314Z","shell.execute_reply.started":"2023-12-10T10:51:58.042439Z","shell.execute_reply":"2023-12-10T10:51:58.050313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG[\"num_epochs\"] = 10\n\nprint(f\"Shape df_train: {df_train.shape}, Shape df_test: {df_test.shape}\")\nwith mlflow.start_run(experiment_id=mlflow_experiment_id) as run:\n    train_loader, valid_loader = get_dataloaders_binary(df_train, df_valid, CONFIG, data_transforms, apply_sampler=True, train_val_split=1, tissue_label_th=0.1)\n\n    model = UBCBinaryModel(CONFIG['model_name'], pretrained=False , checkpoint_path=CONFIG[\"checkpoint_path\"])\n    # model.load_state_dict(torch.load(CONFIG[\"checkpoint_path\"]))\n    model.to(CONFIG['device']);\n\n    optimizer = get_optimizer(CONFIG[\"optimizer\"], model)\n    scheduler = fetch_scheduler(optimizer, CONFIG)\n    _, _, _, save_model_path = train_model(model, train_loader, valid_loader, optimizer, criterion, CONFIG[\"device\"], CONFIG[\"num_epochs\"], scheduler)\n    model.load_state_dict(torch.load(save_model_path))\n    \n    print(\"Validate on Holdout Set:\")\n    df_test = test_on_holdout(model, CONFIG, df_test, val_size=1)\n    df_test_file_path = \"df_test_results.csv\"\n    df_test.to_csv(df_test_file_path, index=False)\n    mlflow.log_params(CONFIG)\n    mlflow.pytorch.log_model(model, \"model\")\n    mlflow.log_params({\"model_path\": save_model_path})\n    mlflow.log_artifact(df_test_file_path)\n    print_logged_info(mlflow.get_run(run_id=run.info.run_id))\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T10:50:02.175288Z","iopub.execute_input":"2023-12-10T10:50:02.175659Z","iopub.status.idle":"2023-12-10T10:51:54.936626Z","shell.execute_reply.started":"2023-12-10T10:50:02.175629Z","shell.execute_reply":"2023-12-10T10:51:54.934859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nmodel = UBCModel(CONFIG['model_name'], CONFIG['num_classes'], pretrained=False , checkpoint_path=None)\nmodel.load_state_dict(torch.load(\"/kaggle/input/effnet-version-28/best_model_checkpoint2023-11-21_15-47-39.pth\"))\nmodel.to(CONFIG['device']);\ndf_test = test_on_holdout(model, CONFIG, df_test, TRAIN_DIR, val_size=1, n_tiles=CONFIG[\"n_tiles_test\"])\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-11-24T19:58:16.905615Z","iopub.execute_input":"2023-11-24T19:58:16.906004Z","iopub.status.idle":"2023-11-24T19:58:18.453133Z","shell.execute_reply.started":"2023-11-24T19:58:16.905973Z","shell.execute_reply":"2023-11-24T19:58:18.452382Z"},"trusted":true},"execution_count":null,"outputs":[]}]}