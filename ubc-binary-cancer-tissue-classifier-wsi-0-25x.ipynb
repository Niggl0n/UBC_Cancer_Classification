{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":45867,"databundleVersionId":6924515,"sourceType":"competition"},{"sourceId":6755371,"sourceType":"datasetVersion","datasetId":3888697},{"sourceId":6917177,"sourceType":"datasetVersion","datasetId":3889865},{"sourceId":6984590,"sourceType":"datasetVersion","datasetId":4014175},{"sourceId":7018060,"sourceType":"datasetVersion","datasetId":4035314},{"sourceId":7029230,"sourceType":"datasetVersion","datasetId":4027203},{"sourceId":7156628,"sourceType":"datasetVersion","datasetId":4133077},{"sourceId":154268140,"sourceType":"kernelVersion"}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction \n\n**This is a basic CNN Model training notebook**\n\nIt is based on: \n- Thumbnail images\n- Basic data transformation (using Albumentation):\n    - resizing images to 512x512\n    - normalizing pixel values\n- CNN Architecture\n\n\n**Todos:**\n\n- Learn about Dataset & DataLoader\n- add augmentations (albumentation)\n- gem pooling","metadata":{}},{"cell_type":"code","source":"# !pip install --quiet torch_optimizer\n# import torch_optimizer as torch_optimizer\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T09:40:40.142940Z","iopub.execute_input":"2023-12-13T09:40:40.143796Z","iopub.status.idle":"2023-12-13T09:40:40.147603Z","shell.execute_reply.started":"2023-12-13T09:40:40.143769Z","shell.execute_reply":"2023-12-13T09:40:40.146924Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install --quiet mlflow dagshub\nimport mlflow.pytorch \nfrom mlflow import MlflowClient\nimport dagshub\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T09:40:40.160118Z","iopub.execute_input":"2023-12-13T09:40:40.160358Z","iopub.status.idle":"2023-12-13T09:41:01.743572Z","shell.execute_reply.started":"2023-12-13T09:40:40.160336Z","shell.execute_reply":"2023-12-13T09:41:01.742735Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nydata-profiling 4.3.1 requires dacite>=1.8, but you have dacite 1.6.0 which is incompatible.\nydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\nimport os\nimport gc\nimport cv2\nimport datetime\nimport math\nimport copy\nimport time\nimport random\nimport glob\nfrom matplotlib import pyplot as plt\nfrom skimage import io\n\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.cuda import amp\nimport torchvision\n\nimport optuna\nfrom optuna.trial import TrialState\n\n# Utils\nimport joblib\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\n\nfrom PIL import Image\nfrom joblib import Parallel, delayed\nfrom tqdm.auto import tqdm\n\n# Sklearn Imports\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import balanced_accuracy_score, confusion_matrix, f1_score\nfrom torch.utils.data.sampler import WeightedRandomSampler\n\n# For Image Models\nimport timm\n\nfrom getpass import getpass\n\n# Albumentations for augmentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\nb_ = Fore.BLUE\nsr_ = Style.RESET_ALL\n\nimport warnings\n# warnings.filterwarnings(\"ignore\")\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"execution":{"iopub.status.busy":"2023-12-13T09:48:22.659722Z","iopub.execute_input":"2023-12-13T09:48:22.660101Z","iopub.status.idle":"2023-12-13T09:48:31.786366Z","shell.execute_reply.started":"2023-12-13T09:48:22.660071Z","shell.execute_reply":"2023-12-13T09:48:31.785410Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"from ubc_utils_models import UBCBinaryModel, get_optimizer, fetch_scheduler, EarlyStopping\nfrom ubc_utils_infrastructure import  print_logged_info, get_or_create_experiment_id\n# from ubc_utils_datasets import ","metadata":{"execution":{"iopub.status.busy":"2023-12-13T09:46:42.376950Z","iopub.execute_input":"2023-12-13T09:46:42.377314Z","iopub.status.idle":"2023-12-13T09:46:42.382141Z","shell.execute_reply.started":"2023-12-13T09:46:42.377285Z","shell.execute_reply":"2023-12-13T09:46:42.380965Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"os.environ[\"MLFLOW_TRACKING_USERNAME\"]=\"Niggl0n\"\nos.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"7a3590e8c5558d4598dacc7810befa70a4baac9e\"\nos.environ['MLFLOW_TRACKING_PROJECTNAME'] = \"UBC_Cancer_Classification\"\n#dagshub.auth.add_app_token(\"7a3590e8c5558d4598dacc7810befa70a4baac9e\")\nmlflow.set_tracking_uri(f'https://dagshub.com/' + os.environ['MLFLOW_TRACKING_USERNAME'] + '/' + os.environ['MLFLOW_TRACKING_PROJECTNAME'] + '.mlflow')","metadata":{"execution":{"iopub.status.busy":"2023-12-13T09:41:20.398338Z","iopub.execute_input":"2023-12-13T09:41:20.398671Z","iopub.status.idle":"2023-12-13T09:41:20.404408Z","shell.execute_reply.started":"2023-12-13T09:41:20.398644Z","shell.execute_reply":"2023-12-13T09:41:20.403548Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"CONFIG = {\n    \"is_submission\": False,\n    \"weighted_loss\": True,\n    \"datetime_now\": datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"), \n    \"n_fold\":5, \n    \"test_fold\": 0,\n    \"valid_fold\": 2,\n    \"seed\": 42,\n    \"img_size\": 512,\n    \"model_name\": \"tf_efficientnet_b0_ns\",   # \"tf_efficientnet_b0_ns\", # \"tf_efficientnetv2_s_in21ft1k\"\n    \"checkpoint_path\": \"/kaggle/input/tf-efficientnet-b0-aa-827b6e33-pth/tf_efficientnet_b0_aa-827b6e33.pth\",\n    \"num_classes\": 5,\n    \"train_batch_size\": 8,\n    \"valid_batch_size\": 8,\n    \"n_tiles\": 10,\n    \"n_tiles_test\": 10,\n    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n    \"num_epochs\": 15,\n    \"early_stopping\": True,\n    \"patience\": 6,\n    \"optimizer\": 'adam',\n    \"scheduler\": 'CosineAnnealingLR',\n    \"min_lr\": 1e-6,\n    \"T_max\": 10,\n    \"momentum\": 0.9,\n    \"weight_decay\": 1e-4,\n}","metadata":{"execution":{"iopub.status.busy":"2023-12-13T09:41:20.405698Z","iopub.execute_input":"2023-12-13T09:41:20.406016Z","iopub.status.idle":"2023-12-13T09:41:20.442311Z","shell.execute_reply.started":"2023-12-13T09:41:20.405986Z","shell.execute_reply":"2023-12-13T09:41:20.441455Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## 1. Data Preparation","metadata":{}},{"cell_type":"code","source":"ROOT_DIR = '/kaggle/input/UBC-OCEAN'\nTRAIN_DIR = '/kaggle/input/ubc-ocean-tiles-w-masks-2048px-scale-0-25'\ndf_orig = pd.read_csv(\"/kaggle/input/UBC-OCEAN/train.csv\")\ndf_orig = df_orig.rename(columns={\"label\":\"subtype\"})\ndf_train = pd.read_csv(\"/kaggle/input/df-tiles-025x-cancer-tissue-binary-labels/tiles_labelled_binary_cancer.csv\", index_col=\"Unnamed: 0\")\ndisplay(df_orig.sample(5))\ndisplay(df_train.sample(5))","metadata":{"execution":{"iopub.status.busy":"2023-12-13T09:54:41.319257Z","iopub.execute_input":"2023-12-13T09:54:41.319926Z","iopub.status.idle":"2023-12-13T09:54:41.480255Z","shell.execute_reply.started":"2023-12-13T09:54:41.319892Z","shell.execute_reply":"2023-12-13T09:54:41.479318Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"     image_id subtype  image_width  image_height  is_tma\n294     35953      EC        70726         30286   False\n258     31297      MC        67580         46491   False\n136     16042    HGSC        77563         26032   False\n315     38097    HGSC        78886         27688   False\n480     57162    LGSC        43752         46643   False","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>subtype</th>\n      <th>image_width</th>\n      <th>image_height</th>\n      <th>is_tma</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>294</th>\n      <td>35953</td>\n      <td>EC</td>\n      <td>70726</td>\n      <td>30286</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>258</th>\n      <td>31297</td>\n      <td>MC</td>\n      <td>67580</td>\n      <td>46491</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>136</th>\n      <td>16042</td>\n      <td>HGSC</td>\n      <td>77563</td>\n      <td>26032</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>315</th>\n      <td>38097</td>\n      <td>HGSC</td>\n      <td>78886</td>\n      <td>27688</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>480</th>\n      <td>57162</td>\n      <td>LGSC</td>\n      <td>43752</td>\n      <td>46643</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"                                                                                       image_path  \\\n35982    /kaggle/input/ubc-ocean-tiles-w-masks-2048px-scale-0-25/train_images/46688/00138_3-9.png   \n11427  /kaggle/input/ubc-ocean-tiles-w-masks-2048px-scale-0-25/train_images/39146/00090_11-12.png   \n22626  /kaggle/input/ubc-ocean-tiles-w-masks-2048px-scale-0-25/train_images/15209/00235_15-13.png   \n14117  /kaggle/input/ubc-ocean-tiles-w-masks-2048px-scale-0-25/train_images/39425/00349_17-16.png   \n4580     /kaggle/input/ubc-ocean-tiles-w-masks-2048px-scale-0-25/train_images/6898/00147_13-9.png   \n\n                                                                                       mask_path  \\\n35982    /kaggle/input/ubc-ocean-tiles-w-masks-2048px-scale-0-25/train_masks/46688/00138_3-9.png   \n11427  /kaggle/input/ubc-ocean-tiles-w-masks-2048px-scale-0-25/train_masks/39146/00090_11-12.png   \n22626  /kaggle/input/ubc-ocean-tiles-w-masks-2048px-scale-0-25/train_masks/15209/00235_15-13.png   \n14117  /kaggle/input/ubc-ocean-tiles-w-masks-2048px-scale-0-25/train_masks/39425/00349_17-16.png   \n4580     /kaggle/input/ubc-ocean-tiles-w-masks-2048px-scale-0-25/train_masks/6898/00147_13-9.png   \n\n       cancer_ratio  non_cancer_ratio    label  \n35982      0.276875               0.0   cancer  \n11427      1.000000               0.0   cancer  \n22626      0.337597               0.0   cancer  \n14117      0.000000               0.0  unknown  \n4580       0.000141               0.0   cancer  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_path</th>\n      <th>mask_path</th>\n      <th>cancer_ratio</th>\n      <th>non_cancer_ratio</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>35982</th>\n      <td>/kaggle/input/ubc-ocean-tiles-w-masks-2048px-scale-0-25/train_images/46688/00138_3-9.png</td>\n      <td>/kaggle/input/ubc-ocean-tiles-w-masks-2048px-scale-0-25/train_masks/46688/00138_3-9.png</td>\n      <td>0.276875</td>\n      <td>0.0</td>\n      <td>cancer</td>\n    </tr>\n    <tr>\n      <th>11427</th>\n      <td>/kaggle/input/ubc-ocean-tiles-w-masks-2048px-scale-0-25/train_images/39146/00090_11-12.png</td>\n      <td>/kaggle/input/ubc-ocean-tiles-w-masks-2048px-scale-0-25/train_masks/39146/00090_11-12.png</td>\n      <td>1.000000</td>\n      <td>0.0</td>\n      <td>cancer</td>\n    </tr>\n    <tr>\n      <th>22626</th>\n      <td>/kaggle/input/ubc-ocean-tiles-w-masks-2048px-scale-0-25/train_images/15209/00235_15-13.png</td>\n      <td>/kaggle/input/ubc-ocean-tiles-w-masks-2048px-scale-0-25/train_masks/15209/00235_15-13.png</td>\n      <td>0.337597</td>\n      <td>0.0</td>\n      <td>cancer</td>\n    </tr>\n    <tr>\n      <th>14117</th>\n      <td>/kaggle/input/ubc-ocean-tiles-w-masks-2048px-scale-0-25/train_images/39425/00349_17-16.png</td>\n      <td>/kaggle/input/ubc-ocean-tiles-w-masks-2048px-scale-0-25/train_masks/39425/00349_17-16.png</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>unknown</td>\n    </tr>\n    <tr>\n      <th>4580</th>\n      <td>/kaggle/input/ubc-ocean-tiles-w-masks-2048px-scale-0-25/train_images/6898/00147_13-9.png</td>\n      <td>/kaggle/input/ubc-ocean-tiles-w-masks-2048px-scale-0-25/train_masks/6898/00147_13-9.png</td>\n      <td>0.000141</td>\n      <td>0.0</td>\n      <td>cancer</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=6, shuffle=True, random_state=CONFIG[\"seed\"],)\nfor fold, ( _, val_) in enumerate(skf.split(X=df_orig, y=df_orig.subtype)):\n    df_orig.loc[val_ , \"kfold\"] = int(fold)\ndisplay(df_orig.head())","metadata":{"execution":{"iopub.status.busy":"2023-12-13T09:54:41.569821Z","iopub.execute_input":"2023-12-13T09:54:41.570122Z","iopub.status.idle":"2023-12-13T09:54:41.589491Z","shell.execute_reply.started":"2023-12-13T09:54:41.570097Z","shell.execute_reply":"2023-12-13T09:54:41.588630Z"},"trusted":true},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"   image_id subtype  image_width  image_height  is_tma  kfold\n0         4    HGSC        23785         20008   False    4.0\n1        66    LGSC        48871         48195   False    2.0\n2        91    HGSC         3388          3388    True    5.0\n3       281    LGSC        42309         15545   False    2.0\n4       286      EC        37204         30020   False    2.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>subtype</th>\n      <th>image_width</th>\n      <th>image_height</th>\n      <th>is_tma</th>\n      <th>kfold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>HGSC</td>\n      <td>23785</td>\n      <td>20008</td>\n      <td>False</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>66</td>\n      <td>LGSC</td>\n      <td>48871</td>\n      <td>48195</td>\n      <td>False</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>91</td>\n      <td>HGSC</td>\n      <td>3388</td>\n      <td>3388</td>\n      <td>True</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>281</td>\n      <td>LGSC</td>\n      <td>42309</td>\n      <td>15545</td>\n      <td>False</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>286</td>\n      <td>EC</td>\n      <td>37204</td>\n      <td>30020</td>\n      <td>False</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_train[\"image_id\"] = df_train[\"image_path\"].map(lambda x: int(x.split('/')[-2]))\ndf_train = df_train[~df_train[\"image_path\"].str.lower().str.contains(\"tma\")]\ndf_train = df_train[df_train[\"label\"]!=\"unknown\"].reset_index(drop=True)\ndf_train = pd.merge(df_train, df_orig, on=\"image_id\", how=\"left\")\ndf_train[\"group\"] = df_train[\"label\"] + \"_\" + df_train[\"subtype\"] \n\nencoder = LabelEncoder()\ndf_train['target_label'] = encoder.fit_transform(df_train['label'])\nwith open(\"label_encoder.pkl\", \"wb\") as fp:\n    joblib.dump(encoder, fp)\ndf_train.to_csv(\"df_data_binary_classification.csv\")\n\n# separate train and test dataset\ndf_test = df_train[df_train[\"kfold\"]==CONFIG[\"test_fold\"]].reset_index(drop=True)\ndf_valid = df_train[df_train[\"kfold\"]==CONFIG[\"valid_fold\"]].reset_index(drop=True)\ndf_train = df_train[~df_train[\"kfold\"].isin([CONFIG[\"valid_fold\"],CONFIG[\"test_fold\"]])].reset_index(drop=True)\nprint(f\"Shape df_train: {df_train.shape}, Shape df_test: {df_valid.shape}, Shape df_test: {df_test.shape} \")\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T09:54:41.797195Z","iopub.execute_input":"2023-12-13T09:54:41.797546Z","iopub.status.idle":"2023-12-13T09:54:41.947444Z","shell.execute_reply.started":"2023-12-13T09:54:41.797517Z","shell.execute_reply":"2023-12-13T09:54:41.946482Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Shape df_train: (11613, 13), Shape df_test: (2778, 13), Shape df_test: (3912, 13) \n","output_type":"stream"}]},{"cell_type":"code","source":"display(df_train.subtype.value_counts())\ndisplay(df_valid.subtype.value_counts())\ndisplay(df_test.subtype.value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-12-13T09:41:20.970749Z","iopub.execute_input":"2023-12-13T09:41:20.971063Z","iopub.status.idle":"2023-12-13T09:41:20.986550Z","shell.execute_reply.started":"2023-12-13T09:41:20.971040Z","shell.execute_reply":"2023-12-13T09:41:20.985583Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"subtype\nHGSC    3492\nEC      2403\nMC      2154\nCC      1994\nLGSC    1570\nName: count, dtype: int64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"subtype\nHGSC    995\nEC      533\nLGSC    518\nCC      445\nMC      287\nName: count, dtype: int64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"subtype\nHGSC    2109\nEC      1007\nCC       307\nLGSC     302\nMC       187\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Todos: \n# - weighted based on (non) cancer ratio and/or label \n# - transformations -> histogram matching\n\nclass LabelledTilesDataset(Dataset):\n    def __init__(\n        self,\n        df_data,\n        # path_img_dir: str =  '',\n        transforms = None,\n        mode: str = 'train',\n        labels_lut = None,\n        #white_thr: int = 225,\n        #thr_max_bg: float = 0.2,\n        train_val_split: float = 0.90,\n        tissue_label_th: float = 0.2\n        # n_tiles: int = 1,\n        # tma_weight: float = 1.0,\n    ):\n        # assert os.path.isdir(path_img_dir)\n        #self.path_img_dir = path_img_dir\n        self.transforms = transforms\n        self.mode = mode\n        #self.white_thr = white_thr\n        #self.thr_max_bg = thr_max_bg\n        # self.train_val_split = train_val_split\n        #self.n_tiles = n_tiles\n        #self.tma_weight = tma_weight\n\n        self.data = df_data\n        self.data[\"max_ratio\"] = self.data[[\"cancer_ratio\", \"non_cancer_ratio\"]].max(axis=1)\n        self.data = self.data[self.data[\"max_ratio\"]>tissue_label_th]\n        self.data = self.data.sample(frac=1, random_state=42).reset_index(drop=True)\n\n        self.labels_unique = sorted(self.data[\"label\"].unique())\n        self.labels_lut = labels_lut or {lb: i for i, lb in enumerate(self.labels_unique)}\n\n        # split dataset   # No splitting needed --> commented\n        #assert 0.0 <= self.train_val_split <= 1.0\n        #frac = int(self.train_val_split * len(self.data))\n        # self.data = self.data[:frac] if mode in [\"train\", \"test\"] else self.data[frac:]  \n        self.labels =  np.array(self.data.target_label.values.tolist())\n        self.img_paths =  self.data.image_path.values.tolist()\n        \n        \n        \n        # set sample weights \n        # self.sample_weights = [self.tma_weight if is_tma == True else 1 for is_tma in self.data[\"is_tma\"]] \n        self.sample_weights =  np.array((self.data[\"target_label\"] + 1) * self.data[\"max_ratio\"])\n        \n    def __getitem__(self, idx: int) -> tuple:\n        #nth_iteration = idx//len(self.data)\n        #if self.mode==\"train\":\n        #    random.seed()\n        #else:\n        #    random.seed(CONFIG[\"seed\"]+nth_iteration)\n        #random.shuffle(self.img_dirs[idx])\n        #for img_path in self.img_paths[idx]:\n        img_path = self.img_paths[idx]\n        assert os.path.isfile(img_path), f\"missing: {img_path}\"\n        tile = cv2.imread(img_path)\n        tile = cv2.cvtColor(tile, cv2.COLOR_BGR2RGB)\n        #black_bg = np.sum(tile, axis=2) == 0\n        #tile[black_bg, :] = 255\n        #mask_bg = np.mean(tile, axis=2) > self.white_thr\n        #if np.sum(mask_bg) < (np.prod(mask_bg.shape) * self.thr_max_bg):\n            #self.img_paths.append(img_path)\n            #print(f\"Idx: {idx}, Path: {img_path}, len img_pths: {len(self.img_paths)}, nunique img_paths: {len(set(self.img_paths))}\")\n        #    break\n\n        # augmentation\n        if self.transforms:\n            tile = self.transforms(image=tile)[\"image\"]\n        #print(f\"img dim: {img.shape}\")\n        return {\n            \"image\": tile,\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n               }\n    def __len__(self) -> int:\n        return len(self.img_paths)\n    \n    def get_sample_weights(self):\n        return torch.from_numpy(self.sample_weights).double()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T09:41:20.987871Z","iopub.execute_input":"2023-12-13T09:41:20.988139Z","iopub.status.idle":"2023-12-13T09:41:21.001129Z","shell.execute_reply.started":"2023-12-13T09:41:20.988116Z","shell.execute_reply":"2023-12-13T09:41:21.000165Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"img_color_mean=[0.8661704276539922, 0.7663107094675368, 0.8574260897185548]\nimg_color_std=[0.08670629753900036, 0.11646580094195522, 0.07164169171856792]\n\ndata_transforms = {\n    \"train\": A.Compose([\n        A.Resize(512, 512),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        # A.RandomBrightnessContrast(p=0.75),\n        A.ShiftScaleRotate(p=0.75),\n        A.OneOf([\n        A.GaussNoise(var_limit=[10, 50]),\n        A.GaussianBlur(),\n        A.MotionBlur(),\n        ], p=0.4),\n        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n        A.CoarseDropout(max_holes=5, max_width=int(512* 0.1), max_height=int(512* 0.1),\n        mask_fill_value=0, p=0.5),\n        A.Normalize(img_color_mean, img_color_std), \n        ToTensorV2()], p=1.),\n    \n    \"valid\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.Normalize(img_color_mean, img_color_std), \n        ToTensorV2()], p=1.)\n}\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T09:41:21.002247Z","iopub.execute_input":"2023-12-13T09:41:21.002596Z","iopub.status.idle":"2023-12-13T09:41:21.015619Z","shell.execute_reply.started":"2023-12-13T09:41:21.002566Z","shell.execute_reply":"2023-12-13T09:41:21.014747Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## 3. Training","metadata":{}},{"cell_type":"code","source":"def train_one_epoch(model, train_loader, optimizer, criterion, device, writer, epoch, scheduler=None):\n    if torch.cuda.is_available():\n        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n    model.train()\n    train_loss = 0.0\n    bar = tqdm(enumerate(train_loader), total=len(train_loader))\n    for step, data in bar:\n        images = data['image'].to(device, dtype=torch.float)\n        labels = data['label'].to(device, dtype=torch.float).unsqueeze(1)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * images.size(0)\n        writer.add_scalar('loss/train_batch', loss.item(), epoch * len(train_loader) + step)\n    \n    # Update learning rate using the scheduler\n    if scheduler:\n        scheduler.step()\n    train_loss /= len(train_loader.dataset)\n    # Log the average training loss for the epoch to TensorBoard\n    writer.add_scalar('loss/train_epoch', train_loss, epoch)\n    # gc.collect()\n    return train_loss\n\ndef validate_one_epoch(model, valid_loader, criterion, device, writer, epoch):\n    model.eval()\n    valid_loss = 0.0\n    valid_acc = 0.0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        bar_val = tqdm(enumerate(valid_loader), total=len(valid_loader))\n        for step, data in bar_val:\n            images = data['image'].to(device, dtype=torch.float)\n            labels = data['label'].to(device, dtype=torch.float).unsqueeze(1)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n        \n            valid_loss += loss.item() * images.size(0)\n\n            # Applying sigmoid to the model outputs\n            probs = torch.sigmoid(outputs)\n            # Thresholding to get binary predictions\n            predicted = (probs > 0.5).float()\n\n            acc = torch.sum(predicted == labels).item() / images.size(0)\n            valid_acc += acc\n            \n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n        \n            writer.add_scalar('loss/valid_batch', loss.item(), epoch * len(valid_loader) + step)\n            writer.add_scalar('acc/valid_batch', acc, epoch * len(valid_loader) + step)\n    valid_loss /= len(valid_loader.dataset)\n    valid_acc /= len(valid_loader)\n    bal_acc = balanced_accuracy_score(all_labels, all_preds)\n    # At the end of your validation loop:\n    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n    micro_f1 = f1_score(all_labels, all_preds, average='micro')\n    weighted_f1 = f1_score(all_labels, all_preds, average='weighted')\n\n    # Logging to TensorBoard\n    writer.add_scalar('loss/val_epoch', valid_loss, epoch)\n    writer.add_scalar('acc/val_epoch', valid_acc, epoch)\n    writer.add_scalar('balanced_acc/val_epoch', bal_acc, epoch)\n    writer.add_scalar('F1/macro', macro_f1, epoch)\n    writer.add_scalar('F1/micro', micro_f1, epoch)\n    writer.add_scalar('F1/weighted', weighted_f1, epoch)\n    # in order to put multiple lines within one graph\n    #writer.add_scalars('run_14h', {'xsinx':i*np.sin(i/r),\n    #                        'xcosx':i*np.cos(i/r),\n    #                        'tanx': np.tan(i/r)}, i)\n    return valid_loss, valid_acc, bal_acc, weighted_f1\n\ndef train_model(model, train_loader, valid_loader, optimizer, criterion, device, num_epochs, scheduler, save_model_path=None):\n    model_name = \"model_epochs\" + str(CONFIG[\"num_epochs\"]) + \"_bs\"+str(CONFIG[\"train_batch_size\"] )+ \"_opt\" +CONFIG[\"optimizer\"]+ \"_sched\" + CONFIG[\"scheduler\"] + \"_lr\"+str(CONFIG[\"learning_rate\"])+ \"_wd\" + str(CONFIG[\"weight_decay\"])\n    print(f\"Training model: {model_name}\")\n    datetime_now =  datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n    if not save_model_path:\n        save_model_path = 'best_model_checkpoint' + datetime_now + '.pth'\n    print(f\"Path for saving model: {save_model_path}\")\n    # Initialize TensorBoard writer\n    writer = SummaryWriter('logs/fit/' + model_name)\n    early_stopping = EarlyStopping(patience=CONFIG[\"patience\"], verbose=True, path=save_model_path)\n    \n    for epoch in range(num_epochs):\n        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, writer, epoch, scheduler)\n        valid_loss, valid_acc, bal_acc, weighted_f1 = validate_one_epoch(model, valid_loader, criterion, device, writer, epoch)\n        print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {train_loss:.4f}, Validation loss: {valid_loss:.4f}, Validation acc: {valid_acc:.4f}, Balanced acc: {bal_acc:.4f}, Weighted F1-Score: {weighted_f1:.4f}\")\n\n        if CONFIG[\"early_stopping\"]:\n            early_stopping(valid_loss, model)\n            if early_stopping.early_stop:\n                print(\"Early stopping\")\n                break\n        writer.close()\n\n        mlflow.log_metrics({\n            'epoch': epoch,\n            'train_loss': train_loss,\n            'valid_loss': valid_loss,\n            'valid_acc': valid_acc,\n            'balanced_acc': bal_acc,\n            'weighted_f1': weighted_f1\n        }, step=epoch)\n    return train_loss, valid_loss, valid_acc, save_model_path\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T09:41:21.016969Z","iopub.execute_input":"2023-12-13T09:41:21.017342Z","iopub.status.idle":"2023-12-13T09:41:21.043481Z","shell.execute_reply.started":"2023-12-13T09:41:21.017310Z","shell.execute_reply":"2023-12-13T09:41:21.042643Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def test_on_holdout(model, CONFIG, df_test, val_size=1.0):\n    if not CONFIG[\"is_submission\"]:\n        model.eval()\n        test_dataset = LabelledTilesDataset(df_test, transforms=data_transforms[\"valid\"], mode=\"test\", train_val_split=1.0)\n        test_loader = DataLoader(test_dataset, batch_size=CONFIG['valid_batch_size'], \n                                  num_workers=2, shuffle=False, pin_memory=True)\n        print(f\"Test-Dataset Size: {len(test_dataset)}\")\n\n        preds = []\n        labels_list = []\n        test_acc = 0.0\n\n        with torch.no_grad():\n            bar = tqdm(enumerate(test_loader), total=len(test_loader))\n            for step, data in bar: \n                # print(step)\n                images = data['image'].to(CONFIG[\"device\"], dtype=torch.float)\n                labels = data['label'].to(CONFIG[\"device\"], dtype=torch.float).unsqueeze(1)\n\n                outputs = model(images)        \n                probs = torch.sigmoid(outputs)\n                predicted = (probs > 0.5).int()\n                preds.append(predicted.detach().cpu().numpy() )\n                labels_list.append(labels.detach().cpu().numpy() )\n                acc = torch.sum(predicted == labels).item() / images.size(0)\n                test_acc  += acc       \n                \n        test_acc /= len(test_loader) \n        preds = np.concatenate(preds).astype(int).flatten()\n        labels_list = np.concatenate(labels_list).flatten()\n        pred_labels = encoder.inverse_transform(preds)\n        \n        # Calculate Balanced Accuracy\n        bal_acc = balanced_accuracy_score(labels_list, preds)\n        # Calculate Confusion Matrix\n        conf_matrix = confusion_matrix(labels_list, preds)\n        macro_f1 = f1_score(labels_list, preds, average='macro')\n\n    \n        print(f\"Test Accuracy: {test_acc}\")\n        print(f\"Balanced Accuracy: {bal_acc}\")\n        print(f\"Confusion Matrix: {conf_matrix}\")\n        mlflow.log_metrics({\n            'test_acc': test_acc,\n            'test_balanced_acc': bal_acc,\n            'test_f1_score': macro_f1,\n        })\n        return df_test\n    else:\n        print(\"Skip validation on training set due to submission!\")\n        return None","metadata":{"execution":{"iopub.status.busy":"2023-12-13T09:41:21.044922Z","iopub.execute_input":"2023-12-13T09:41:21.045248Z","iopub.status.idle":"2023-12-13T09:41:21.057491Z","shell.execute_reply.started":"2023-12-13T09:41:21.045218Z","shell.execute_reply":"2023-12-13T09:41:21.056623Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def get_dataloaders_binary(df_train, df_valid, CONFIG, data_transforms,train_val_split=0.9, apply_sampler=False, sample_fac=1, tissue_label_th=0.2):\n    # df_train = df[df[\"kfold\"]!=fold].reset_index(drop=True)\n    train_dataset = LabelledTilesDataset(df_train, transforms=data_transforms[\"train\"], mode=\"train\", train_val_split=train_val_split, tissue_label_th=tissue_label_th)\n    if apply_sampler:\n        samples_weights = train_dataset.get_sample_weights()\n        train_sampler = WeightedRandomSampler(samples_weights, len(samples_weights)*sample_fac)\n        train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], num_workers=2, sampler=train_sampler, shuffle=False, pin_memory=True)\n    else:\n        train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], num_workers=2, sampler=None, shuffle=True, pin_memory=True)\n\n    \n    valid_dataset = LabelledTilesDataset(df_valid, transforms=data_transforms[\"valid\"], mode=\"valid\", train_val_split=train_val_split, tissue_label_th=tissue_label_th)\n    if apply_sampler:\n        samples_weights = valid_dataset.get_sample_weights()\n        valid_sampler = WeightedRandomSampler(samples_weights, len(samples_weights)*sample_fac)\n    else:\n        valid_sampler=None\n    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], num_workers=2, sampler=valid_sampler, shuffle=False, pin_memory=True)\n    print(f\"Len Train Dataset: {len(train_dataset)}, Len Validation Dataset: {len(valid_dataset)}\" )\n    return train_loader, valid_loader","metadata":{"execution":{"iopub.status.busy":"2023-12-13T09:41:21.058911Z","iopub.execute_input":"2023-12-13T09:41:21.059345Z","iopub.status.idle":"2023-12-13T09:41:21.070446Z","shell.execute_reply.started":"2023-12-13T09:41:21.059314Z","shell.execute_reply":"2023-12-13T09:41:21.069620Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"if CONFIG[\"weighted_loss\"]:\n    pos_weight = 3 # df_train[df_train[\"target_label\"]==0].shape[0] / df_train[df_train[\"target_label\"]==1].shape[0]\n    pos_weight = torch.tensor([pos_weight]).to(CONFIG['device'], dtype=torch.float)  # 'weight' should be a float\n    print(f\"Class weights: {pos_weight}\")\n    CONFIG[\"pos_weight\"] = pos_weight\nelse:\n    pos_weight=None\ncriterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n\n\nmlflow_experiment_id = get_or_create_experiment_id(\"UBC_binary_tissue_classification\")\nmlflow_experiment_id","metadata":{"execution":{"iopub.status.busy":"2023-12-13T09:41:21.071535Z","iopub.execute_input":"2023-12-13T09:41:21.071897Z","iopub.status.idle":"2023-12-13T09:41:24.265836Z","shell.execute_reply.started":"2023-12-13T09:41:21.071866Z","shell.execute_reply":"2023-12-13T09:41:24.264131Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Class weights: tensor([3.], device='cuda:0')\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'6'"},"metadata":{}}]},{"cell_type":"code","source":"def get_optimizer(optimizer_name, model, CONFIG):\n    if optimizer_name.lower() == \"adam\":\n        CONFIG['learning_rate'] = 1e-4\n        CONFIG['weight_decay'] = 1e-5\n        CONFIG['betas'] = (0.9, 0.999)\n        CONFIG['eps'] = 1e-8\n        optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'], betas=CONFIG['betas'], eps=CONFIG['eps'],  weight_decay=CONFIG['weight_decay'])\n    elif optimizer_name.lower() == \"sgd\":\n        CONFIG['learning_rate'] = 1e-3\n        CONFIG['weight_decay'] = 1e-3\n        CONFIG['momentum'] = 1e-3\n        optimizer = optim.SGD(model.parameters(), lr=CONFIG['learning_rate'], momentum=CONFIG['momentum'], weight_decay=CONFIG['weight_decay'])\n    elif optimizer_name.lower() == \"radam\":\n        CONFIG['learning_rate'] = 1e-4\n        CONFIG['weight_decay'] = 0\n        CONFIG['betas'] = (0.9, 0.999)\n        CONFIG['eps'] = 1e-8\n        optimizer = torch_optimizer.RAdam(\n            model.parameters(),\n            lr= CONFIG['learning_rate'],\n            betas=CONFIG['betas'],\n            eps=CONFIG['eps'],\n            weight_decay=CONFIG['weight_decay'],\n        )\n    elif optimizer_name.lower() == \"rmsprop\":\n        CONFIG['learning_rate'] = 0.256\n        CONFIG['alpha'] = 0.9\n        CONFIG['momentum'] = 0.9\n        CONFIG['weight_decay'] = 1e-5\n        optimizer = optim.RMSprop(model.parameters(), lr=CONFIG['learning_rate'], alpha=CONFIG['learning_rate'], \n                                  momentum=CONFIG['learning_rate'], weight_decay=CONFIG['learning_rate'])\n    else:\n        raise ValueError(\"Invalid Optimizer given!\")\n    return optimizer","metadata":{"execution":{"iopub.status.busy":"2023-12-13T09:47:53.280116Z","iopub.execute_input":"2023-12-13T09:47:53.280477Z","iopub.status.idle":"2023-12-13T09:47:53.291163Z","shell.execute_reply.started":"2023-12-13T09:47:53.280448Z","shell.execute_reply":"2023-12-13T09:47:53.290158Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"CONFIG[\"num_epochs\"] = 1\nprint(f\"Shape df_train: {df_train.shape}, Shape df_test: {df_test.shape}\")\nwith mlflow.start_run(experiment_id=mlflow_experiment_id) as run:\n    train_loader, valid_loader = get_dataloaders_binary(df_train.head(100), df_valid.head(100), CONFIG, data_transforms, apply_sampler=True, train_val_split=1, tissue_label_th=0.1)\n\n    model = UBCBinaryModel(CONFIG['model_name'], pretrained=False , checkpoint_path=CONFIG[\"checkpoint_path\"])\n    # model.load_state_dict(torch.load(CONFIG[\"checkpoint_path\"]))\n    model.to(CONFIG['device']);\n\n    optimizer = get_optimizer(CONFIG[\"optimizer\"], model, CONFIG)\n    scheduler = fetch_scheduler(optimizer, CONFIG)\n    _, _, _, save_model_path = train_model(model, train_loader, valid_loader, optimizer, criterion, CONFIG[\"device\"], CONFIG[\"num_epochs\"], scheduler)\n    model.load_state_dict(torch.load(save_model_path))\n    \n    print(\"Validate on Holdout Set:\")\n    df_test = test_on_holdout(model, CONFIG, df_test.head(), val_size=1)\n    df_test_file_path = \"df_test_binary_classification.csv\"\n    df_test.to_csv(df_test_file_path, index=False)\n    mlflow.log_params(CONFIG)\n    mlflow.pytorch.log_model(model, \"model\")\n    mlflow.log_params({\"model_path\": save_model_path})\n    mlflow.log_artifact(df_test_file_path)\n    print_logged_info(mlflow.get_run(run_id=run.info.run_id))\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T09:53:20.124804Z","iopub.execute_input":"2023-12-13T09:53:20.125649Z","iopub.status.idle":"2023-12-13T09:53:42.795388Z","shell.execute_reply.started":"2023-12-13T09:53:20.125606Z","shell.execute_reply":"2023-12-13T09:53:42.793923Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Shape df_train: (11613, 14), Shape df_test: (3912, 14)\nLen Train Dataset: 78, Len Validation Dataset: 76\nTraining model: model_epochs1_bs8_optadam_schedCosineAnnealingLR_lr0.0001_wd1e-05\nPath for saving model: best_model_checkpoint2023-12-13_09-53-20.pth\n[INFO] Using GPU: Tesla P100-PCIE-16GB\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5fe2b486e8e47ee93f8f88741510af3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4582505c097d4f5fa4c210ab6a9e48de"}},"metadata":{}},{"name":"stdout","text":"Epoch 1/1 - Train loss: 0.6095, Validation loss: 1.4068, Validation acc: 0.6125, Balanced acc: 0.5000, Weighted F1-Score: 0.4726\nValidation loss decreased (inf --> 1.406800). Saving model ...\nValidate on Holdout Set:\nTest-Dataset Size: 3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d346b2f9c761455aa1470eed15b91921"}},"metadata":{}},{"name":"stdout","text":"Test Accuracy: 1.0\nBalanced Accuracy: 1.0\nConfusion Matrix: [[3]]\nrun_id: edefe2990529476b916427bd36e7aa27\nartifacts: ['model/MLmodel', 'model/conda.yaml', 'model/data', 'model/python_env.yaml', 'model/requirements.txt']\nparams: {'model_path': 'best_model_checkpoint2023-12-13_09-53-20.pth', 'is_submission': 'False', 'weighted_loss': 'True', 'datetime_now': '2023-12-13_09-41-20', 'n_fold': '5', 'test_fold': '0', 'valid_fold': '2', 'seed': '42', 'img_size': '512', 'model_name': 'tf_efficientnet_b0_ns', 'checkpoint_path': '/kaggle/input/tf-efficientnet-b0-aa-827b6e33-pth/tf_efficientnet_b0_aa-827b6e33.pth', 'num_classes': '5', 'train_batch_size': '8', 'valid_batch_size': '8', 'n_tiles': '10', 'n_tiles_test': '10', 'device': 'cuda:0', 'num_epochs': '1', 'early_stopping': 'True', 'patience': '6', 'optimizer': 'adam', 'scheduler': 'CosineAnnealingLR', 'min_lr': '1e-06', 'T_max': '10', 'momentum': '0.9', 'weight_decay': '1e-05', 'pos_weight': \"tensor([3.], device='cuda:0')\", 'learning_rate': '0.0001', 'betas': '(0.9, 0.999)', 'eps': '1e-08'}\nmetrics: {'epoch': 0.0, 'train_loss': 0.609511453371782, 'valid_loss': 1.40679957992152, 'valid_acc': 0.6125, 'balanced_acc': 0.5, 'weighted_f1': 0.472614462986735, 'test_acc': 1.0, 'test_balanced_acc': 1.0, 'test_f1_score': 1.0}\ntags: {}\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[29], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m print_logged_info(mlflow\u001b[38;5;241m.\u001b[39mget_run(run_id\u001b[38;5;241m=\u001b[39mrun\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mrun_id))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCONFIG.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m---> 27\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mdump(CONFIG, fp)  \u001b[38;5;66;03m# encode dict into JSON\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"],"ename":"NameError","evalue":"name 'json' is not defined","output_type":"error"}]},{"cell_type":"code","source":"\"\"\"\nmodel = UBCModel(CONFIG['model_name'], CONFIG['num_classes'], pretrained=False , checkpoint_path=None)\nmodel.load_state_dict(torch.load(\"/kaggle/input/effnet-version-28/best_model_checkpoint2023-11-21_15-47-39.pth\"))\nmodel.to(CONFIG['device']);\ndf_test = test_on_holdout(model, CONFIG, df_test, TRAIN_DIR, val_size=1, n_tiles=CONFIG[\"n_tiles_test\"])\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-12-13T09:41:25.636492Z","iopub.status.idle":"2023-12-13T09:41:25.636851Z","shell.execute_reply.started":"2023-12-13T09:41:25.636688Z","shell.execute_reply":"2023-12-13T09:41:25.636704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try: \n    del CONFIG[\"device\"], CONFIG[\"pos_weight\"]\nexcept KeyError:\n    pass\nwith open(\"CONFIG.pkl\", \"wb\") as fp:\n    joblib.dump(CONFIG, fp)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T10:13:32.039099Z","iopub.execute_input":"2023-12-13T10:13:32.039451Z","iopub.status.idle":"2023-12-13T10:13:32.046006Z","shell.execute_reply.started":"2023-12-13T10:13:32.039421Z","shell.execute_reply":"2023-12-13T10:13:32.045112Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"CONFIG","metadata":{"execution":{"iopub.status.busy":"2023-12-13T10:13:15.847057Z","iopub.execute_input":"2023-12-13T10:13:15.847752Z","iopub.status.idle":"2023-12-13T10:13:15.855751Z","shell.execute_reply.started":"2023-12-13T10:13:15.847718Z","shell.execute_reply":"2023-12-13T10:13:15.854908Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"{'is_submission': False,\n 'weighted_loss': True,\n 'datetime_now': '2023-12-13_09-41-20',\n 'n_fold': 5,\n 'test_fold': 0,\n 'valid_fold': 2,\n 'seed': 42,\n 'img_size': 512,\n 'model_name': 'tf_efficientnet_b0_ns',\n 'checkpoint_path': '/kaggle/input/tf-efficientnet-b0-aa-827b6e33-pth/tf_efficientnet_b0_aa-827b6e33.pth',\n 'num_classes': 5,\n 'train_batch_size': 8,\n 'valid_batch_size': 8,\n 'n_tiles': 10,\n 'n_tiles_test': 10,\n 'num_epochs': 1,\n 'early_stopping': True,\n 'patience': 6,\n 'optimizer': 'adam',\n 'scheduler': 'CosineAnnealingLR',\n 'min_lr': 1e-06,\n 'T_max': 10,\n 'momentum': 0.9,\n 'weight_decay': 1e-05,\n 'pos_weight': tensor([3.], device='cuda:0'),\n 'learning_rate': 0.0001,\n 'betas': (0.9, 0.999),\n 'eps': 1e-08}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}