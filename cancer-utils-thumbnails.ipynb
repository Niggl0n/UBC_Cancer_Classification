{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":45867,"databundleVersionId":6924515,"sourceType":"competition"}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Standard library imports\nimport copy\nimport datetime\nimport gc\nimport glob\nimport heapq\nimport joblib\nimport math\nimport os\nimport random\nimport time\nfrom collections import defaultdict\nfrom itertools import chain\n\n# Related third-party imports\nimport albumentations as A\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nfrom albumentations.pytorch import ToTensorV2\nfrom colorama import Fore, Back, Style\nfrom joblib import Parallel, delayed\nfrom matplotlib import pyplot as plt\nfrom PIL import Image\nfrom skimage import io\nfrom sklearn.metrics import balanced_accuracy_score, confusion_matrix, f1_score, accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.cuda import amp\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm, tqdm_notebook\n\nfrom torch.utils.data.sampler import WeightedRandomSampler\n\n\n# Local application/library specific imports\n# (Your local imports here, if any)\n\n# Set up for colored terminal text\nb_ = Fore.BLUE\nsr_ = Style.RESET_ALL\n\n# Disable warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Additional note: The following import seems to be from a very specific context or not typically used.\n# You might want to review if it's necessary or correct:\n# from joblib.externals.loky.backend.context import get_context\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-22T13:54:42.912671Z","iopub.execute_input":"2023-12-22T13:54:42.913382Z","iopub.status.idle":"2023-12-22T13:54:50.636940Z","shell.execute_reply.started":"2023-12-22T13:54:42.913340Z","shell.execute_reply":"2023-12-22T13:54:50.635719Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## 1. Datasets & Preprocessing","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedGroupKFold\n\ndef create_data(WSI_DIRS, TMA_DIR, CONFIG, cancer_th=None):\n    df_orig = pd.read_csv(\"/kaggle/input/UBC-OCEAN/train.csv\")\n    df_orig = df_orig.rename(columns={\"label\":\"subtype\"})\n    \n    df_masks = create_img_dataframe_from_folder(\"/kaggle/input/ubc-ovarian-cancer-competition-supplemental-masks/\")\n\n    dfs = []\n    cols = ['image_path', 'image_id', 'subtype', 'image_width', 'image_height', 'is_tma']\n    for dir_ in WSI_DIRS:\n        df = create_img_dataframe_from_directory(dir_)\n        df[\"image_id\"] = df[\"image_id\"].astype(int)\n        df = pd.merge(df, df_orig, on=\"image_id\", how=\"left\")\n        df = df[df[\"is_tma\"]!=True]\n        dfs.append(df[cols])\n    \n    df = create_img_dataframe_from_directory(TMA_DIR)\n    df[\"image_id\"] = df[\"image_id\"].astype(int)\n    df = pd.merge(df, df_orig, on=\"image_id\", how=\"left\")\n    df = df[df[\"is_tma\"]==True]\n    dfs.append(df[cols])\n    \n    df_train = pd.concat(dfs, axis=0, ignore_index=True)\n    df_train = df_train.rename(columns={\"subtype\":\"label\"})\n    print(df_train.shape, df_train.image_id.nunique())\n\n    encoder = LabelEncoder()\n    df_train['target_label'] = encoder.fit_transform(df_train['label'])\n    with open(\"label_encoder.pkl\", \"wb\") as fp:\n        joblib.dump(encoder, fp)\n    \n    # use stratified K Fold for crossvalidation \n    sgkf = StratifiedGroupKFold(n_splits=CONFIG['n_fold'], shuffle=True, random_state=CONFIG[\"seed\"])\n\n    for fold, ( _, val_) in enumerate(sgkf.split(X=df_train, y=df_train.target_label, groups=df_train.image_id.values)):\n        df_train.loc[val_ , \"kfold\"] = int(fold)\n    display(df_train.head())\n\n    # assert that images for which we have masks are not part of test set (avoid information leakage)\n    df_train.loc[df_train[\"image_id\"].isin(df_masks[\"image_id\"]), \"kfold\"] = CONFIG[\"n_fold\"] + 1\n    display(df_train[\"kfold\"].value_counts())\n    # separate train and test dataset\n    df_test = df_train[df_train[\"kfold\"]==CONFIG[\"test_fold\"]].reset_index(drop=True)\n    df_train = df_train[df_train[\"kfold\"]!=CONFIG[\"test_fold\"]].reset_index(drop=True)\n    print(f\"Shape df_train: {df_train.shape}, Shape df_test: {df_test.shape} \")\n    display(df_train.label.value_counts())\n    display(df_test.label.value_counts())\n    df_test.image_id.nunique()\n    return df_train, df_test, encoder, df_orig, df_masks","metadata":{"execution":{"iopub.status.busy":"2023-12-22T13:54:41.434316Z","iopub.execute_input":"2023-12-22T13:54:41.434685Z","iopub.status.idle":"2023-12-22T13:54:42.907515Z","shell.execute_reply.started":"2023-12-22T13:54:41.434658Z","shell.execute_reply":"2023-12-22T13:54:42.906190Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class UBCDataset(Dataset):\n    def __init__(self, df, transforms=None, apply_vertical_crop=True):\n        self.df = df\n        self.filenames = df.file_path.values\n        self.labels =  df.target_label.values\n        self.transforms = transforms\n        self.apply_vertical_crop = apply_vertical_crop\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_path = self.filenames[idx]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if self.apply_vertical_crop:\n            img = crop_vertical(img)\n                \n        if self.transforms:\n            img = self.transforms(image=img)[\"image\"]\n            \n        return {\n            \"image\": img,\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long)\n               }\n\ndef crop_vertical(image):\n    \"\"\"\n    Function crops images if multiple slices contained and separated by black vertical background.\n    \"\"\"\n    vertical_sum = np.sum(image, axis=(0, 2))\n\n    # Identify the positions where the sum is zero\n    zero_positions = np.where(vertical_sum == 0)[0]\n\n    if len(zero_positions)==0:\n        cropped_images = [image]\n    else:\n        # If the image does not start with a black area, add index 0\n        if zero_positions[0] != 0:\n            zero_positions = np.insert(zero_positions, 0, 0)\n\n        # If the image does not end with a black area, add the image width\n        if zero_positions[-1] != image.shape[1] - 1:\n            zero_positions = np.append(zero_positions, image.shape[1] - 1)\n\n        start_idx = zero_positions[0]\n        cropped_images = []\n\n        for idx in range(1, len(zero_positions)):\n            end_idx = zero_positions[idx]\n            if end_idx - start_idx > 1:  # If the width of the cropped section is greater than 1\n                cropped = image[:, start_idx:end_idx]\n                # only include samples which are of min size\n                if cropped.shape[1]>200:  \n                    cropped_images.append(cropped)\n                    # cv2.imwrite(f\"{save_prefix}_{idx}.jpg\", cropped)\n            start_idx = end_idx\n\n    final_crops = []\n    # remove black bars above/below the crops \n    for cropped in cropped_images:\n        horizontal_sum = np.sum(cropped, axis=(1, 2))\n        zero_positions = np.where(horizontal_sum == 0)[0]\n        img_ = np.delete(cropped, zero_positions, axis=0)\n        final_crops.append(img_)\n    if len(final_crops)==0:\n        return image\n    return final_crops[0]\n\n\ndef custom_center_crop_or_resize(image, crop_size):\n    # If both dimensions of the image are greater than or equal to the desired size, apply CenterCrop\n    if image.shape[0] >= crop_size[0] and image.shape[1] >= crop_size[1]:\n        return A.CenterCrop(crop_size[0], crop_size[1])(image=image)[\"image\"]\n    # Else, just resize the image to the desired size\n    else:\n        return A.Resize(crop_size[0], crop_size[1])(image=image)[\"image\"]","metadata":{"execution":{"iopub.status.busy":"2023-12-22T13:54:50.638543Z","iopub.execute_input":"2023-12-22T13:54:50.639126Z","iopub.status.idle":"2023-12-22T13:54:50.654508Z","shell.execute_reply.started":"2023-12-22T13:54:50.639090Z","shell.execute_reply":"2023-12-22T13:54:50.653449Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def _color_means(img_path):\n    img = np.array(Image.open(img_path))\n    mask = np.sum(img[..., :3], axis=2) == 0\n    img[mask, :] = 255\n    if np.max(img) > 1.5:\n        img = img / 255.0\n    clr_mean = {i: np.mean(img[..., i]) for i in range(3)}\n    clr_std = {i: np.std(img[..., i]) for i in range(3)}\n    return clr_mean, clr_std\n\n\"\"\"\nls_images = glob.glob(os.path.join(TRAIN_DIR, \"*\", \"*.png\"))\nclr_mean_std = Parallel(n_jobs=os.cpu_count())(delayed(_color_means)(fn) for fn in tqdm(ls_images[:9000]))\n\nimg_color_mean = pd.DataFrame([c[0] for c in clr_mean_std]).describe()\ndisplay(img_color_mean.T)\nimg_color_std = pd.DataFrame([c[1] for c in clr_mean_std]).describe()\ndisplay(img_color_std.T)\n\nimg_color_mean = list(img_color_mean.T[\"mean\"])\nimg_color_std = list(img_color_std.T[\"mean\"])\nprint(f\"{img_color_mean=}\\n{img_color_std=}\")\n\"\"\"\n\n## histogram matching \n#from skimage.exposure import match_histograms\n#ref_img = np.array(Image.open(\"/kaggle/input/tiles-of-cancer-2048px-scale-0-25/10077/000067_16-3.png\"))\n#bef_img = np.array(Image.open(\"/kaggle/input/tiles-of-cancer-2048px-scale-0-25/12522/000028_6-2.png\"))\n#start = time.time()\n#aft_img = match_histograms(bef_img, ref_img, channel_axis=-1)\n#print(time.time()-start)\n\n\n\"\"\"        \nA.Normalize(\n    mean=[0.485, 0.456, 0.406], \n    std=[0.229, 0.224, 0.225], \n    max_pixel_value=255.0, \n    p=1.0\n),\n\"\"\"\n","metadata":{"execution":{"iopub.status.busy":"2023-12-22T13:54:50.656878Z","iopub.execute_input":"2023-12-22T13:54:50.657315Z","iopub.status.idle":"2023-12-22T13:54:50.676121Z","shell.execute_reply.started":"2023-12-22T13:54:50.657277Z","shell.execute_reply":"2023-12-22T13:54:50.674642Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'        \\nA.Normalize(\\n    mean=[0.485, 0.456, 0.406], \\n    std=[0.229, 0.224, 0.225], \\n    max_pixel_value=255.0, \\n    p=1.0\\n),\\n'"},"metadata":{}}]},{"cell_type":"code","source":"img_color_mean=[0.8661704276539922, 0.7663107094675368, 0.8574260897185548]\nimg_color_std=[0.08670629753900036, 0.11646580094195522, 0.07164169171856792]\n","metadata":{"execution":{"iopub.status.busy":"2023-12-22T13:54:50.677385Z","iopub.execute_input":"2023-12-22T13:54:50.678424Z","iopub.status.idle":"2023-12-22T13:54:50.688043Z","shell.execute_reply.started":"2023-12-22T13:54:50.678386Z","shell.execute_reply":"2023-12-22T13:54:50.686973Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class CancerTilesDataset(Dataset):\n    @staticmethod\n    def get_img_dir(data_row):\n        # based on if is_tma or not we select the respective image path\n        if data_row.is_tma == True:\n            return glob.glob(os.path.join(\"/kaggle/input/ubc-tma-tiles-512-05scale/UBC_TMA_tiles_1024p_scale05\", str(data_row.image_id), \"*.png\"))\n        else:\n            return glob.glob(os.path.join(\"/kaggle/input/tiles-of-cancer-2048px-scale-0-25\", str(data_row.image_id), \"*.png\")) \n\n    def __init__(\n        self,\n        df_data,\n        path_img_dir: str =  '',\n        transforms = None,\n        mode: str = 'train',\n        labels_lut = None,\n        white_thr: int = 225,\n        thr_max_bg: float = 0.2,\n        train_val_split: float = 0.90,\n        n_tiles: int = 1,\n        tma_weight: float = 1.0,\n    ):\n        assert os.path.isdir(path_img_dir)\n        self.path_img_dir = path_img_dir\n        self.transforms = transforms\n        self.mode = mode\n        self.white_thr = white_thr\n        self.thr_max_bg = thr_max_bg\n        self.train_val_split = train_val_split\n        self.n_tiles = n_tiles\n        self.tma_weight = tma_weight\n\n        self.data = df_data\n        self.labels_unique = sorted(self.data[\"label\"].unique())\n        self.labels_lut = labels_lut or {lb: i for i, lb in enumerate(self.labels_unique)}\n\n        self.data.is_tma = self.data.is_tma.astype(bool)\n        self.data = self.data.sample(frac=1, random_state=42).reset_index(drop=True)\n\n        # split dataset\n        assert 0.0 <= self.train_val_split <= 1.0\n        frac = int(self.train_val_split * len(self.data))\n        self.data = self.data[:frac] if mode in [\"train\", \"test\"] else self.data[frac:]\n        self.img_dirs = [CancerTilesDataset.get_img_dir(row) for i, row in self.data.iterrows()] \n        self.img_dirs = self.img_dirs * self.n_tiles\n        self.img_paths = []\n        #print(f\"missing: {sum([not os.path.isfile(os.path.join(self.path_img_dir, im))\n        #                       for im in self.img_names])}\")\n        # self.labels = list(self.data['label'])\n        self.labels =  np.array(self.data.target_label.values.tolist() * self.n_tiles)\n        \n        # set sample weights \n        self.sample_weights = [self.tma_weight if is_tma == True else 1 for is_tma in self.data[\"is_tma\"]] \n        self.sample_weights =  np.array(self.sample_weights * self.n_tiles)\n        \n    def __getitem__(self, idx: int) -> tuple:\n        nth_iteration = idx//len(self.data)\n        if self.mode==\"train\":\n            random.seed()\n        else:\n            random.seed(CONFIG[\"seed\"]+nth_iteration)\n        random.shuffle(self.img_dirs[idx])\n        for img_path in self.img_dirs[idx]:\n            assert os.path.isfile(img_path), f\"missing: {img_path}\"\n            tile = cv2.imread(img_path)\n            tile = cv2.cvtColor(tile, cv2.COLOR_BGR2RGB)\n        \n            # tile = np.array(Image.open(img_path))[..., :3]\n            black_bg = np.sum(tile, axis=2) == 0\n            tile[black_bg, :] = 255\n            mask_bg = np.mean(tile, axis=2) > self.white_thr\n            if np.sum(mask_bg) < (np.prod(mask_bg.shape) * self.thr_max_bg):\n                self.img_paths.append(img_path)\n                print(f\"Idx: {idx}, Path: {img_path}, len img_pths: {len(self.img_paths)}, nunique img_paths: {len(set(self.img_paths))}\")\n                break\n\n        # augmentation\n        if self.transforms:\n            tile = self.transforms(image=tile)[\"image\"]\n        #print(f\"img dim: {img.shape}\")\n        return {\n            \"image\": tile,\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n               }\n    def __len__(self) -> int:\n        return len(self.img_dirs)\n    \n    def get_sample_weights(self):\n        return torch.from_numpy(self.sample_weights).double()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T13:54:50.690123Z","iopub.execute_input":"2023-12-22T13:54:50.690624Z","iopub.status.idle":"2023-12-22T13:54:50.711060Z","shell.execute_reply.started":"2023-12-22T13:54:50.690580Z","shell.execute_reply":"2023-12-22T13:54:50.709730Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def delete_tiles(directory_path):\n    if os.path.isdir(directory_path):\n        for filename in os.listdir(directory_path):\n            if os.path.isfile(os.path.join(directory_path, filename)):\n                os.remove(os.path.join(directory_path, filename))\n\ndef extract_image_tiles(\n    p_img, img_id, tmp_dir, size: int = 2048, scale: float = 0.5,\n    drop_thr: float = 0.8, white_thr: int = 245, max_samples: int = 50\n) -> list:\n    delete_tiles(tmp_dir)  # empty directory from previous images\n    im = pyvips.Image.new_from_file(p_img)\n    w = h = size\n    # https://stackoverflow.com/a/47581978/4521646\n    idxs = [(y, y + h, x, x + w) for y in range(0, im.height, h) for x in range(0, im.width, w)]\n    # random subsample\n    max_samples = max_samples if isinstance(max_samples, int) else int(len(idxs) * max_samples)\n    random.seed(42)\n    random.shuffle(idxs)\n    images = []\n    i = 0\n    for y, y_, x, x_ in (idxs):\n        i += 1\n        img_path = f\"{tmp_dir}/{str(i)}.png\"\n        # https://libvips.github.io/pyvips/vimage.html#pyvips.Image.crop\n        tile = im.crop(x, y, min(w, im.width - x), min(h, im.height - y)).numpy()[..., :3]\n        if tile.shape[:2] != (h, w):\n            tile_ = tile\n            tile_size = (h, w) if tile.ndim == 2 else (h, w, tile.shape[2])\n            tile = np.zeros(tile_size, dtype=tile.dtype)\n            tile[:tile_.shape[0], :tile_.shape[1], ...] = tile_\n        black_bg = np.sum(tile, axis=2) == 0\n        tile[black_bg, :] = 255\n        mask_bg = np.mean(tile, axis=2) > white_thr\n        if np.sum(mask_bg) >= (np.prod(mask_bg.shape) * drop_thr):\n            #print(f\"skip almost empty tile: {k:06}_{int(x_ / w)}-{int(y_ / h)}\")\n            continue\n        # print(tile.shape, tile.dtype, tile.min(), tile.max())\n        new_size = int(size * scale), int(size * scale)\n        tile = Image.fromarray(tile).resize(new_size, Image.LANCZOS)\n        tile.save(img_path)\n        images.append(img_path)\n        # need to set counter check as some empty tiles could be skipped earlier\n        if len(images) >= max_samples:\n            break\n    return images\n\n\nclass TilesInferenceDataset(Dataset):\n\n    def __init__(\n        self,\n        img_path: str,\n        img_id: str = None,\n        tmp_dir: str = None,\n        size: int = 2048,\n        scale: float = 0.25,\n        white_thr: int = 225,\n        thr_max_bg: float = 0.6,\n        max_samples: int = 30,\n        transforms = None,\n        is_submission: bool = True,\n    ):\n        self.max_samples = max_samples\n        self.white_thr = white_thr\n        self.thr_max_bg = thr_max_bg\n        self.is_submission = is_submission\n        \n        self.transforms = transforms\n        if self.is_submission:\n            # print(img_path)\n            assert os.path.isfile(img_path)\n            self.imgs = extract_image_tiles(\n                img_path, img_id, tmp_dir, size=size, scale=scale,\n                drop_thr=self.thr_max_bg, max_samples=max_samples)\n        else:  # test\n            all_imgs = glob.glob(os.path.join(img_path, img_id, \"*.png\"))\n            # Filter images based on background threshold\n            self.imgs = []\n            for img_path in all_imgs:\n                img = cv2.imread(img_path)\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n                black_bg = np.sum(img, axis=2) == 0\n                img[black_bg, :] = 255\n                mask_bg = np.mean(img, axis=2) > self.white_thr\n                if np.sum(mask_bg) <= (np.prod(mask_bg.shape) * self.thr_max_bg):\n                    self.imgs.append(img_path)  # Include this image\n            self.imgs = self.imgs[:self.max_samples]\n            # print(self.imgs)\n\n    def __getitem__(self, idx: int) -> tuple:\n        img = cv2.imread(self.imgs[idx])\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        # filter background\n        mask = np.sum(img, axis=2) == 0\n        img[mask, :] = 255\n        if np.max(img) < 1.5:\n            img = np.clip(img * 255, 0, 255).astype(np.uint8)\n        if self.transforms:\n            img = self.transforms(image=img)[\"image\"]\n        return img\n\n    def __len__(self) -> int:\n        return len(self.imgs)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-22T13:54:50.712729Z","iopub.execute_input":"2023-12-22T13:54:50.713141Z","iopub.status.idle":"2023-12-22T13:54:50.743893Z","shell.execute_reply.started":"2023-12-22T13:54:50.713109Z","shell.execute_reply":"2023-12-22T13:54:50.742277Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_dict_to_tensor(dict_):\n    \"\"\"Converts the values of a dict into a PyTorch tensor.\"\"\"\n\n    # Create a new PyTorch tensor\n    tensor = torch.empty(len(dict_))\n\n    # Iterate over the dict and for each key-value pair, convert the value to a PyTorch tensor and add it to the new tensor\n    for i, (key, value) in enumerate(dict_.items()):\n        tensor[i] = value\n\n    # Return the new tensor\n    return tensor\n\ndef get_class_weights(df_train):\n    label_counts = df_train.target_label.value_counts().sort_index().to_dict()\n    ratios_dict = {}\n    for key,val in label_counts.items():\n        ratios_dict[key] = val / df_train.shape[0]\n    ratios_dict\n    weights = {}\n    sum_weights = 0\n    for key, val in ratios_dict.items():\n        weights[key] = 1 / val\n        sum_weights +=  1 / val\n    for key, val in weights.items():\n        weights[key] = val / sum_weights\n    weight_tensor = convert_dict_to_tensor(weights)\n    return weight_tensor\n\ndef get_dataloaders(df, TRAIN_DIR, CONFIG, data_transforms, n_tiles=1, train_val_split=0.9, apply_sampler=True, tma_weight=1, sample_fac=1):\n    # df_train = df[df[\"kfold\"]!=fold].reset_index(drop=True)\n    train_dataset = CancerTilesDataset(df, TRAIN_DIR, transforms=data_transforms[\"train\"], mode=\"train\", n_tiles=n_tiles, train_val_split=train_val_split, tma_weight=tma_weight)\n    if apply_sampler:\n        samples_weights = train_dataset.get_sample_weights()\n        train_sampler = WeightedRandomSampler(samples_weights, len(samples_weights)*sample_fac)\n    else:\n        train_sampler = None\n    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], num_workers=2, sampler=train_sampler, shuffle=False, pin_memory=True)\n    \n    valid_dataset = CancerTilesDataset(df, TRAIN_DIR, transforms=data_transforms[\"valid\"], mode=\"valid\", n_tiles=n_tiles, train_val_split=train_val_split, tma_weight=tma_weight)\n    if apply_sampler:\n        samples_weights = valid_dataset.get_sample_weights()\n        valid_sampler = WeightedRandomSampler(samples_weights, len(samples_weights)*sample_fac)\n    else:\n        valid_sampler=None\n    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], num_workers=2, sampler=valid_sampler, shuffle=False, pin_memory=True)\n    print(f\"Len Train Dataset: {len(train_dataset)}, Len Validation Dataset: {len(valid_dataset)}\" )\n    return train_loader, valid_loader, df\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-22T13:54:50.745377Z","iopub.execute_input":"2023-12-22T13:54:50.746431Z","iopub.status.idle":"2023-12-22T13:54:50.768030Z","shell.execute_reply.started":"2023-12-22T13:54:50.746379Z","shell.execute_reply":"2023-12-22T13:54:50.765087Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}