{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4fe8124",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T09:39:43.993975Z",
     "iopub.status.busy": "2023-11-30T09:39:43.993221Z",
     "iopub.status.idle": "2023-11-30T09:40:09.498079Z",
     "shell.execute_reply": "2023-11-30T09:40:09.496837Z"
    },
    "papermill": {
     "duration": 25.519303,
     "end_time": "2023-11-30T09:40:09.501292",
     "exception": false,
     "start_time": "2023-11-30T09:39:43.981989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "ydata-profiling 4.3.1 requires dacite>=1.8, but you have dacite 1.6.0 which is incompatible.\r\n",
      "ydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --quiet mlflow dagshub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbaaed07",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-30T09:40:09.524213Z",
     "iopub.status.busy": "2023-11-30T09:40:09.523192Z",
     "iopub.status.idle": "2023-11-30T09:40:21.697691Z",
     "shell.execute_reply": "2023-11-30T09:40:21.696556Z"
    },
    "papermill": {
     "duration": 12.188578,
     "end_time": "2023-11-30T09:40:21.700325",
     "exception": false,
     "start_time": "2023-11-30T09:40:09.511747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "import mlflow.pytorch \n",
    "import dagshub\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp\n",
    "import torchvision\n",
    "\n",
    "from itertools import chain\n",
    "import heapq\n",
    "\n",
    "# Utils\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sklearn Imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix, f1_score, accuracy_score\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from skimage import io\n",
    "\n",
    "# For Image Models\n",
    "import timm\n",
    "\n",
    "from joblib.externals.loky.backend.context import get_context\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Albumentations for augmentations\n",
    "# import albumentations as A\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# For colored terminal text\n",
    "from colorama import Fore, Back, Style\n",
    "b_ = Fore.BLUE\n",
    "sr_ = Style.RESET_ALL\n",
    "\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d74c2f72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T09:40:21.721292Z",
     "iopub.status.busy": "2023-11-30T09:40:21.720213Z",
     "iopub.status.idle": "2023-11-30T09:40:21.727687Z",
     "shell.execute_reply": "2023-11-30T09:40:21.726627Z"
    },
    "papermill": {
     "duration": 0.020693,
     "end_time": "2023-11-30T09:40:21.730258",
     "exception": false,
     "start_time": "2023-11-30T09:40:21.709565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "CONFIG = {\n",
    "    \"is_submission\": False,\n",
    "    \"datetime_now\": datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"), \n",
    "    \"n_fold\": 5,\n",
    "    'fold': 1,\n",
    "    'test_fold': 0,\n",
    "    \"seed\": 42,\n",
    "    \"img_size\": 512,\n",
    "    \"crop_vertical\":True,\n",
    "    \"model_name\": \"tf_efficientnetv2_s_in21ft1k\",   # \"tf_efficientnet_b0_ns\", # \"tf_efficientnetv2_s_in21ft1k\"\n",
    "    \"checkpoint_path\": \"/kaggle/input/tf-efficientnetv2-s-in21ft1k/tf_efficientnetv2_s_in21ft1k.pth\",\n",
    "    \"num_classes\": 5,\n",
    "    \"valid_batch_size\": 16,\n",
    "    \"train_batch_size\": 16,\n",
    "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    # \"model_path\": '/kaggle/input/efficientnetb0-training-crop-images/best_model_checkpoint2023-10-26_09-10-29.pth',\n",
    "    \"encoder_path\": \"/kaggle/input/effnet-version-28/label_encoder_2023-11-21_15-45-54.pkl\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "072c5b8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T09:40:21.750243Z",
     "iopub.status.busy": "2023-11-30T09:40:21.749864Z",
     "iopub.status.idle": "2023-11-30T09:40:21.755383Z",
     "shell.execute_reply": "2023-11-30T09:40:21.754450Z"
    },
    "papermill": {
     "duration": 0.018261,
     "end_time": "2023-11-30T09:40:21.757704",
     "exception": false,
     "start_time": "2023-11-30T09:40:21.739443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_or_create_experiment_id(name):\n",
    "    exp = mlflow.get_experiment_by_name(name)\n",
    "    if exp is None:\n",
    "        exp_id = mlflow.create_experiment(name)\n",
    "        return exp_id\n",
    "    return exp.experiment_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571d357a",
   "metadata": {
    "papermill": {
     "duration": 0.010395,
     "end_time": "2023-11-30T09:40:21.777705",
     "exception": false,
     "start_time": "2023-11-30T09:40:21.767310",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Datasets & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8382acbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T09:40:21.804441Z",
     "iopub.status.busy": "2023-11-30T09:40:21.803639Z",
     "iopub.status.idle": "2023-11-30T09:40:21.810811Z",
     "shell.execute_reply": "2023-11-30T09:40:21.809030Z"
    },
    "papermill": {
     "duration": 0.024784,
     "end_time": "2023-11-30T09:40:21.814536",
     "exception": false,
     "start_time": "2023-11-30T09:40:21.789752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_train_file_path(df_train_row, TRAIN_DIR):\n",
    "    if df_train_row.is_tma == False:\n",
    "        return f\"{TRAIN_DIR}/{df_train_row.image_id}_thumbnail.png\"\n",
    "    else:\n",
    "        return f\"{TRAIN_DIR}/{df_train_row.image_id}.png\"\n",
    "\n",
    "\n",
    "def get_test_file_path(image_id,TEST_DIR):\n",
    "    if os.path.exists(f\"{TEST_DIR}/{image_id}.png\"):\n",
    "        return f\"{TEST_DIR}/{image_id}.png\"\n",
    "    else:\n",
    "        return f\"{ALT_TEST_DIR}/{image_id}.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f308e0f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T09:40:21.840829Z",
     "iopub.status.busy": "2023-11-30T09:40:21.840218Z",
     "iopub.status.idle": "2023-11-30T09:40:21.865818Z",
     "shell.execute_reply": "2023-11-30T09:40:21.864462Z"
    },
    "papermill": {
     "duration": 0.041003,
     "end_time": "2023-11-30T09:40:21.868861",
     "exception": false,
     "start_time": "2023-11-30T09:40:21.827858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UBCDataset(Dataset):\n",
    "    def __init__(self, df, transforms=None, apply_vertical_crop=True):\n",
    "        self.df = df\n",
    "        self.filenames = df.file_path.values\n",
    "        self.labels =  df.target_label.values\n",
    "        self.transforms = transforms\n",
    "        self.apply_vertical_crop = apply_vertical_crop\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.filenames[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.apply_vertical_crop:\n",
    "            img = crop_vertical(img)\n",
    "                \n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "            \n",
    "        return {\n",
    "            \"image\": img,\n",
    "            \"label\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "               }\n",
    "\n",
    "def crop_vertical(image):\n",
    "    \"\"\"\n",
    "    Function crops images if multiple slices contained and separated by black vertical background.\n",
    "    \"\"\"\n",
    "    vertical_sum = np.sum(image, axis=(0, 2))\n",
    "\n",
    "    # Identify the positions where the sum is zero\n",
    "    zero_positions = np.where(vertical_sum == 0)[0]\n",
    "\n",
    "    if len(zero_positions)==0:\n",
    "        cropped_images = [image]\n",
    "    else:\n",
    "        # If the image does not start with a black area, add index 0\n",
    "        if zero_positions[0] != 0:\n",
    "            zero_positions = np.insert(zero_positions, 0, 0)\n",
    "\n",
    "        # If the image does not end with a black area, add the image width\n",
    "        if zero_positions[-1] != image.shape[1] - 1:\n",
    "            zero_positions = np.append(zero_positions, image.shape[1] - 1)\n",
    "\n",
    "        start_idx = zero_positions[0]\n",
    "        cropped_images = []\n",
    "\n",
    "        for idx in range(1, len(zero_positions)):\n",
    "            end_idx = zero_positions[idx]\n",
    "            if end_idx - start_idx > 1:  # If the width of the cropped section is greater than 1\n",
    "                cropped = image[:, start_idx:end_idx]\n",
    "                # only include samples which are of min size\n",
    "                if cropped.shape[1]>200:  \n",
    "                    cropped_images.append(cropped)\n",
    "                    # cv2.imwrite(f\"{save_prefix}_{idx}.jpg\", cropped)\n",
    "            start_idx = end_idx\n",
    "\n",
    "    final_crops = []\n",
    "    # remove black bars above/below the crops \n",
    "    for cropped in cropped_images:\n",
    "        horizontal_sum = np.sum(cropped, axis=(1, 2))\n",
    "        zero_positions = np.where(horizontal_sum == 0)[0]\n",
    "        img_ = np.delete(cropped, zero_positions, axis=0)\n",
    "        final_crops.append(img_)\n",
    "    if len(final_crops)==0:\n",
    "        return image\n",
    "    return final_crops[0]\n",
    "\n",
    "\n",
    "def custom_center_crop_or_resize(image, crop_size):\n",
    "    # If both dimensions of the image are greater than or equal to the desired size, apply CenterCrop\n",
    "    if image.shape[0] >= crop_size[0] and image.shape[1] >= crop_size[1]:\n",
    "        return A.CenterCrop(crop_size[0], crop_size[1])(image=image)[\"image\"]\n",
    "    # Else, just resize the image to the desired size\n",
    "    else:\n",
    "        return A.Resize(crop_size[0], crop_size[1])(image=image)[\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5c9e7c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T09:40:21.889917Z",
     "iopub.status.busy": "2023-11-30T09:40:21.889472Z",
     "iopub.status.idle": "2023-11-30T09:40:21.897328Z",
     "shell.execute_reply": "2023-11-30T09:40:21.896035Z"
    },
    "papermill": {
     "duration": 0.021175,
     "end_time": "2023-11-30T09:40:21.899904",
     "exception": false,
     "start_time": "2023-11-30T09:40:21.878729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _color_means(img_path):\n",
    "    img = np.array(Image.open(img_path))\n",
    "    mask = np.sum(img[..., :3], axis=2) == 0\n",
    "    img[mask, :] = 255\n",
    "    if np.max(img) > 1.5:\n",
    "        img = img / 255.0\n",
    "    clr_mean = {i: np.mean(img[..., i]) for i in range(3)}\n",
    "    clr_std = {i: np.std(img[..., i]) for i in range(3)}\n",
    "    return clr_mean, clr_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d75340e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T09:40:21.920438Z",
     "iopub.status.busy": "2023-11-30T09:40:21.920022Z",
     "iopub.status.idle": "2023-11-30T09:40:21.930993Z",
     "shell.execute_reply": "2023-11-30T09:40:21.929801Z"
    },
    "papermill": {
     "duration": 0.024685,
     "end_time": "2023-11-30T09:40:21.933782",
     "exception": false,
     "start_time": "2023-11-30T09:40:21.909097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_color_mean=[0.8661704276539922, 0.7663107094675368, 0.8574260897185548]\n",
    "img_color_std=[0.08670629753900036, 0.11646580094195522, 0.07164169171856792]\n",
    "\n",
    "data_transforms = {\n",
    "    \"train\": A.Compose([\n",
    "        A.Resize(512, 512),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.75),\n",
    "        A.ShiftScaleRotate(p=0.75),\n",
    "        A.OneOf([\n",
    "        A.GaussNoise(var_limit=[10, 50]),\n",
    "        A.GaussianBlur(),\n",
    "        A.MotionBlur(),\n",
    "        ], p=0.4),\n",
    "        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
    "        A.CoarseDropout(max_holes=1, max_width=int(512* 0.3), max_height=int(512* 0.3),\n",
    "        mask_fill_value=0, p=0.5),\n",
    "        A.Normalize(img_color_mean, img_color_std), \n",
    "        ToTensorV2()], p=1.),\n",
    "    \n",
    "    \"valid\": A.Compose([\n",
    "        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "        A.Normalize(img_color_mean, img_color_std), \n",
    "        ToTensorV2()], p=1.)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1936e69",
   "metadata": {
    "papermill": {
     "duration": 0.009024,
     "end_time": "2023-11-30T09:40:21.952104",
     "exception": false,
     "start_time": "2023-11-30T09:40:21.943080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4cde1c8",
   "metadata": {
    "papermill": {
     "duration": 0.008918,
     "end_time": "2023-11-30T09:40:21.970374",
     "exception": false,
     "start_time": "2023-11-30T09:40:21.961456",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.2. Tiles Dataset\n",
    "\n",
    "### 2.2.1. Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29749f51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T09:40:21.990807Z",
     "iopub.status.busy": "2023-11-30T09:40:21.990419Z",
     "iopub.status.idle": "2023-11-30T09:40:22.009718Z",
     "shell.execute_reply": "2023-11-30T09:40:22.008629Z"
    },
    "papermill": {
     "duration": 0.032551,
     "end_time": "2023-11-30T09:40:22.012139",
     "exception": false,
     "start_time": "2023-11-30T09:40:21.979588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CancerTilesDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df_data,\n",
    "        path_img_dir: str =  '',\n",
    "        transforms = None,\n",
    "        mode: str = 'train',\n",
    "        labels_lut = None,\n",
    "        white_thr: int = 225,\n",
    "        thr_max_bg: float = 0.2,\n",
    "        split: float = 0.90,\n",
    "        n_tiles: int = 1\n",
    "    ):\n",
    "        assert os.path.isdir(path_img_dir)\n",
    "        self.path_img_dir = path_img_dir\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "        self.white_thr = white_thr\n",
    "        self.thr_max_bg = thr_max_bg\n",
    "        self.train_val_split = train_val_split\n",
    "        self.n_tiles = n_tiles\n",
    "\n",
    "        self.data = df_data\n",
    "        self.labels_unique = sorted(self.data[\"label\"].unique())\n",
    "        self.labels_lut = labels_lut or {lb: i for i, lb in enumerate(self.labels_unique)}\n",
    "        # shuffle data\n",
    "        self.data = self.data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "        # split dataset\n",
    "        assert 0.0 <= self.train_val_split <= 1.0\n",
    "        frac = int(self.train_val_split * len(self.data))\n",
    "        self.data = self.data[:frac] if mode in [\"train\", \"test\"] else self.data[frac:]\n",
    "        self.img_dirs = [glob.glob(os.path.join(path_img_dir, str(idx), \"*.png\")) for idx in self.data[\"image_id\"]] \n",
    "        self.img_dirs = self.img_dirs * self.n_tiles\n",
    "        self.img_paths = []\n",
    "        #print(f\"missing: {sum([not os.path.isfile(os.path.join(self.path_img_dir, im))\n",
    "        #                       for im in self.img_names])}\")\n",
    "        # self.labels = list(self.data['label'])\n",
    "        self.labels =  np.array(self.data.target_label.values.tolist() * self.n_tiles)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        nth_iteration = idx//len(self.data)\n",
    "        if self.mode==\"train\":\n",
    "            random.seed()\n",
    "        else:\n",
    "            random.seed(CONFIG[\"seed\"]+nth_iteration)\n",
    "        random.shuffle(self.img_dirs[idx])\n",
    "        for img_path in self.img_dirs[idx]:\n",
    "            assert os.path.isfile(img_path), f\"missing: {img_path}\"\n",
    "            tile = cv2.imread(img_path)\n",
    "            tile = cv2.cvtColor(tile, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "            # tile = np.array(Image.open(img_path))[..., :3]\n",
    "            black_bg = np.sum(tile, axis=2) == 0\n",
    "            tile[black_bg, :] = 255\n",
    "            mask_bg = np.mean(tile, axis=2) > self.white_thr\n",
    "            if np.sum(mask_bg) < (np.prod(mask_bg.shape) * self.thr_max_bg):\n",
    "                #self.img_paths.append(img_path)\n",
    "                #print(f\"Idx: {idx}, Path: {img_path}, len img_pths: {len(self.img_paths)}, nunique img_paths: {len(set(self.img_paths))}\")\n",
    "                break\n",
    "\n",
    "        # augmentation\n",
    "        if self.transforms:\n",
    "            tile = self.transforms(image=tile)[\"image\"]\n",
    "        #print(f\"img dim: {img.shape}\")\n",
    "        return {\n",
    "            \"image\": tile,\n",
    "            \"label\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "               }\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.img_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8a3bcf",
   "metadata": {
    "papermill": {
     "duration": 0.009037,
     "end_time": "2023-11-30T09:40:22.030805",
     "exception": false,
     "start_time": "2023-11-30T09:40:22.021768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd0d9e40",
   "metadata": {
    "papermill": {
     "duration": 0.008995,
     "end_time": "2023-11-30T09:40:22.049113",
     "exception": false,
     "start_time": "2023-11-30T09:40:22.040118",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.2.2 Inference Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "289b2b88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T09:40:22.070143Z",
     "iopub.status.busy": "2023-11-30T09:40:22.069413Z",
     "iopub.status.idle": "2023-11-30T09:40:22.094960Z",
     "shell.execute_reply": "2023-11-30T09:40:22.094088Z"
    },
    "papermill": {
     "duration": 0.039114,
     "end_time": "2023-11-30T09:40:22.097658",
     "exception": false,
     "start_time": "2023-11-30T09:40:22.058544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def delete_tiles(directory_path):\n",
    "    if os.path.isdir(directory_path):\n",
    "        for filename in os.listdir(directory_path):\n",
    "            if os.path.isfile(os.path.join(directory_path, filename)):\n",
    "                os.remove(os.path.join(directory_path, filename))\n",
    "\n",
    "def extract_image_tiles(\n",
    "    p_img, img_id, tmp_dir, size: int = 2048, scale: float = 0.5,\n",
    "    drop_thr: float = 0.8, white_thr: int = 245, max_samples: int = 50\n",
    ") -> list:\n",
    "    delete_tiles(tmp_dir)  # empty directory from previous images\n",
    "    im = pyvips.Image.new_from_file(p_img)\n",
    "    w = h = size\n",
    "    # https://stackoverflow.com/a/47581978/4521646\n",
    "    idxs = [(y, y + h, x, x + w) for y in range(0, im.height, h) for x in range(0, im.width, w)]\n",
    "    # random subsample\n",
    "    max_samples = max_samples if isinstance(max_samples, int) else int(len(idxs) * max_samples)\n",
    "    random.seed(42)\n",
    "    random.shuffle(idxs)\n",
    "    images = []\n",
    "    i = 0\n",
    "    for y, y_, x, x_ in (idxs):\n",
    "        i += 1\n",
    "        img_path = f\"{tmp_dir}/{str(i)}.png\"\n",
    "        # https://libvips.github.io/pyvips/vimage.html#pyvips.Image.crop\n",
    "        tile = im.crop(x, y, min(w, im.width - x), min(h, im.height - y)).numpy()[..., :3]\n",
    "        if tile.shape[:2] != (h, w):\n",
    "            tile_ = tile\n",
    "            tile_size = (h, w) if tile.ndim == 2 else (h, w, tile.shape[2])\n",
    "            tile = np.zeros(tile_size, dtype=tile.dtype)\n",
    "            tile[:tile_.shape[0], :tile_.shape[1], ...] = tile_\n",
    "        black_bg = np.sum(tile, axis=2) == 0\n",
    "        tile[black_bg, :] = 255\n",
    "        mask_bg = np.mean(tile, axis=2) > white_thr\n",
    "        if np.sum(mask_bg) >= (np.prod(mask_bg.shape) * drop_thr):\n",
    "            #print(f\"skip almost empty tile: {k:06}_{int(x_ / w)}-{int(y_ / h)}\")\n",
    "            continue\n",
    "        # print(tile.shape, tile.dtype, tile.min(), tile.max())\n",
    "        new_size = int(size * scale), int(size * scale)\n",
    "        tile = Image.fromarray(tile).resize(new_size, Image.LANCZOS)\n",
    "        tile.save(img_path)\n",
    "        images.append(img_path)\n",
    "        # need to set counter check as some empty tiles could be skipped earlier\n",
    "        if len(images) >= max_samples:\n",
    "            break\n",
    "    return images\n",
    "\n",
    "\n",
    "class TilesInferenceDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_path: str,\n",
    "        img_id: str = None,\n",
    "        tmp_dir: str = None,\n",
    "        size: int = 2048,\n",
    "        scale: float = 0.25,\n",
    "        white_thr: int = 225,\n",
    "        thr_max_bg: float = 0.6,\n",
    "        max_samples: int = 30,\n",
    "        transforms = None,\n",
    "        is_submission: bool = True,\n",
    "    ):\n",
    "        self.max_samples = max_samples\n",
    "        self.white_thr = white_thr\n",
    "        self.thr_max_bg = thr_max_bg\n",
    "        self.is_submission = is_submission\n",
    "        \n",
    "        self.transforms = transforms\n",
    "        if self.is_submission:\n",
    "            # print(img_path)\n",
    "            assert os.path.isfile(img_path)\n",
    "            self.imgs = extract_image_tiles(\n",
    "                img_path, img_id, tmp_dir, size=size, scale=scale,\n",
    "                drop_thr=self.thr_max_bg, max_samples=max_samples)\n",
    "        else:  # test\n",
    "            all_imgs = glob.glob(os.path.join(img_path, img_id, \"*.png\"))\n",
    "            # Filter images based on background threshold\n",
    "            self.imgs = []\n",
    "            for img_path in all_imgs:\n",
    "                img = cv2.imread(img_path)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                black_bg = np.sum(img, axis=2) == 0\n",
    "                img[black_bg, :] = 255\n",
    "                mask_bg = np.mean(img, axis=2) > self.white_thr\n",
    "                if np.sum(mask_bg) <= (np.prod(mask_bg.shape) * self.thr_max_bg):\n",
    "                    self.imgs.append(img_path)  # Include this image\n",
    "            self.imgs = self.imgs[:self.max_samples]\n",
    "            # print(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        img = cv2.imread(self.imgs[idx])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        # filter background\n",
    "        mask = np.sum(img, axis=2) == 0\n",
    "        img[mask, :] = 255\n",
    "        if np.max(img) < 1.5:\n",
    "            img = np.clip(img * 255, 0, 255).astype(np.uint8)\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "        return img\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.imgs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665f3347",
   "metadata": {
    "papermill": {
     "duration": 0.009098,
     "end_time": "2023-11-30T09:40:22.116107",
     "exception": false,
     "start_time": "2023-11-30T09:40:22.107009",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43221424",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T09:40:22.137115Z",
     "iopub.status.busy": "2023-11-30T09:40:22.136310Z",
     "iopub.status.idle": "2023-11-30T09:40:22.150237Z",
     "shell.execute_reply": "2023-11-30T09:40:22.149430Z"
    },
    "papermill": {
     "duration": 0.027318,
     "end_time": "2023-11-30T09:40:22.152887",
     "exception": false,
     "start_time": "2023-11-30T09:40:22.125569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super(GeM, self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gem(x, p=self.p, eps=self.eps)\n",
    "        \n",
    "    def gem(self, x, p=3, eps=1e-6):\n",
    "        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \\\n",
    "                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n",
    "                ', ' + 'eps=' + str(self.eps) + ')'\n",
    "\n",
    "\n",
    "class UBCModel(nn.Module):\n",
    "    '''\n",
    "    EfficientNet B0 fine-tune.\n",
    "    '''\n",
    "    def __init__(self, model_name, num_classes, pretrained=False, checkpoint_path=None):\n",
    "        '''\n",
    "        Fine tune for EfficientNetB0\n",
    "        Args\n",
    "            n_classes : int - Number of classification categories.\n",
    "            learnable_modules : tuple - Names of the modules to fine-tune.\n",
    "        Return\n",
    "            \n",
    "        '''\n",
    "        super(UBCModel, self).__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained, checkpoint_path=checkpoint_path)\n",
    "\n",
    "        in_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Identity()\n",
    "        self.model.global_pool = nn.Identity()\n",
    "        self.pooling = GeM()\n",
    "        self.linear = nn.Linear(in_features, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Forward function for the fine-tuned model\n",
    "        Args\n",
    "            x: \n",
    "        Return\n",
    "            result\n",
    "        \"\"\"\n",
    "        features = self.model(images)\n",
    "        pooled_features = self.pooling(features).flatten(1)\n",
    "        output = self.linear(pooled_features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f87f743b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T09:40:22.174367Z",
     "iopub.status.busy": "2023-11-30T09:40:22.173581Z",
     "iopub.status.idle": "2023-11-30T09:40:22.184602Z",
     "shell.execute_reply": "2023-11-30T09:40:22.183580Z"
    },
    "papermill": {
     "duration": 0.02454,
     "end_time": "2023-11-30T09:40:22.187246",
     "exception": false,
     "start_time": "2023-11-30T09:40:22.162706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, delta=0, path='checkpoint.pth', trace_func=print):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decreases.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bc368c",
   "metadata": {
    "papermill": {
     "duration": 0.008913,
     "end_time": "2023-11-30T09:40:22.205772",
     "exception": false,
     "start_time": "2023-11-30T09:40:22.196859",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63ca1285",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T09:40:22.227271Z",
     "iopub.status.busy": "2023-11-30T09:40:22.226886Z",
     "iopub.status.idle": "2023-11-30T09:40:22.244175Z",
     "shell.execute_reply": "2023-11-30T09:40:22.242873Z"
    },
    "papermill": {
     "duration": 0.031542,
     "end_time": "2023-11-30T09:40:22.246630",
     "exception": false,
     "start_time": "2023-11-30T09:40:22.215088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def fetch_scheduler(optimizer, CONFIG):\n",
    "    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], \n",
    "                                                   eta_min=CONFIG['min_lr'], verbose=False)\n",
    "    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n",
    "        CONFIG['T_0'] = 10\n",
    "        CONFIG['T_mult'] = 2\n",
    "        CONFIG['min_lr'] = 1e-6\n",
    "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CONFIG['T_0'], T_mult=CONFIG['T_mult'],\n",
    "                                                             eta_min=CONFIG['min_lr'], verbose=False)\n",
    "    elif CONFIG['scheduler'] == 'ReduceLROnPlateau':\n",
    "        scheduler =  ReduceLROnPlateau(optimizer, mode='min', factor=kwargs.get('factor', 0.1), patience=kwargs.get('patience', 5), verbose=False)\n",
    "    elif CONFIG['scheduler'] == 'LambdaLR':\n",
    "        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    elif CONFIG['scheduler'] == None:\n",
    "        return None\n",
    "    return scheduler\n",
    "\n",
    "def get_optimizer(optimizer_name, model):\n",
    "    if optimizer_name.lower() == \"adam\":\n",
    "        CONFIG['learning_rate'] = 1e-4\n",
    "        CONFIG['weight_decay'] = 1e-5\n",
    "        CONFIG['betas'] = (0.9, 0.999)\n",
    "        CONFIG['eps'] = 1e-8\n",
    "        optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'], betas=CONFIG['betas'], eps=CONFIG['eps'],  weight_decay=CONFIG['weight_decay'])\n",
    "    elif optimizer_name.lower() == \"sgd\":\n",
    "        CONFIG['learning_rate'] = 1e-3\n",
    "        CONFIG['weight_decay'] = 1e-3\n",
    "        CONFIG['momentum'] = 1e-3\n",
    "        optimizer = optim.SGD(model.parameters(), lr=CONFIG['learning_rate'], momentum=CONFIG['momentum'], weight_decay=CONFIG['weight_decay'])\n",
    "    elif optimizer_name.lower() == \"radam\":\n",
    "        CONFIG['learning_rate'] = 1e-4\n",
    "        CONFIG['weight_decay'] = 0\n",
    "        CONFIG['betas'] = (0.9, 0.999)\n",
    "        CONFIG['eps'] = 1e-8\n",
    "        optimizer = torch_optimizer.RAdam(\n",
    "            model.parameters(),\n",
    "            lr= CONFIG['learning_rate'],\n",
    "            betas=CONFIG['betas'],\n",
    "            eps=CONFIG['eps'],\n",
    "            weight_decay=CONFIG['weight_decay'],\n",
    "        )\n",
    "    elif optimizer_name.lower() == \"rmsprop\":\n",
    "        CONFIG['learning_rate'] = 0.256\n",
    "        CONFIG['alpha'] = 0.9\n",
    "        CONFIG['momentum'] = 0.9\n",
    "        CONFIG['weight_decay'] = 1e-5\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=CONFIG['learning_rate'], alpha=CONFIG['learning_rate'], \n",
    "                                  momentum=CONFIG['learning_rate'], weight_decay=CONFIG['learning_rate'])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid Optimizer given!\")\n",
    "    return optimizer\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fac52989",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T09:40:22.268325Z",
     "iopub.status.busy": "2023-11-30T09:40:22.267339Z",
     "iopub.status.idle": "2023-11-30T09:40:22.288587Z",
     "shell.execute_reply": "2023-11-30T09:40:22.286644Z"
    },
    "papermill": {
     "duration": 0.036826,
     "end_time": "2023-11-30T09:40:22.292901",
     "exception": false,
     "start_time": "2023-11-30T09:40:22.256075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def convert_dict_to_tensor(dict_):\n",
    "    \"\"\"Converts the values of a dict into a PyTorch tensor.\"\"\"\n",
    "\n",
    "    # Create a new PyTorch tensor\n",
    "    tensor = torch.empty(len(dict_))\n",
    "\n",
    "    # Iterate over the dict and for each key-value pair, convert the value to a PyTorch tensor and add it to the new tensor\n",
    "    for i, (key, value) in enumerate(dict_.items()):\n",
    "        tensor[i] = value\n",
    "\n",
    "    # Return the new tensor\n",
    "    return tensor\n",
    "\n",
    "def get_class_weights(df_train):\n",
    "    label_counts = df_train.target_label.value_counts().sort_index().to_dict()\n",
    "    ratios_dict = {}\n",
    "    for key,val in label_counts.items():\n",
    "        ratios_dict[key] = val / df_train.shape[0]\n",
    "    ratios_dict\n",
    "    weights = {}\n",
    "    sum_weights = 0\n",
    "    for key, val in ratios_dict.items():\n",
    "        weights[key] = 1 / val\n",
    "        sum_weights +=  1 / val\n",
    "    for key, val in weights.items():\n",
    "        weights[key] = val / sum_weights\n",
    "    weight_tensor = convert_dict_to_tensor(weights)\n",
    "    return weight_tensor\n",
    "\n",
    "def get_dataloaders(df, TRAIN_DIR, CONFIG, data_transforms, n_tiles=1, train_val_split=0.9):\n",
    "    # df_train = df[df[\"kfold\"]!=fold].reset_index(drop=True)\n",
    "    train_dataset = CancerTilesDataset(df, TRAIN_DIR, transforms=data_transforms[\"train\"], mode=\"train\", n_tiles=n_tiles, train_val_split=train_val_split)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n",
    "                              num_workers=2, shuffle=False, pin_memory=True)\n",
    "    valid_dataset = CancerTilesDataset(df, TRAIN_DIR, transforms=data_transforms[\"valid\"], mode=\"valid\", n_tiles=n_tiles, train_val_split=train_val_split)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n",
    "                              num_workers=2, shuffle=False, pin_memory=True)\n",
    "    print(f\"Len Train Dataset: {len(train_dataset)}, Len Validation Dataset: {len(valid_dataset)}\" )\n",
    "    return train_loader, valid_loader, df\n",
    "\n",
    "def print_logged_info(r):\n",
    "    tags = {k: v for k, v in r.data.tags.items() if not k.startswith(\"mlflow.\")}\n",
    "    artifacts = [f.path for f in MlflowClient().list_artifacts(r.info.run_id, \"model\")]\n",
    "    print(f\"run_id: {r.info.run_id}\")\n",
    "    print(f\"artifacts: {artifacts}\")\n",
    "    print(f\"params: {r.data.params}\")\n",
    "    print(f\"metrics: {r.data.metrics}\")\n",
    "    print(f\"tags: {tags}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e12a42a",
   "metadata": {
    "papermill": {
     "duration": 0.012022,
     "end_time": "2023-11-30T09:40:22.317989",
     "exception": false,
     "start_time": "2023-11-30T09:40:22.305967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef1154cc",
   "metadata": {
    "papermill": {
     "duration": 0.010199,
     "end_time": "2023-11-30T09:40:22.344257",
     "exception": false,
     "start_time": "2023-11-30T09:40:22.334058",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Inference & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "406180f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T09:40:22.365881Z",
     "iopub.status.busy": "2023-11-30T09:40:22.365112Z",
     "iopub.status.idle": "2023-11-30T09:40:22.377110Z",
     "shell.execute_reply": "2023-11-30T09:40:22.376249Z"
    },
    "papermill": {
     "duration": 0.026208,
     "end_time": "2023-11-30T09:40:22.379715",
     "exception": false,
     "start_time": "2023-11-30T09:40:22.353507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_predictions(df):\n",
    "    # Total Accuracy\n",
    "    total_accuracy = accuracy_score(df['target_label'], df['label'])\n",
    "\n",
    "    # Balanced Accuracy\n",
    "    balanced_accuracy = balanced_accuracy_score(df['target_label'], df['label'])\n",
    "\n",
    "    # F1 Score\n",
    "    f1 = f1_score(df['target_label'], df['label'], average='weighted')\n",
    "\n",
    "    # Accuracy Per Class\n",
    "    cm = confusion_matrix(df['target_label'], df['label'])\n",
    "    class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "    print(f\"Total Accuracy: {total_accuracy}\")\n",
    "    print(f\"Balanced Accuracy: {balanced_accuracy}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Accuracy Per Class: {class_accuracy}\")\n",
    "    display(cm)\n",
    "    \n",
    "def most_frequent(List):\n",
    "    return max(set(List), key = List.count)\n",
    "\n",
    "def score_predictions(preds, method=\"sum\", N=10):\n",
    "    if method==\"sum\":  # sum up the predictions of all tiles/models\n",
    "        lb = np.argmax(np.sum(preds, axis=0))\n",
    "    elif method in [\"most_frequent\", \"most_votes\", \"majority_vote\"]:  # get majority vote over all tiles/models \n",
    "        lb = most_frequent(np.argmax(preds, axis=1).tolist())\n",
    "    elif method == \"n_highest_sum\":  # sum up predictions of N-most decicive tiles/models\n",
    "        max_vals = np.max(preds, axis=1).tolist()\n",
    "        max_idxs = [max_vals.index(i) for i in heapq.nlargest(N, max_vals)]\n",
    "        n_tiles_preds = np.take(preds, max_idxs, axis=0).tolist()\n",
    "        lb = np.argmax(np.sum(n_tiles_preds, axis=0))\n",
    "    else:\n",
    "        print(\"No method found: Apply Sum Method for Scoring predictions!\")\n",
    "        lb = np.argmax(np.sum(preds, axis=0))\n",
    "    return lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58f5ccc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T09:40:22.401467Z",
     "iopub.status.busy": "2023-11-30T09:40:22.400662Z",
     "iopub.status.idle": "2023-11-30T09:40:22.421375Z",
     "shell.execute_reply": "2023-11-30T09:40:22.420449Z"
    },
    "papermill": {
     "duration": 0.034492,
     "end_time": "2023-11-30T09:40:22.424024",
     "exception": false,
     "start_time": "2023-11-30T09:40:22.389532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def infer_with_softmax(model, images):\n",
    "    with torch.no_grad():\n",
    "        logits = model(images)\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        return probabilities\n",
    "    \n",
    "def apply_thr_outlier_detect(predictions, label, threshold = 0.60):\n",
    "    # Convert logits to probabilities\n",
    "    probabilities = softmax(predictions)\n",
    "\n",
    "    # Apply the threshold\n",
    "    max_probabilities = np.max(probabilities)\n",
    "    if max_probabilities < threshold:\n",
    "        print(\"Outlier detected:\", max_probabilities)\n",
    "        return \"Other\"\n",
    "    else:\n",
    "        return label\n",
    "    \n",
    "def infer_single_image_ensemble(idx_row, models, CONFIG, encoder, score_method=\"sum\", max_samples=30, thr_max_bg=0.1, is_submission=True, device=\"cuda\") -> dict:\n",
    "    \"\"\"\n",
    "    Create tiled-dataset based on test image.\n",
    "    Iterate throuh all tiles and apply model prediction.\n",
    "    Select highest of sum of all logits.\n",
    "    \"\"\"\n",
    "    row = dict(idx_row[1])\n",
    "    img_id = str(row[\"image_id\"])\n",
    "    result = {\"image_id\": img_id}\n",
    "    tmp_dir = \"tmp_tiles_\"+str(img_id)\n",
    "    print(\"Image ID: \", img_id)\n",
    "    if is_submission:\n",
    "        result[\"target_label\"] = encoder.inverse_transform(np.array(row[\"target_label\"]).ravel())[0]\n",
    "        # delete old directory if exists and create new empty directory to temporarily save image tiles   \n",
    "        if os.path.exists(tmp_dir) and os.path.isdir(tmp_dir):\n",
    "            shutil.rmtree(tmp_dir)\n",
    "        os.mkdir(tmp_dir)  \n",
    "        dataset = TilesInferenceDataset(\n",
    "            os.path.join(\"/kaggle/input/UBC-OCEAN/\", \"train_images\", f\"{img_id}.png\"), tmp_dir=tmp_dir, \n",
    "            size=2048, scale=0.25, thr_max_bg=thr_max_bg, transforms=data_transforms[\"valid\"], max_samples=max_samples)\n",
    "    else:\n",
    "        dataset = TilesInferenceDataset(\n",
    "            \"/kaggle/input/tiles-of-cancer-2048px-scale-0-25\", img_id,\n",
    "            size=2048, scale=0.25, thr_max_bg=thr_max_bg, transforms=data_transforms[\"valid\"], is_submission=is_submission, max_samples=max_samples)\n",
    "        result[\"target_label\"] = encoder.inverse_transform(np.array(row[\"target_label\"]).ravel())[0]\n",
    "        \n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=CONFIG[\"valid_batch_size\"], num_workers=2, shuffle=False,\n",
    "        multiprocessing_context=get_context('loky')         # see: https://github.com/pytorch/pytorch/issues/44687#issuecomment-790842173\n",
    "    )\n",
    "    if not len(dataset):  # if no tiles available, set to \"Other\" class\n",
    "        if not is_submission: \n",
    "            print (f\"seem no tiles were cut for `{row['image_id']}`. Set to label Other\")\n",
    "        result[\"label\"] = \"Other\"\n",
    "        return result\n",
    "    \n",
    "    if not isinstance(models, list):\n",
    "        models = [models]\n",
    "    \n",
    "    model_preds_sum = []\n",
    "    for i,model in enumerate(models):\n",
    "        #print(f\"Apply Model {i+1} of {len(models)}\")\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        for imgs in dataloader:\n",
    "            # print(f\"{imgs.shape}\")\n",
    "            probabilities = infer_with_softmax(model, imgs.to(device))\n",
    "            preds += probabilities.cpu().numpy().tolist()\n",
    "        if not is_submission:\n",
    "            print(f\"Sum contrinution from all tiles: {np.sum(preds, axis=0)}\")\n",
    "            print(f\"Max contribution over all tiles: {np.max(preds, axis=0)}\")\n",
    "        model_preds_sum.append(preds)\n",
    "        \n",
    "    model_preds_sum = sum(model_preds_sum, [])\n",
    "    prediction = score_predictions(model_preds_sum, method=score_method)\n",
    "    \n",
    "    result[\"label\"] = encoder.inverse_transform(np.array(prediction).reshape(-1,))[0]        \n",
    "    result[\"label\"] = apply_thr_outlier_detect(prediction, result[\"label\"], threshold=0.6)\n",
    "    result[\"predictions\"] = np.sum(model_preds_sum, axis=0).tolist()    \n",
    "    if os.path.exists(tmp_dir) and os.path.isdir(tmp_dir):\n",
    "        shutil.rmtree(tmp_dir)\n",
    "    \n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e483fc",
   "metadata": {
    "papermill": {
     "duration": 0.009122,
     "end_time": "2023-11-30T09:40:22.442661",
     "exception": false,
     "start_time": "2023-11-30T09:40:22.433539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3314ed",
   "metadata": {
    "papermill": {
     "duration": 0.009095,
     "end_time": "2023-11-30T09:40:22.461713",
     "exception": false,
     "start_time": "2023-11-30T09:40:22.452618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2033cad",
   "metadata": {
    "papermill": {
     "duration": 0.009018,
     "end_time": "2023-11-30T09:40:22.480165",
     "exception": false,
     "start_time": "2023-11-30T09:40:22.471147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 6924515,
     "sourceId": 45867,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 44.203902,
   "end_time": "2023-11-30T09:40:24.115839",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-30T09:39:39.911937",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
