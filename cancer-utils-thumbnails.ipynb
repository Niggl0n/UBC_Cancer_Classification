{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":45867,"databundleVersionId":6924515,"sourceType":"competition"}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --quiet mlflow dagshub\n\nimport mlflow\nfrom mlflow import MlflowClient\nimport mlflow.pytorch \nimport dagshub","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nimport datetime\nimport os\nimport gc\nimport cv2\nimport math\nimport copy\nimport time\nimport random\nimport glob\nfrom matplotlib import pyplot as plt\n\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\nimport torchvision\n\nfrom itertools import chain\nimport heapq\n\n# Utils\nimport joblib\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\n# Sklearn Imports\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import balanced_accuracy_score, confusion_matrix, f1_score, accuracy_score\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom skimage import io\n\n# For Image Models\nimport timm\n\nfrom joblib.externals.loky.backend.context import get_context\n\n\n\n\nimport os\nimport numpy as np\nimport random\nfrom PIL import Image\n\nfrom tqdm.auto import tqdm\nfrom joblib import Parallel, delayed\n\n# Albumentations for augmentations\n# import albumentations as A\n# from albumentations.pytorch import ToTensorV2\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\nb_ = Fore.BLUE\nsr_ = Style.RESET_ALL\n\nimport warnings\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-08T09:01:23.034698Z","iopub.execute_input":"2023-11-08T09:01:23.035618Z","iopub.status.idle":"2023-11-08T09:01:23.044530Z","shell.execute_reply.started":"2023-11-08T09:01:23.035545Z","shell.execute_reply":"2023-11-08T09:01:23.042889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nCONFIG = {\n    \"is_submission\": False,\n    \"datetime_now\": datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"), \n    \"n_fold\": 5,\n    'fold': 1,\n    'test_fold': 0,\n    \"seed\": 42,\n    \"img_size\": 512,\n    \"crop_vertical\":True,\n    \"model_name\": \"tf_efficientnetv2_s_in21ft1k\",   # \"tf_efficientnet_b0_ns\", # \"tf_efficientnetv2_s_in21ft1k\"\n    \"checkpoint_path\": \"/kaggle/input/tf-efficientnetv2-s-in21ft1k/tf_efficientnetv2_s_in21ft1k.pth\",\n    \"num_classes\": 5,\n    \"valid_batch_size\": 16,\n    \"train_batch_size\": 16,\n    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n    # \"model_path\": '/kaggle/input/efficientnetb0-training-crop-images/best_model_checkpoint2023-10-26_09-10-29.pth',\n    \"encoder_path\": \"/kaggle/input/effnet-version-28/label_encoder_2023-11-21_15-45-54.pkl\"\n}\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_or_create_experiment_id(name):\n    exp = mlflow.get_experiment_by_name(name)\n    if exp is None:\n        exp_id = mlflow.create_experiment(name)\n        return exp_id\n    return exp.experiment_id","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Datasets & Preprocessing","metadata":{}},{"cell_type":"code","source":"def get_train_file_path(df_train_row, TRAIN_DIR, thumbnail=False):\n    if thumbnail:\n        return f\"{TRAIN_DIR}/{df_train_row.image_id}_thumbnail.png\"\n    else:\n        return f\"{TRAIN_DIR}/{df_train_row.image_id}.png\"\n\n\ndef get_test_file_path(image_id,TEST_DIR):\n    if os.path.exists(f\"{TEST_DIR}/{image_id}.png\"):\n        return f\"{TEST_DIR}/{image_id}.png\"\n    else:\n        return f\"{ALT_TEST_DIR}/{image_id}.png\"","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:54:48.504494Z","iopub.execute_input":"2023-11-29T17:54:48.506684Z","iopub.status.idle":"2023-11-29T17:54:48.526390Z","shell.execute_reply.started":"2023-11-29T17:54:48.506625Z","shell.execute_reply":"2023-11-29T17:54:48.524784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UBCDataset(Dataset):\n    def __init__(self, df, transforms=None, apply_vertical_crop=True):\n        self.df = df\n        self.filenames = df.file_path.values\n        self.labels =  df.target_label.values\n        self.transforms = transforms\n        self.apply_vertical_crop = apply_vertical_crop\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_path = self.filenames[idx]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if self.apply_vertical_crop:\n            img = crop_vertical(img)\n                \n        if self.transforms:\n            img = self.transforms(image=img)[\"image\"]\n            \n        return {\n            \"image\": img,\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long)\n               }\n\ndef crop_vertical(image):\n    \"\"\"\n    Function crops images if multiple slices contained and separated by black vertical background.\n    \"\"\"\n    vertical_sum = np.sum(image, axis=(0, 2))\n\n    # Identify the positions where the sum is zero\n    zero_positions = np.where(vertical_sum == 0)[0]\n\n    if len(zero_positions)==0:\n        cropped_images = [image]\n    else:\n        # If the image does not start with a black area, add index 0\n        if zero_positions[0] != 0:\n            zero_positions = np.insert(zero_positions, 0, 0)\n\n        # If the image does not end with a black area, add the image width\n        if zero_positions[-1] != image.shape[1] - 1:\n            zero_positions = np.append(zero_positions, image.shape[1] - 1)\n\n        start_idx = zero_positions[0]\n        cropped_images = []\n\n        for idx in range(1, len(zero_positions)):\n            end_idx = zero_positions[idx]\n            if end_idx - start_idx > 1:  # If the width of the cropped section is greater than 1\n                cropped = image[:, start_idx:end_idx]\n                # only include samples which are of min size\n                if cropped.shape[1]>200:  \n                    cropped_images.append(cropped)\n                    # cv2.imwrite(f\"{save_prefix}_{idx}.jpg\", cropped)\n            start_idx = end_idx\n\n    final_crops = []\n    # remove black bars above/below the crops \n    for cropped in cropped_images:\n        horizontal_sum = np.sum(cropped, axis=(1, 2))\n        zero_positions = np.where(horizontal_sum == 0)[0]\n        img_ = np.delete(cropped, zero_positions, axis=0)\n        final_crops.append(img_)\n    if len(final_crops)==0:\n        return image\n    return final_crops[0]\n\n\ndef custom_center_crop_or_resize(image, crop_size):\n    # If both dimensions of the image are greater than or equal to the desired size, apply CenterCrop\n    if image.shape[0] >= crop_size[0] and image.shape[1] >= crop_size[1]:\n        return A.CenterCrop(crop_size[0], crop_size[1])(image=image)[\"image\"]\n    # Else, just resize the image to the desired size\n    else:\n        return A.Resize(crop_size[0], crop_size[1])(image=image)[\"image\"]","metadata":{"execution":{"iopub.status.busy":"2023-11-08T09:01:23.563750Z","iopub.execute_input":"2023-11-08T09:01:23.564635Z","iopub.status.idle":"2023-11-08T09:01:23.580361Z","shell.execute_reply.started":"2023-11-08T09:01:23.564568Z","shell.execute_reply":"2023-11-08T09:01:23.578762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _color_means(img_path):\n    img = np.array(Image.open(img_path))\n    mask = np.sum(img[..., :3], axis=2) == 0\n    img[mask, :] = 255\n    if np.max(img) > 1.5:\n        img = img / 255.0\n    clr_mean = {i: np.mean(img[..., i]) for i in range(3)}\n    clr_std = {i: np.std(img[..., i]) for i in range(3)}\n    return clr_mean, clr_std\n\n\"\"\"\nls_images = glob.glob(os.path.join(TRAIN_DIR, \"*\", \"*.png\"))\nclr_mean_std = Parallel(n_jobs=os.cpu_count())(delayed(_color_means)(fn) for fn in tqdm(ls_images[:9000]))\n\nimg_color_mean = pd.DataFrame([c[0] for c in clr_mean_std]).describe()\ndisplay(img_color_mean.T)\nimg_color_std = pd.DataFrame([c[1] for c in clr_mean_std]).describe()\ndisplay(img_color_std.T)\n\nimg_color_mean = list(img_color_mean.T[\"mean\"])\nimg_color_std = list(img_color_std.T[\"mean\"])\nprint(f\"{img_color_mean=}\\n{img_color_std=}\")\n\"\"\"\n\n## histogram matching \n#from skimage.exposure import match_histograms\n#ref_img = np.array(Image.open(\"/kaggle/input/tiles-of-cancer-2048px-scale-0-25/10077/000067_16-3.png\"))\n#bef_img = np.array(Image.open(\"/kaggle/input/tiles-of-cancer-2048px-scale-0-25/12522/000028_6-2.png\"))\n#start = time.time()\n#aft_img = match_histograms(bef_img, ref_img, channel_axis=-1)\n#print(time.time()-start)\n\n\n\"\"\"        \nA.Normalize(\n    mean=[0.485, 0.456, 0.406], \n    std=[0.229, 0.224, 0.225], \n    max_pixel_value=255.0, \n    p=1.0\n),\n\"\"\"\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_color_mean=[0.8661704276539922, 0.7663107094675368, 0.8574260897185548]\nimg_color_std=[0.08670629753900036, 0.11646580094195522, 0.07164169171856792]\n\n\"\"\"\ndata_transforms = {\n    \"train\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.RandomBrightnessContrast(p=0.75),\n        A.ShiftScaleRotate(p=0.75),\n        A.OneOf([\n        A.GaussNoise(var_limit=[10, 50]),\n        A.GaussianBlur(),\n        A.MotionBlur(),\n        ], p=0.4),\n        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n        A.CoarseDropout(max_holes=1, max_width=int(512* 0.3), max_height=int(512* 0.3),\n        mask_fill_value=0, p=0.5),\n        A.Normalize(img_color_mean, img_color_std), \n        ToTensorV2()], p=1.),\n     \n    \"valid\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.Normalize(img_color_mean, img_color_std), \n        ToTensorV2()], p=1.),\n    \n    \"test\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.Normalize(img_color_mean, img_color_std), \n        ToTensorV2()], p=1.),\n    \n    \n\n}\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-11-08T09:01:23.642588Z","iopub.execute_input":"2023-11-08T09:01:23.642990Z","iopub.status.idle":"2023-11-08T09:01:23.650299Z","shell.execute_reply.started":"2023-11-08T09:01:23.642965Z","shell.execute_reply":"2023-11-08T09:01:23.649530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2. Tiles Dataset\n\n### 2.2.1. Train Dataset","metadata":{}},{"cell_type":"code","source":"class CancerTilesDataset(Dataset):\n    @staticmethod\n    def get_img_dir(data_row):\n        # based on if is_tma or not we select the respective image path\n        if data_row.is_tma == True:\n            return glob.glob(os.path.join(\"/kaggle/input/ubc-tma-tiles-512-05scale/UBC_TMA_tiles_1024p_scale05\", str(data_row.image_id), \"*.png\"))\n        else:\n            return glob.glob(os.path.join(\"/kaggle/input/tiles-of-cancer-2048px-scale-0-25\", str(data_row.image_id), \"*.png\")) \n\n    def __init__(\n        self,\n        df_data,\n        path_img_dir: str =  '',\n        transforms = None,\n        mode: str = 'train',\n        labels_lut = None,\n        white_thr: int = 225,\n        thr_max_bg: float = 0.2,\n        train_val_split: float = 0.90,\n        n_tiles: int = 1,\n        tma_weight: float = 1.0,\n    ):\n        assert os.path.isdir(path_img_dir)\n        self.path_img_dir = path_img_dir\n        self.transforms = transforms\n        self.mode = mode\n        self.white_thr = white_thr\n        self.thr_max_bg = thr_max_bg\n        self.train_val_split = train_val_split\n        self.n_tiles = n_tiles\n        self.tma_weight = tma_weight\n\n        self.data = df_data\n        self.labels_unique = sorted(self.data[\"label\"].unique())\n        self.labels_lut = labels_lut or {lb: i for i, lb in enumerate(self.labels_unique)}\n\n        self.data.is_tma = self.data.is_tma.astype(bool)\n        self.data = self.data.sample(frac=1, random_state=42).reset_index(drop=True)\n\n        # split dataset\n        assert 0.0 <= self.train_val_split <= 1.0\n        frac = int(self.train_val_split * len(self.data))\n        self.data = self.data[:frac] if mode in [\"train\", \"test\"] else self.data[frac:]\n        self.img_dirs = [CancerTilesDataset.get_img_dir(row) for i, row in self.data.iterrows()] \n        self.img_dirs = self.img_dirs * self.n_tiles\n        self.img_paths = []\n        #print(f\"missing: {sum([not os.path.isfile(os.path.join(self.path_img_dir, im))\n        #                       for im in self.img_names])}\")\n        # self.labels = list(self.data['label'])\n        self.labels =  np.array(self.data.target_label.values.tolist() * self.n_tiles)\n        \n        # set sample weights \n        self.sample_weights = [self.tma_weight if is_tma == True else 1 for is_tma in self.data[\"is_tma\"]] \n        self.sample_weights =  np.array(self.sample_weights * self.n_tiles)\n        \n    def __getitem__(self, idx: int) -> tuple:\n        nth_iteration = idx//len(self.data)\n        if self.mode==\"train\":\n            random.seed()\n        else:\n            random.seed(CONFIG[\"seed\"]+nth_iteration)\n        random.shuffle(self.img_dirs[idx])\n        for img_path in self.img_dirs[idx]:\n            assert os.path.isfile(img_path), f\"missing: {img_path}\"\n            tile = cv2.imread(img_path)\n            tile = cv2.cvtColor(tile, cv2.COLOR_BGR2RGB)\n        \n            # tile = np.array(Image.open(img_path))[..., :3]\n            black_bg = np.sum(tile, axis=2) == 0\n            tile[black_bg, :] = 255\n            mask_bg = np.mean(tile, axis=2) > self.white_thr\n            if np.sum(mask_bg) < (np.prod(mask_bg.shape) * self.thr_max_bg):\n                self.img_paths.append(img_path)\n                print(f\"Idx: {idx}, Path: {img_path}, len img_pths: {len(self.img_paths)}, nunique img_paths: {len(set(self.img_paths))}\")\n                break\n\n        # augmentation\n        if self.transforms:\n            tile = self.transforms(image=tile)[\"image\"]\n        #print(f\"img dim: {img.shape}\")\n        return {\n            \"image\": tile,\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n               }\n    def __len__(self) -> int:\n        return len(self.img_dirs)\n    \n    def get_sample_weights(self):\n        return torch.from_numpy(self.sample_weights).double()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2.2 Inference Dataset","metadata":{}},{"cell_type":"code","source":"def delete_tiles(directory_path):\n    if os.path.isdir(directory_path):\n        for filename in os.listdir(directory_path):\n            if os.path.isfile(os.path.join(directory_path, filename)):\n                os.remove(os.path.join(directory_path, filename))\n\ndef extract_image_tiles(\n    p_img, img_id, tmp_dir, size: int = 2048, scale: float = 0.5,\n    drop_thr: float = 0.8, white_thr: int = 245, max_samples: int = 50\n) -> list:\n    delete_tiles(tmp_dir)  # empty directory from previous images\n    im = pyvips.Image.new_from_file(p_img)\n    w = h = size\n    # https://stackoverflow.com/a/47581978/4521646\n    idxs = [(y, y + h, x, x + w) for y in range(0, im.height, h) for x in range(0, im.width, w)]\n    # random subsample\n    max_samples = max_samples if isinstance(max_samples, int) else int(len(idxs) * max_samples)\n    random.seed(42)\n    random.shuffle(idxs)\n    images = []\n    i = 0\n    for y, y_, x, x_ in (idxs):\n        i += 1\n        img_path = f\"{tmp_dir}/{str(i)}.png\"\n        # https://libvips.github.io/pyvips/vimage.html#pyvips.Image.crop\n        tile = im.crop(x, y, min(w, im.width - x), min(h, im.height - y)).numpy()[..., :3]\n        if tile.shape[:2] != (h, w):\n            tile_ = tile\n            tile_size = (h, w) if tile.ndim == 2 else (h, w, tile.shape[2])\n            tile = np.zeros(tile_size, dtype=tile.dtype)\n            tile[:tile_.shape[0], :tile_.shape[1], ...] = tile_\n        black_bg = np.sum(tile, axis=2) == 0\n        tile[black_bg, :] = 255\n        mask_bg = np.mean(tile, axis=2) > white_thr\n        if np.sum(mask_bg) >= (np.prod(mask_bg.shape) * drop_thr):\n            #print(f\"skip almost empty tile: {k:06}_{int(x_ / w)}-{int(y_ / h)}\")\n            continue\n        # print(tile.shape, tile.dtype, tile.min(), tile.max())\n        new_size = int(size * scale), int(size * scale)\n        tile = Image.fromarray(tile).resize(new_size, Image.LANCZOS)\n        tile.save(img_path)\n        images.append(img_path)\n        # need to set counter check as some empty tiles could be skipped earlier\n        if len(images) >= max_samples:\n            break\n    return images\n\n\nclass TilesInferenceDataset(Dataset):\n\n    def __init__(\n        self,\n        img_path: str,\n        img_id: str = None,\n        tmp_dir: str = None,\n        size: int = 2048,\n        scale: float = 0.25,\n        white_thr: int = 225,\n        thr_max_bg: float = 0.6,\n        max_samples: int = 30,\n        transforms = None,\n        is_submission: bool = True,\n    ):\n        self.max_samples = max_samples\n        self.white_thr = white_thr\n        self.thr_max_bg = thr_max_bg\n        self.is_submission = is_submission\n        \n        self.transforms = transforms\n        if self.is_submission:\n            # print(img_path)\n            assert os.path.isfile(img_path)\n            self.imgs = extract_image_tiles(\n                img_path, img_id, tmp_dir, size=size, scale=scale,\n                drop_thr=self.thr_max_bg, max_samples=max_samples)\n        else:  # test\n            all_imgs = glob.glob(os.path.join(img_path, img_id, \"*.png\"))\n            # Filter images based on background threshold\n            self.imgs = []\n            for img_path in all_imgs:\n                img = cv2.imread(img_path)\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n                black_bg = np.sum(img, axis=2) == 0\n                img[black_bg, :] = 255\n                mask_bg = np.mean(img, axis=2) > self.white_thr\n                if np.sum(mask_bg) <= (np.prod(mask_bg.shape) * self.thr_max_bg):\n                    self.imgs.append(img_path)  # Include this image\n            self.imgs = self.imgs[:self.max_samples]\n            # print(self.imgs)\n\n    def __getitem__(self, idx: int) -> tuple:\n        img = cv2.imread(self.imgs[idx])\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        # filter background\n        mask = np.sum(img, axis=2) == 0\n        img[mask, :] = 255\n        if np.max(img) < 1.5:\n            img = np.clip(img * 255, 0, 255).astype(np.uint8)\n        if self.transforms:\n            img = self.transforms(image=img)[\"image\"]\n        return img\n\n    def __len__(self) -> int:\n        return len(self.imgs)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Model Architecture","metadata":{}},{"cell_type":"code","source":"class GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6):\n        super(GeM, self).__init__()\n        self.p = nn.Parameter(torch.ones(1)*p)\n        self.eps = eps\n\n    def forward(self, x):\n        return self.gem(x, p=self.p, eps=self.eps)\n        \n    def gem(self, x, p=3, eps=1e-6):\n        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n        \n    def __repr__(self):\n        return self.__class__.__name__ + \\\n                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n                ', ' + 'eps=' + str(self.eps) + ')'\n\n\nclass UBCModel(nn.Module):\n    '''\n    EfficientNet B0 fine-tune.\n    '''\n    def __init__(self, model_name, num_classes, pretrained=False, checkpoint_path=None):\n        '''\n        Fine tune for EfficientNetB0\n        Args\n            n_classes : int - Number of classification categories.\n            learnable_modules : tuple - Names of the modules to fine-tune.\n        Return\n            \n        '''\n        super(UBCModel, self).__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained, checkpoint_path=checkpoint_path)\n\n        in_features = self.model.classifier.in_features\n        self.model.classifier = nn.Identity()\n        self.model.global_pool = nn.Identity()\n        self.pooling = GeM()\n        self.linear = nn.Linear(in_features, num_classes)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, images):\n        \"\"\"\n        Forward function for the fine-tuned model\n        Args\n            x: \n        Return\n            result\n        \"\"\"\n        features = self.model(images)\n        pooled_features = self.pooling(features).flatten(1)\n        output = self.linear(pooled_features)\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-11-08T09:01:23.860451Z","iopub.execute_input":"2023-11-08T09:01:23.861541Z","iopub.status.idle":"2023-11-08T09:01:23.873475Z","shell.execute_reply.started":"2023-11-08T09:01:23.861465Z","shell.execute_reply":"2023-11-08T09:01:23.872397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=5, verbose=False, delta=0, path='checkpoint.pth', trace_func=print):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = float('inf')\n        self.delta = delta\n        self.path = path\n        self.trace_func = trace_func\n\n    def __call__(self, val_loss, model):\n        score = -val_loss\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Saves model when validation loss decreases.'''\n        if self.verbose:\n            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...')\n        torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss","metadata":{"execution":{"iopub.status.busy":"2023-11-08T09:01:23.943061Z","iopub.execute_input":"2023-11-08T09:01:23.943590Z","iopub.status.idle":"2023-11-08T09:01:23.954300Z","shell.execute_reply.started":"2023-11-08T09:01:23.943557Z","shell.execute_reply":"2023-11-08T09:01:23.952590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Training","metadata":{}},{"cell_type":"code","source":"\n\ndef fetch_scheduler(optimizer, CONFIG):\n    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], \n                                                   eta_min=CONFIG['min_lr'], verbose=False)\n    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n        CONFIG['T_0'] = 10\n        CONFIG['T_mult'] = 2\n        CONFIG['min_lr'] = 1e-6\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CONFIG['T_0'], T_mult=CONFIG['T_mult'],\n                                                             eta_min=CONFIG['min_lr'], verbose=False)\n    elif CONFIG['scheduler'] == 'ReduceLROnPlateau':\n        scheduler =  ReduceLROnPlateau(optimizer, mode='min', factor=kwargs.get('factor', 0.1), patience=kwargs.get('patience', 5), verbose=False)\n    elif CONFIG['scheduler'] == 'LambdaLR':\n        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda)\n    elif CONFIG['scheduler'] == None:\n        return None\n    return scheduler\n\ndef get_optimizer(optimizer_name, model):\n    if optimizer_name.lower() == \"adam\":\n        CONFIG['learning_rate'] = 1e-4\n        CONFIG['weight_decay'] = 1e-5\n        CONFIG['betas'] = (0.9, 0.999)\n        CONFIG['eps'] = 1e-8\n        optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'], betas=CONFIG['betas'], eps=CONFIG['eps'],  weight_decay=CONFIG['weight_decay'])\n    elif optimizer_name.lower() == \"sgd\":\n        CONFIG['learning_rate'] = 1e-3\n        CONFIG['weight_decay'] = 1e-3\n        CONFIG['momentum'] = 1e-3\n        optimizer = optim.SGD(model.parameters(), lr=CONFIG['learning_rate'], momentum=CONFIG['momentum'], weight_decay=CONFIG['weight_decay'])\n    elif optimizer_name.lower() == \"radam\":\n        CONFIG['learning_rate'] = 1e-4\n        CONFIG['weight_decay'] = 0\n        CONFIG['betas'] = (0.9, 0.999)\n        CONFIG['eps'] = 1e-8\n        optimizer = torch_optimizer.RAdam(\n            model.parameters(),\n            lr= CONFIG['learning_rate'],\n            betas=CONFIG['betas'],\n            eps=CONFIG['eps'],\n            weight_decay=CONFIG['weight_decay'],\n        )\n    elif optimizer_name.lower() == \"rmsprop\":\n        CONFIG['learning_rate'] = 0.256\n        CONFIG['alpha'] = 0.9\n        CONFIG['momentum'] = 0.9\n        CONFIG['weight_decay'] = 1e-5\n        optimizer = optim.RMSprop(model.parameters(), lr=CONFIG['learning_rate'], alpha=CONFIG['learning_rate'], \n                                  momentum=CONFIG['learning_rate'], weight_decay=CONFIG['learning_rate'])\n    else:\n        raise ValueError(\"Invalid Optimizer given!\")\n    return optimizer\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-08T09:01:24.025798Z","iopub.execute_input":"2023-11-08T09:01:24.026205Z","iopub.status.idle":"2023-11-08T09:01:24.039156Z","shell.execute_reply.started":"2023-11-08T09:01:24.026177Z","shell.execute_reply":"2023-11-08T09:01:24.037687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef convert_dict_to_tensor(dict_):\n    \"\"\"Converts the values of a dict into a PyTorch tensor.\"\"\"\n\n    # Create a new PyTorch tensor\n    tensor = torch.empty(len(dict_))\n\n    # Iterate over the dict and for each key-value pair, convert the value to a PyTorch tensor and add it to the new tensor\n    for i, (key, value) in enumerate(dict_.items()):\n        tensor[i] = value\n\n    # Return the new tensor\n    return tensor\n\ndef get_class_weights(df_train):\n    label_counts = df_train.target_label.value_counts().sort_index().to_dict()\n    ratios_dict = {}\n    for key,val in label_counts.items():\n        ratios_dict[key] = val / df_train.shape[0]\n    ratios_dict\n    weights = {}\n    sum_weights = 0\n    for key, val in ratios_dict.items():\n        weights[key] = 1 / val\n        sum_weights +=  1 / val\n    for key, val in weights.items():\n        weights[key] = val / sum_weights\n    weight_tensor = convert_dict_to_tensor(weights)\n    return weight_tensor\n\ndef get_dataloaders(df, TRAIN_DIR, CONFIG, data_transforms, n_tiles=1, train_val_split=0.9, apply_sampler=True, tma_weight=1, sample_fac=1):\n    # df_train = df[df[\"kfold\"]!=fold].reset_index(drop=True)\n    train_dataset = CancerTilesDataset(df, TRAIN_DIR, transforms=data_transforms[\"train\"], mode=\"train\", n_tiles=n_tiles, train_val_split=train_val_split, tma_weight=tma_weight)\n    if apply_sampler:\n        samples_weights = train_dataset.get_sample_weights()\n        train_sampler = WeightedRandomSampler(samples_weights, len(samples_weights)*sample_fac)\n    else:\n        train_sampler = None\n    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], num_workers=2, sampler=train_sampler, shuffle=False, pin_memory=True)\n    \n    valid_dataset = CancerTilesDataset(df, TRAIN_DIR, transforms=data_transforms[\"valid\"], mode=\"valid\", n_tiles=n_tiles, train_val_split=train_val_split, tma_weight=tma_weight)\n    if apply_sampler:\n        samples_weights = valid_dataset.get_sample_weights()\n        valid_sampler = WeightedRandomSampler(samples_weights, len(samples_weights)*sample_fac)\n    else:\n        valid_sampler=None\n    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], num_workers=2, sampler=valid_sampler, shuffle=False, pin_memory=True)\n    print(f\"Len Train Dataset: {len(train_dataset)}, Len Validation Dataset: {len(valid_dataset)}\" )\n    return train_loader, valid_loader, df\n\ndef print_logged_info(r):\n    tags = {k: v for k, v in r.data.tags.items() if not k.startswith(\"mlflow.\")}\n    artifacts = [f.path for f in MlflowClient().list_artifacts(r.info.run_id, \"model\")]\n    print(f\"run_id: {r.info.run_id}\")\n    print(f\"artifacts: {artifacts}\")\n    print(f\"params: {r.data.params}\")\n    print(f\"metrics: {r.data.metrics}\")\n    print(f\"tags: {tags}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-08T09:01:24.095699Z","iopub.execute_input":"2023-11-08T09:01:24.096089Z","iopub.status.idle":"2023-11-08T09:01:24.104752Z","shell.execute_reply.started":"2023-11-08T09:01:24.096062Z","shell.execute_reply":"2023-11-08T09:01:24.103358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Inference & Evaluation","metadata":{}},{"cell_type":"code","source":"def eval_predictions(df):\n    # Total Accuracy\n    total_accuracy = accuracy_score(df['target_label'], df['label'])\n\n    # Balanced Accuracy\n    balanced_accuracy = balanced_accuracy_score(df['target_label'], df['label'])\n\n    # F1 Score\n    f1 = f1_score(df['target_label'], df['label'], average='weighted')\n\n    # Accuracy Per Class\n    cm = confusion_matrix(df['target_label'], df['label'])\n    class_accuracy = cm.diagonal() / cm.sum(axis=1)\n\n    print(f\"Total Accuracy: {total_accuracy}\")\n    print(f\"Balanced Accuracy: {balanced_accuracy}\")\n    print(f\"F1 Score: {f1}\")\n    print(f\"Accuracy Per Class: {class_accuracy}\")\n    display(cm)\n    \ndef most_frequent(List):\n    return max(set(List), key = List.count)\n\ndef score_predictions(preds, method=\"sum\", N=10):\n    if method==\"sum\":  # sum up the predictions of all tiles/models\n        lb = np.argmax(np.sum(preds, axis=0))\n    elif method in [\"most_frequent\", \"most_votes\", \"majority_vote\"]:  # get majority vote over all tiles/models \n        lb = most_frequent(np.argmax(preds, axis=1).tolist())\n    elif method == \"n_highest_sum\":  # sum up predictions of N-most decicive tiles/models\n        max_vals = np.max(preds, axis=1).tolist()\n        max_idxs = [max_vals.index(i) for i in heapq.nlargest(N, max_vals)]\n        n_tiles_preds = np.take(preds, max_idxs, axis=0).tolist()\n        lb = np.argmax(np.sum(n_tiles_preds, axis=0))\n    else:\n        print(\"No method found: Apply Sum Method for Scoring predictions!\")\n        lb = np.argmax(np.sum(preds, axis=0))\n    return lb","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def infer_single_image_ensemble(idx_row, models, CONFIG, score_method=\"sum\", max_samples=30, is_submission=True, device=\"cuda\") -> dict:\n    \"\"\"\n    Create tiled-dataset based on test image.\n    Iterate throuh all tiles and apply model prediction.\n    Select highest of sum of all logits.\n    \"\"\"\n    row = dict(idx_row[1])\n    img_id = str(row[\"image_id\"])\n    result = {\"image_id\": img_id}\n    if is_submission:\n        print(\"Image ID: \", img_id)\n        result[\"target_label\"] = row[\"target_label\"]\n        # prepare data - cut and load tiles\n        dataset = TilesInferenceDataset(\n            os.path.join(\"/kaggle/input/UBC-OCEAN/\", \"train_images\", f\"{img_id}.png\"),\n            size=2048, scale=0.25, transforms=data_transforms[\"valid\"], max_samples=max_samples)\n    else:\n        print(\"Image ID: \", img_id)\n        dataset = TilesInferenceDataset(\n            \"/kaggle/input/tiles-of-cancer-2048px-scale-0-25\",\n            img_id,\n            size=2048, scale=0.25, transforms=data_transforms[\"valid\"], is_submission=is_submission, max_samples=max_samples)\n        result[\"target_label\"] = row[\"target_label\"]\n        \n    dataloader = DataLoader(\n        dataset, batch_size=CONFIG[\"valid_batch_size\"], num_workers=1, shuffle=False,\n        # see: https://github.com/pytorch/pytorch/issues/44687#issuecomment-790842173\n        multiprocessing_context=get_context('loky')\n    )\n    if not len(dataset):\n        if not is_submission: \n            print (f\"seem no tiles were cut for `{row['image_id']}`. Set to label 0\")\n            print (row)\n        result[\"label\"] = 0\n        return result\n    \n    if not isinstance(models, list):\n        models = [models]\n    \n    model_preds_sum = []\n    for i,model in enumerate(models):\n        #print(f\"Apply Model {i+1} of {len(models)}\")\n        model = model.to(device)\n        model.eval()\n        preds = []\n        # iterate over images and collect predictions | \n        for imgs in dataloader:\n            # print(f\"{imgs.shape}\")\n            with torch.no_grad():\n                pred = model(imgs.to(device))\n            preds += pred.cpu().numpy().tolist()\n        if not is_submission:\n            print(f\"Sum contrinution from all tiles: {np.sum(preds, axis=0)}\")\n            print(f\"Max contribution over all tiles: {np.max(preds, axis=0)}\")\n        model_preds_sum.append(preds)\n    model_preds_sum = sum(model_preds_sum, [])\n    # decide label\n    prediction = score_predictions(model_preds_sum, method=score_method)\n    result[\"label\"] = prediction\n    if not is_submission: \n        result[\"target_label\"] = row[\"target_label\"]\n    result[\"predictions\"] = np.sum(model_preds_sum, axis=0).tolist()\n    print(result)\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-11-08T09:01:24.160819Z","iopub.execute_input":"2023-11-08T09:01:24.161342Z","iopub.status.idle":"2023-11-08T09:01:24.173773Z","shell.execute_reply.started":"2023-11-08T09:01:24.161304Z","shell.execute_reply":"2023-11-08T09:01:24.172311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}