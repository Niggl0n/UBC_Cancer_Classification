{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Niggl0n/UBC_Cancer_Classification/blob/main/cancer-oos-outlier-convautoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11e9d597",
      "metadata": {
        "papermill": {
          "duration": 0.010638,
          "end_time": "2023-11-22T13:16:58.151954",
          "exception": false,
          "start_time": "2023-11-22T13:16:58.141316",
          "status": "completed"
        },
        "tags": [],
        "id": "11e9d597"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "**This is a basic CNN Model training notebook**\n",
        "\n",
        "It is based on:\n",
        "- Thumbnail images\n",
        "- Basic data transformation (using Albumentation):\n",
        "    - resizing images to 512x512\n",
        "    - normalizing pixel values\n",
        "- CNN Architecture\n",
        "\n",
        "\n",
        "**Todos:**\n",
        "\n",
        "- Learn about Dataset & DataLoader\n",
        "- add augmentations (albumentation)\n",
        "- gem pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "121bde67",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-22T13:16:58.174088Z",
          "iopub.status.busy": "2023-11-22T13:16:58.173708Z",
          "iopub.status.idle": "2023-11-22T13:16:58.178679Z",
          "shell.execute_reply": "2023-11-22T13:16:58.177884Z"
        },
        "papermill": {
          "duration": 0.018346,
          "end_time": "2023-11-22T13:16:58.180675",
          "exception": false,
          "start_time": "2023-11-22T13:16:58.162329",
          "status": "completed"
        },
        "tags": [],
        "id": "121bde67"
      },
      "outputs": [],
      "source": [
        "# !pip install --quiet torch_optimizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"hello world\")"
      ],
      "metadata": {
        "id": "eoR-sdBTU9Um"
      },
      "id": "eoR-sdBTU9Um",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fbff414",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-22T13:16:58.202700Z",
          "iopub.status.busy": "2023-11-22T13:16:58.202340Z",
          "iopub.status.idle": "2023-11-22T13:17:19.422446Z",
          "shell.execute_reply": "2023-11-22T13:17:19.421396Z"
        },
        "papermill": {
          "duration": 21.234055,
          "end_time": "2023-11-22T13:17:19.425063",
          "exception": false,
          "start_time": "2023-11-22T13:16:58.191008",
          "status": "completed"
        },
        "tags": [],
        "id": "1fbff414",
        "outputId": "9fe0fe96-da1c-46ad-b71d-9c1153ad9b6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
            "ydata-profiling 4.3.1 requires dacite>=1.8, but you have dacite 1.6.0 which is incompatible.\r\n",
            "ydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --quiet mlflow dagshub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19240782",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2023-11-22T13:17:19.448419Z",
          "iopub.status.busy": "2023-11-22T13:17:19.447563Z",
          "iopub.status.idle": "2023-11-22T13:17:57.769572Z",
          "shell.execute_reply": "2023-11-22T13:17:57.768728Z"
        },
        "papermill": {
          "duration": 38.33645,
          "end_time": "2023-11-22T13:17:57.772158",
          "exception": false,
          "start_time": "2023-11-22T13:17:19.435708",
          "status": "completed"
        },
        "tags": [],
        "id": "19240782"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input/tiles-of-cancer-2048px-scale-0-25'):\n",
        "    for filename in filenames:\n",
        "        # print(os.path.join(dirname, filename))\n",
        "        continue\n",
        "\n",
        "\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5750adf",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-22T13:17:57.795802Z",
          "iopub.status.busy": "2023-11-22T13:17:57.795299Z",
          "iopub.status.idle": "2023-11-22T13:18:16.288071Z",
          "shell.execute_reply": "2023-11-22T13:18:16.287203Z"
        },
        "papermill": {
          "duration": 18.50677,
          "end_time": "2023-11-22T13:18:16.290499",
          "exception": false,
          "start_time": "2023-11-22T13:17:57.783729",
          "status": "completed"
        },
        "tags": [],
        "id": "f5750adf",
        "outputId": "84aff8f0-b01a-462d-d752-4b112d7c285b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import cv2\n",
        "import datetime\n",
        "import math\n",
        "import copy\n",
        "import time\n",
        "import random\n",
        "import glob\n",
        "from matplotlib import pyplot as plt\n",
        "from skimage import io\n",
        "\n",
        "\n",
        "# For data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Pytorch Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.cuda import amp\n",
        "import torchvision\n",
        "\n",
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "\n",
        "# Utils\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "# Sklearn Imports\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import balanced_accuracy_score, confusion_matrix, f1_score, mean_absolute_error,mean_squared_error\n",
        "\n",
        "# For Image Models\n",
        "import timm\n",
        "\n",
        "import dagshub\n",
        "from getpass import getpass\n",
        "import mlflow.pytorch\n",
        "from mlflow import MlflowClient\n",
        "\n",
        "# Albumentations for augmentations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# For colored terminal text\n",
        "from colorama import Fore, Back, Style\n",
        "b_ = Fore.BLUE\n",
        "sr_ = Style.RESET_ALL\n",
        "\n",
        "import warnings\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# For descriptive error messages\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cf94f5f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-22T13:18:16.313234Z",
          "iopub.status.busy": "2023-11-22T13:18:16.312857Z",
          "iopub.status.idle": "2023-11-22T13:18:19.621449Z",
          "shell.execute_reply": "2023-11-22T13:18:19.620420Z"
        },
        "papermill": {
          "duration": 3.322455,
          "end_time": "2023-11-22T13:18:19.623769",
          "exception": false,
          "start_time": "2023-11-22T13:18:16.301314",
          "status": "completed"
        },
        "tags": [],
        "id": "5cf94f5f",
        "outputId": "6e367e9e-839b-484a-d0a4-1119e3068c49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='mlflow-artifacts:/5a5b33e1ffb34569bbed108e47ed4be3', creation_time=1700649468372, experiment_id='4', last_update_time=1700649468372, lifecycle_stage='active', name='UBC_OOS_Outlier_Enc_SVM', tags={}>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.environ[\"MLFLOW_TRACKING_USERNAME\"]=\"Niggl0n\"\n",
        "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"7a3590e8c5558d4598dacc7810befa70a4baac9e\"\n",
        "os.environ['MLFLOW_TRACKING_PROJECTNAME'] = \"UBC_Cancer_Classification\"\n",
        "dagshub.auth.add_app_token(\"7a3590e8c5558d4598dacc7810befa70a4baac9e\")\n",
        "mlflow.set_tracking_uri(f'https://dagshub.com/' + os.environ['MLFLOW_TRACKING_USERNAME'] + '/' + os.environ['MLFLOW_TRACKING_PROJECTNAME'] + '.mlflow')\n",
        "mlflow.set_experiment(experiment_name=\"UBC_OOS_Outlier_Enc_SVM\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d6cb728",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-22T13:18:19.648215Z",
          "iopub.status.busy": "2023-11-22T13:18:19.647822Z",
          "iopub.status.idle": "2023-11-22T13:18:19.791887Z",
          "shell.execute_reply": "2023-11-22T13:18:19.790814Z"
        },
        "papermill": {
          "duration": 0.15792,
          "end_time": "2023-11-22T13:18:19.794080",
          "exception": false,
          "start_time": "2023-11-22T13:18:19.636160",
          "status": "completed"
        },
        "tags": [],
        "id": "1d6cb728",
        "outputId": "6bffe851-2f68-47a4-acd7-3970f28d181c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'4'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_or_create_experiment_id(name):\n",
        "    exp = mlflow.get_experiment_by_name(name)\n",
        "    if exp is None:\n",
        "        exp_id = mlflow.create_experiment(name)\n",
        "        return exp_id\n",
        "    return exp.experiment_id\n",
        "\n",
        "mlflow_experiment_id = get_or_create_experiment_id(\"UBC_OOS_Outlier_Enc_SVM\")\n",
        "mlflow_experiment_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02779d38",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-22T13:18:19.817184Z",
          "iopub.status.busy": "2023-11-22T13:18:19.816767Z",
          "iopub.status.idle": "2023-11-22T13:18:19.883429Z",
          "shell.execute_reply": "2023-11-22T13:18:19.882430Z"
        },
        "papermill": {
          "duration": 0.080563,
          "end_time": "2023-11-22T13:18:19.885450",
          "exception": false,
          "start_time": "2023-11-22T13:18:19.804887",
          "status": "completed"
        },
        "tags": [],
        "id": "02779d38"
      },
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    \"anomaly_class\": \"HGSC\",\n",
        "    \"is_submission\": False,\n",
        "    \"datetime_now\": datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"),\n",
        "    \"n_fold\":5,\n",
        "    \"seed\": 42,\n",
        "    \"model_name\": \"tf_efficientnet_b0_ns\",   # \"tf_efficientnet_b0_ns\", # \"tf_efficientnetv2_s_in21ft1k\"\n",
        "    \"checkpoint_path\": \"/kaggle/input/tf-efficientnet-b0-aa-827b6e33-pth/tf_efficientnet_b0_aa-827b6e33.pth\",\n",
        "    \"img_size\": 256,\n",
        "    \"train_batch_size\": 32,\n",
        "    \"valid_batch_size\": 32,\n",
        "    \"n_tiles\": 10,\n",
        "    \"n_tiles_test\": 10,\n",
        "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"num_epochs\": 6,\n",
        "    \"early_stopping\": True,\n",
        "    \"patience\": 3,\n",
        "    \"optimizer\": 'adam',\n",
        "    \"scheduler\": 'CosineAnnealingLR',\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "036e4619",
      "metadata": {
        "papermill": {
          "duration": 0.010485,
          "end_time": "2023-11-22T13:18:19.906607",
          "exception": false,
          "start_time": "2023-11-22T13:18:19.896122",
          "status": "completed"
        },
        "tags": [],
        "id": "036e4619"
      },
      "source": [
        "## 1. Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44d8e12e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-22T13:18:19.929492Z",
          "iopub.status.busy": "2023-11-22T13:18:19.929119Z",
          "iopub.status.idle": "2023-11-22T13:18:19.934933Z",
          "shell.execute_reply": "2023-11-22T13:18:19.934031Z"
        },
        "papermill": {
          "duration": 0.019733,
          "end_time": "2023-11-22T13:18:19.936958",
          "exception": false,
          "start_time": "2023-11-22T13:18:19.917225",
          "status": "completed"
        },
        "tags": [],
        "id": "44d8e12e"
      },
      "outputs": [],
      "source": [
        "ROOT_DIR = '/kaggle/input/UBC-OCEAN'\n",
        "TRAIN_DIR = '/kaggle/input/tiles-of-cancer-2048px-scale-0-25/'\n",
        "TEST_DIR = '/kaggle/input/UBC-OCEAN/test_thumbnails'\n",
        "\n",
        "# ALT_TEST_DIR = '/kaggle/input/UBC-OCEAN/test_images'\n",
        "# TMA_TRAIN_DIR = '/kaggle/input/UBC-OCEAN/train_images'\n",
        "\n",
        "def get_train_file_path(df_train_row):\n",
        "    return f\"{TRAIN_DIR}/{df_train_row.image_id}_thumbnail.png\"\n",
        "\n",
        "def get_test_file_path(image_id):\n",
        "    if os.path.exists(f\"{TEST_DIR}/{image_id}_thumbnail.png\"):\n",
        "        return f\"{TEST_DIR}/{image_id}_thumbnail.png\"\n",
        "    else:\n",
        "        return f\"{ALT_TEST_DIR}/{image_id}.png\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bc005f8",
      "metadata": {
        "papermill": {
          "duration": 0.010406,
          "end_time": "2023-11-22T13:18:19.958067",
          "exception": false,
          "start_time": "2023-11-22T13:18:19.947661",
          "status": "completed"
        },
        "tags": [],
        "id": "7bc005f8"
      },
      "source": [
        "## Create Train and Holdout Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a0217d7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-22T13:18:19.981216Z",
          "iopub.status.busy": "2023-11-22T13:18:19.980851Z",
          "iopub.status.idle": "2023-11-22T13:18:20.041486Z",
          "shell.execute_reply": "2023-11-22T13:18:20.040556Z"
        },
        "papermill": {
          "duration": 0.074896,
          "end_time": "2023-11-22T13:18:20.043561",
          "exception": false,
          "start_time": "2023-11-22T13:18:19.968665",
          "status": "completed"
        },
        "tags": [],
        "id": "4a0217d7",
        "outputId": "ed0bd7e0-59e0-4b31-8022-d22b55d1c9f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(538, 5)\n",
            "(538, 5)\n",
            "Shape Training Set: (190, 7), Shape Holdout Set: (126, 7), Shape Anomaly Set: (222, 7)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>label</th>\n",
              "      <th>image_width</th>\n",
              "      <th>image_height</th>\n",
              "      <th>is_tma</th>\n",
              "      <th>target_label</th>\n",
              "      <th>kfold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>66</td>\n",
              "      <td>LGSC</td>\n",
              "      <td>48871</td>\n",
              "      <td>48195</td>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>281</td>\n",
              "      <td>LGSC</td>\n",
              "      <td>42309</td>\n",
              "      <td>15545</td>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>286</td>\n",
              "      <td>EC</td>\n",
              "      <td>37204</td>\n",
              "      <td>30020</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1660</td>\n",
              "      <td>CC</td>\n",
              "      <td>83340</td>\n",
              "      <td>20447</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1943</td>\n",
              "      <td>CC</td>\n",
              "      <td>73730</td>\n",
              "      <td>34949</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   image_id label  image_width  image_height  is_tma  target_label  kfold\n",
              "0        66  LGSC        48871         48195   False             2    2.0\n",
              "1       281  LGSC        42309         15545   False             2    2.0\n",
              "2       286    EC        37204         30020   False             1    2.0\n",
              "3      1660    CC        83340         20447   False             0    1.0\n",
              "4      1943    CC        73730         34949   False             0    0.0"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_images = sorted(glob.glob(f\"{TRAIN_DIR}/*.png\"))\n",
        "df_train = pd.read_csv(\"/kaggle/input/UBC-OCEAN/train.csv\")\n",
        "print(df_train.shape)\n",
        "# df_train['file_path'] = df_train.apply(lambda row: get_train_file_path(row), axis=1)\n",
        "# only consider WSI / Thumbnail images\n",
        "#df_train = df_train[\n",
        "#    df_train[\"file_path\"].isin(train_images) ].reset_index(drop=True)\n",
        "print(df_train.shape)\n",
        "\n",
        "# encode to numericalt target\n",
        "encoder = LabelEncoder()\n",
        "df_train['target_label'] = encoder.fit_transform(df_train['label'])\n",
        "with open(\"label_encoder_\"+ CONFIG[\"datetime_now\"] +\".pkl\", \"wb\") as fp:\n",
        "    joblib.dump(encoder, fp)\n",
        "\n",
        "# use stratified K Fold for crossvalidation\n",
        "skf = StratifiedKFold(n_splits=CONFIG['n_fold'], shuffle=True, random_state=CONFIG[\"seed\"])\n",
        "\n",
        "for fold, ( _, val_) in enumerate(skf.split(X=df_train, y=df_train.label)):\n",
        "    df_train.loc[val_ , \"kfold\"] = int(fold)\n",
        "\n",
        "df_anomaly = df_train[df_train[\"label\"]==CONFIG[\"anomaly_class\"]].reset_index(drop=True)\n",
        "df_train = df_train[df_train[\"label\"]!=CONFIG[\"anomaly_class\"]].reset_index(drop=True)\n",
        "df_holdout = df_train[df_train[\"kfold\"].isin([3,4])].reset_index(drop=True)\n",
        "df_train = df_train[df_train[\"kfold\"].isin([0,1,2])].reset_index(drop=True)\n",
        "\n",
        "# encode to numericalt target\n",
        "encoder = LabelEncoder()\n",
        "df_train['target_label'] = encoder.fit_transform(df_train['label'])\n",
        "with open(\"label_encoder_\"+ CONFIG[\"datetime_now\"] +\".pkl\", \"wb\") as fp:\n",
        "    joblib.dump(encoder, fp)\n",
        "df_holdout['target_label'] = encoder.transform(df_holdout['label'])\n",
        "\n",
        "\n",
        "print(f\"Shape Training Set: {df_train.shape}, Shape Holdout Set: {df_holdout.shape}, Shape Anomaly Set: {df_anomaly.shape}\")\n",
        "\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3daa1b99",
      "metadata": {
        "papermill": {
          "duration": 0.010908,
          "end_time": "2023-11-22T13:18:20.065377",
          "exception": false,
          "start_time": "2023-11-22T13:18:20.054469",
          "status": "completed"
        },
        "tags": [],
        "id": "3daa1b99"
      },
      "source": [
        "## Create Pytorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db26b61e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-22T13:18:20.089297Z",
          "iopub.status.busy": "2023-11-22T13:18:20.088929Z",
          "iopub.status.idle": "2023-11-22T13:18:20.106059Z",
          "shell.execute_reply": "2023-11-22T13:18:20.104859Z"
        },
        "papermill": {
          "duration": 0.031873,
          "end_time": "2023-11-22T13:18:20.108332",
          "exception": false,
          "start_time": "2023-11-22T13:18:20.076459",
          "status": "completed"
        },
        "tags": [],
        "id": "db26b61e"
      },
      "outputs": [],
      "source": [
        "class CancerTilesDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        df_data,\n",
        "        path_img_dir: str =  '',\n",
        "        transforms = None,\n",
        "        mode: str = 'train',\n",
        "        labels_lut = None,\n",
        "        white_thr: int = 225,\n",
        "        thr_max_bg: float = 0.2,\n",
        "        split: float = 0.90,\n",
        "        n_tiles: int = 1\n",
        "    ):\n",
        "        assert os.path.isdir(path_img_dir)\n",
        "        self.path_img_dir = path_img_dir\n",
        "        self.transforms = transforms\n",
        "        self.mode = mode\n",
        "        self.white_thr = white_thr\n",
        "        self.thr_max_bg = thr_max_bg\n",
        "        self.split = split\n",
        "        self.n_tiles = n_tiles\n",
        "\n",
        "        self.data = df_data\n",
        "        self.labels_unique = sorted(self.data[\"label\"].unique())\n",
        "        self.labels_lut = labels_lut or {lb: i for i, lb in enumerate(self.labels_unique)}\n",
        "        # shuffle data\n",
        "        self.data = self.data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "        # split dataset\n",
        "        assert 0.0 <= self.split <= 1.0\n",
        "        frac = int(self.split * len(self.data))\n",
        "        self.data = self.data[:frac] if mode in [\"train\", \"test\"] else self.data[frac:]\n",
        "        self.img_dirs = [glob.glob(os.path.join(path_img_dir, str(idx), \"*.png\")) for idx in self.data[\"image_id\"]]\n",
        "        self.img_dirs = self.img_dirs * self.n_tiles\n",
        "        self.img_paths = []\n",
        "        #print(f\"missing: {sum([not os.path.isfile(os.path.join(self.path_img_dir, im))\n",
        "        #                       for im in self.img_names])}\")\n",
        "        # self.labels = list(self.data['label'])\n",
        "        self.labels =  np.array(self.data.target_label.values.tolist() * self.n_tiles)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> tuple:\n",
        "        nth_iteration = idx//len(self.data)\n",
        "        if self.mode==\"train\":\n",
        "            random.seed()\n",
        "        else:\n",
        "            random.seed(CONFIG[\"seed\"]+nth_iteration)\n",
        "        random.shuffle(self.img_dirs[idx])\n",
        "        for img_path in self.img_dirs[idx]:\n",
        "            assert os.path.isfile(img_path), f\"missing: {img_path}\"\n",
        "            tile = cv2.imread(img_path)\n",
        "            tile = cv2.cvtColor(tile, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # tile = np.array(Image.open(img_path))[..., :3]\n",
        "            black_bg = np.sum(tile, axis=2) == 0\n",
        "            tile[black_bg, :] = 255\n",
        "            mask_bg = np.mean(tile, axis=2) > self.white_thr\n",
        "            if np.sum(mask_bg) < (np.prod(mask_bg.shape) * self.thr_max_bg):\n",
        "                #self.img_paths.append(img_path)\n",
        "                #print(f\"Idx: {idx}, Path: {img_path}, len img_pths: {len(self.img_paths)}, nunique img_paths: {len(set(self.img_paths))}\")\n",
        "                break\n",
        "\n",
        "        # augmentation\n",
        "        if self.transforms:\n",
        "            tile = self.transforms(image=tile)[\"image\"]\n",
        "        #print(f\"img dim: {img.shape}\")\n",
        "        return {\n",
        "            \"image\": tile,\n",
        "            \"label\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "               }\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.img_dirs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a577503",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-22T13:18:20.132013Z",
          "iopub.status.busy": "2023-11-22T13:18:20.131617Z",
          "iopub.status.idle": "2023-11-22T13:18:20.141065Z",
          "shell.execute_reply": "2023-11-22T13:18:20.140038Z"
        },
        "papermill": {
          "duration": 0.023784,
          "end_time": "2023-11-22T13:18:20.143089",
          "exception": false,
          "start_time": "2023-11-22T13:18:20.119305",
          "status": "completed"
        },
        "tags": [],
        "id": "2a577503"
      },
      "outputs": [],
      "source": [
        "img_color_mean=[0.8661704276539922, 0.7663107094675368, 0.8574260897185548]\n",
        "img_color_std=[0.08670629753900036, 0.11646580094195522, 0.07164169171856792]\n",
        "\n",
        "data_transforms = {\n",
        "    \"train\": A.Compose([\n",
        "        A.Resize(512, 512),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.5),\n",
        "        A.RandomBrightnessContrast(p=0.75),\n",
        "        A.ShiftScaleRotate(p=0.75),\n",
        "        A.OneOf([\n",
        "        A.GaussNoise(var_limit=[10, 50]),\n",
        "        A.GaussianBlur(),\n",
        "        A.MotionBlur(),\n",
        "        ], p=0.4),\n",
        "        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
        "        A.CoarseDropout(max_holes=1, max_width=int(512* 0.3), max_height=int(512* 0.3),\n",
        "        mask_fill_value=0, p=0.5),\n",
        "        A.Normalize(img_color_mean, img_color_std),\n",
        "        ToTensorV2()], p=1.),\n",
        "\n",
        "    \"valid\": A.Compose([\n",
        "        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
        "        A.Normalize(img_color_mean, img_color_std),\n",
        "        ToTensorV2()], p=1.)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f4aba44",
      "metadata": {
        "papermill": {
          "duration": 0.010748,
          "end_time": "2023-11-22T13:18:20.164727",
          "exception": false,
          "start_time": "2023-11-22T13:18:20.153979",
          "status": "completed"
        },
        "tags": [],
        "id": "5f4aba44"
      },
      "source": [
        "## 2. Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bb84eab",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-22T13:18:20.188307Z",
          "iopub.status.busy": "2023-11-22T13:18:20.187938Z",
          "iopub.status.idle": "2023-11-22T13:18:20.200671Z",
          "shell.execute_reply": "2023-11-22T13:18:20.199820Z"
        },
        "papermill": {
          "duration": 0.027158,
          "end_time": "2023-11-22T13:18:20.202825",
          "exception": false,
          "start_time": "2023-11-22T13:18:20.175667",
          "status": "completed"
        },
        "tags": [],
        "id": "8bb84eab"
      },
      "outputs": [],
      "source": [
        "class GeM(nn.Module):\n",
        "    def __init__(self, p=3, eps=1e-6):\n",
        "        super(GeM, self).__init__()\n",
        "        self.p = nn.Parameter(torch.ones(1)*p)\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.gem(x, p=self.p, eps=self.eps)\n",
        "\n",
        "    def gem(self, x, p=3, eps=1e-6):\n",
        "        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + \\\n",
        "                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n",
        "                ', ' + 'eps=' + str(self.eps) + ')'\n",
        "\n",
        "\n",
        "class UBCOutlierModel(nn.Module):\n",
        "\n",
        "    def __init__(self, model_name, num_classes, pretrained=False, checkpoint_path=None):\n",
        "        '''\n",
        "        Fine tune for EfficientNetB0\n",
        "        Args\n",
        "            n_classes : int - Number of classification categories.\n",
        "            learnable_modules : tuple - Names of the modules to fine-tune.\n",
        "        Return\n",
        "\n",
        "        '''\n",
        "        super(UBCOutlierModel, self).__init__()\n",
        "        self.model = timm.create_model(model_name, pretrained=pretrained, checkpoint_path=checkpoint_path)\n",
        "\n",
        "        in_features = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Identity()\n",
        "        self.model.global_pool = nn.Identity()\n",
        "        self.pooling = GeM()\n",
        "        self.linear = nn.Linear(in_features, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"\n",
        "        Forward function for the fine-tuned model\n",
        "        Args\n",
        "            x:\n",
        "        Return\n",
        "            result\n",
        "        \"\"\"\n",
        "        features = self.model(images)\n",
        "        pooled_features = self.pooling(features).flatten(1)\n",
        "        output = self.linear(pooled_features)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "730ef825",
      "metadata": {
        "papermill": {
          "duration": 0.010745,
          "end_time": "2023-11-22T13:18:20.224891",
          "exception": false,
          "start_time": "2023-11-22T13:18:20.214146",
          "status": "completed"
        },
        "tags": [],
        "id": "730ef825"
      },
      "source": [
        "## 3. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "670e591c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-22T13:18:20.248694Z",
          "iopub.status.busy": "2023-11-22T13:18:20.248325Z",
          "iopub.status.idle": "2023-11-22T13:18:20.257990Z",
          "shell.execute_reply": "2023-11-22T13:18:20.257011Z"
        },
        "papermill": {
          "duration": 0.023854,
          "end_time": "2023-11-22T13:18:20.259962",
          "exception": false,
          "start_time": "2023-11-22T13:18:20.236108",
          "status": "completed"
        },
        "tags": [],
        "id": "670e591c"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, verbose=False, delta=0, path='checkpoint.pth', trace_func=print):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = float('inf')\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decreases.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model to path {self.path}')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bf1957e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-22T13:18:20.284120Z",
          "iopub.status.busy": "2023-11-22T13:18:20.283198Z",
          "iopub.status.idle": "2023-11-22T13:18:20.296362Z",
          "shell.execute_reply": "2023-11-22T13:18:20.295373Z"
        },
        "papermill": {
          "duration": 0.027754,
          "end_time": "2023-11-22T13:18:20.298706",
          "exception": false,
          "start_time": "2023-11-22T13:18:20.270952",
          "status": "completed"
        },
        "tags": [],
        "id": "9bf1957e"
      },
      "outputs": [],
      "source": [
        "def convert_dict_to_tensor(dict_):\n",
        "    \"\"\"Converts the values of a dict into a PyTorch tensor.\"\"\"\n",
        "\n",
        "    # Create a new PyTorch tensor\n",
        "    tensor = torch.empty(len(dict_))\n",
        "\n",
        "    # Iterate over the dict and for each key-value pair, convert the value to a PyTorch tensor and add it to the new tensor\n",
        "    for i, (key, value) in enumerate(dict_.items()):\n",
        "        tensor[i] = value\n",
        "\n",
        "    # Return the new tensor\n",
        "    return tensor\n",
        "\n",
        "def get_class_weights(df_train):\n",
        "    label_counts = df_train.target_label.value_counts().sort_index().to_dict()\n",
        "    ratios_dict = {}\n",
        "    for key,val in label_counts.items():\n",
        "        ratios_dict[key] = val / df_train.shape[0]\n",
        "    ratios_dict\n",
        "    weights = {}\n",
        "    sum_weights = 0\n",
        "    for key, val in ratios_dict.items():\n",
        "        weights[key] = 1 / val\n",
        "        sum_weights +=  1 / val\n",
        "    for key, val in weights.items():\n",
        "        weights[key] = val / sum_weights\n",
        "    weight_tensor = convert_dict_to_tensor(weights)\n",
        "    return weight_tensor\n",
        "\n",
        "def get_dataloaders(df, n_tiles=1):\n",
        "    # df_train = df[df[\"kfold\"]!=fold].reset_index(drop=True)\n",
        "    train_dataset = CancerTilesDataset(df_train, TRAIN_DIR, transforms=data_transforms[\"train\"], mode=\"train\", n_tiles=n_tiles)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'],\n",
        "                              num_workers=2, shuffle=False, pin_memory=True)\n",
        "    valid_dataset = CancerTilesDataset(df_train, TRAIN_DIR, transforms=data_transforms[\"valid\"], mode=\"valid\", n_tiles=n_tiles)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'],\n",
        "                              num_workers=2, shuffle=False, pin_memory=True)\n",
        "    print(f\"Len Train Dataset: {len(train_dataset)}, Len Validation Dataset: {len(valid_dataset)}\" )\n",
        "    return train_loader, valid_loader, df_train\n",
        "\n",
        "def print_logged_info(r):\n",
        "    tags = {k: v for k, v in r.data.tags.items() if not k.startswith(\"mlflow.\")}\n",
        "    artifacts = [f.path for f in MlflowClient().list_artifacts(r.info.run_id, \"model\")]\n",
        "    print(f\"run_id: {r.info.run_id}\")\n",
        "    print(f\"artifacts: {artifacts}\")\n",
        "    print(f\"params: {r.data.params}\")\n",
        "    print(f\"metrics: {r.data.metrics}\")\n",
        "    print(f\"tags: {tags}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d058f59",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-22T13:18:20.322806Z",
          "iopub.status.busy": "2023-11-22T13:18:20.321910Z",
          "iopub.status.idle": "2023-11-22T13:18:20.336677Z",
          "shell.execute_reply": "2023-11-22T13:18:20.335682Z"
        },
        "papermill": {
          "duration": 0.029112,
          "end_time": "2023-11-22T13:18:20.338716",
          "exception": false,
          "start_time": "2023-11-22T13:18:20.309604",
          "status": "completed"
        },
        "tags": [],
        "id": "9d058f59"
      },
      "outputs": [],
      "source": [
        "def fetch_scheduler(optimizer):\n",
        "    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n",
        "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'],\n",
        "                                                   eta_min=CONFIG['min_lr'], verbose=False)\n",
        "    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n",
        "        CONFIG['T_0'] = 20\n",
        "        CONFIG['T_mult'] = 2\n",
        "        CONFIG['min_lr'] = 1e-6\n",
        "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CONFIG['T_0'], T_mult=CONFIG['T_mult'],\n",
        "                                                             eta_min=CONFIG['min_lr'], verbose=False)\n",
        "    elif CONFIG['scheduler'] == 'ReduceLROnPlateau':\n",
        "        scheduler =  ReduceLROnPlateau(optimizer, mode='min', factor=kwargs.get('factor', 0.1), patience=kwargs.get('patience', 5), verbose=False)\n",
        "    elif CONFIG['scheduler'] == 'LambdaLR':\n",
        "        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "    elif CONFIG['scheduler'] == None:\n",
        "        return None\n",
        "\n",
        "    return scheduler\n",
        "\n",
        "def get_optimizer(optimizer_name, model):\n",
        "    if optimizer_name.lower() == \"adam\":\n",
        "        CONFIG['learning_rate'] = 1e-4\n",
        "        CONFIG['weight_decay'] = 1e-5\n",
        "        CONFIG['betas'] = (0.9, 0.999)\n",
        "        CONFIG['eps'] = 1e-8\n",
        "        optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'], betas=CONFIG['betas'], eps=CONFIG['eps'],  weight_decay=CONFIG['weight_decay'])\n",
        "    elif optimizer_name.lower() == \"sgd\":\n",
        "        CONFIG['learning_rate'] = 1e-3\n",
        "        CONFIG['weight_decay'] = 1e-3\n",
        "        CONFIG['momentum'] = 1e-3\n",
        "        optimizer = optim.SGD(model.parameters(), lr=CONFIG['learning_rate'], momentum=CONFIG['momentum'], weight_decay=CONFIG['weight_decay'])\n",
        "    elif optimizer_name.lower() == \"radam\":\n",
        "        CONFIG['learning_rate'] = 1e-4\n",
        "        CONFIG['weight_decay'] = 0\n",
        "        CONFIG['betas'] = (0.9, 0.999)\n",
        "        CONFIG['eps'] = 1e-8\n",
        "        optimizer = torch_optimizer.RAdam(\n",
        "            model.parameters(),\n",
        "            lr= CONFIG['learning_rate'],\n",
        "            betas=CONFIG['betas'],\n",
        "            eps=CONFIG['eps'],\n",
        "            weight_decay=CONFIG['weight_decay'],\n",
        "        )\n",
        "    elif optimizer_name.lower() == \"rmsprop\":\n",
        "        CONFIG['learning_rate'] = 0.256\n",
        "        CONFIG['alpha'] = 0.9\n",
        "        CONFIG['momentum'] = 0.9\n",
        "        CONFIG['weight_decay'] = 1e-5\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=CONFIG['learning_rate'], alpha=CONFIG['learning_rate'],\n",
        "                                  momentum=CONFIG['learning_rate'], weight_decay=CONFIG['learning_rate'])\n",
        "    else:\n",
        "        raise ValueError(\"Invalid Optimizer given!\")\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b33127dd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-22T13:18:20.362644Z",
          "iopub.status.busy": "2023-11-22T13:18:20.362262Z",
          "iopub.status.idle": "2023-11-22T13:18:20.386768Z",
          "shell.execute_reply": "2023-11-22T13:18:20.385742Z"
        },
        "papermill": {
          "duration": 0.039339,
          "end_time": "2023-11-22T13:18:20.388937",
          "exception": false,
          "start_time": "2023-11-22T13:18:20.349598",
          "status": "completed"
        },
        "tags": [],
        "id": "b33127dd"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, train_loader, optimizer, criterion, device, writer, epoch, scheduler=None):\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "    for step, data in bar:\n",
        "        images = data['image'].to(device, dtype=torch.float)\n",
        "        labels = data['label'].to(device, dtype=torch.long)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # Focal Loss\n",
        "        #criterion = FocalLoss(gamma=0.7)\n",
        "        #m = torch.nn.Softmax(dim=-1)\n",
        "        #loss = criterion(m(outputs), labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        # Update learning rate using the scheduler\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        # Log the training loss to TensorBoard\n",
        "        writer.add_scalar('loss/train_batch', loss.item(), epoch * len(train_loader) + step)\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    # Log the average training loss for the epoch to TensorBoard\n",
        "    writer.add_scalar('loss/train_epoch', train_loss, epoch)\n",
        "    # gc.collect()\n",
        "    return train_loss\n",
        "\n",
        "def validate_one_epoch(model, valid_loader, criterion, device, writer, epoch):\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    valid_acc = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        bar_val = tqdm(enumerate(valid_loader), total=len(valid_loader))\n",
        "        for step, data in bar_val:\n",
        "            images = data['image'].to(device, dtype=torch.float)\n",
        "            labels = data['label'].to(device, dtype=torch.long)\n",
        "            outputs = model(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            # Focal Loss\n",
        "            #criterion = FocalLoss(gamma=0.7)\n",
        "            #m = torch.nn.Softmax(dim=-1)\n",
        "            #loss = criterion(m(outputs), labels)\n",
        "\n",
        "            valid_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(model.softmax(outputs), 1)\n",
        "            acc = torch.sum( predicted == labels )\n",
        "            valid_acc  += acc.item()\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            writer.add_scalar('loss/valid_batch', loss.item(), epoch * len(valid_loader) + step)\n",
        "            writer.add_scalar('acc/valid_batch', acc.item(), epoch * len(valid_loader) + step)\n",
        "    valid_loss /= len(valid_loader.dataset)\n",
        "    valid_acc /= len(valid_loader.dataset)\n",
        "    bal_acc = balanced_accuracy_score(all_labels, all_preds)\n",
        "    # At the end of your validation loop:\n",
        "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    micro_f1 = f1_score(all_labels, all_preds, average='micro')\n",
        "    weighted_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    # Logging to TensorBoard\n",
        "    writer.add_scalar('loss/val_epoch', valid_loss, epoch)\n",
        "    writer.add_scalar('acc/val_epoch', valid_acc, epoch)\n",
        "    writer.add_scalar('balanced_acc/val_epoch', bal_acc, epoch)\n",
        "    writer.add_scalar('F1/macro', macro_f1, epoch)\n",
        "    writer.add_scalar('F1/micro', micro_f1, epoch)\n",
        "    writer.add_scalar('F1/weighted', weighted_f1, epoch)\n",
        "    # in order to put multiple lines within one graph\n",
        "    #writer.add_scalars('run_14h', {'xsinx':i*np.sin(i/r),\n",
        "    #                        'xcosx':i*np.cos(i/r),\n",
        "    #                        'tanx': np.tan(i/r)}, i)\n",
        "    return valid_loss, valid_acc, bal_acc, weighted_f1\n",
        "\n",
        "def train_model(model, train_loader, valid_loader, optimizer, criterion, device, num_epochs, scheduler, save_model_path=None):\n",
        "    model_name = \"model_epochs\" + str(CONFIG[\"num_epochs\"]) + \"_bs\"+str(CONFIG[\"train_batch_size\"] )+ \"_opt\" +CONFIG[\"optimizer\"]+ \"_sched\" + CONFIG[\"scheduler\"] + \"_lr\"+str(CONFIG[\"learning_rate\"])+ \"_wd\" + str(CONFIG[\"weight_decay\"])\n",
        "    print(f\"Training model: {model_name}\")\n",
        "    datetime_now =  datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    if not save_model_path:\n",
        "        save_model_path = 'best_model_checkpoint' + datetime_now + '.pth'\n",
        "    print(f\"Path for saving model: {save_model_path}\")\n",
        "    # Initialize TensorBoard writer\n",
        "    writer = SummaryWriter('logs/fit/' + model_name)\n",
        "    early_stopping = EarlyStopping(patience=CONFIG[\"patience\"], verbose=True, path=save_model_path)\n",
        "    #if scheduler_type:\n",
        "    #    print(f\"Define {scheduler_type} scheduler\")\n",
        "    #    scheduler = get_lr_scheduler(optimizer, scheduler_type, num_epochs=num_epochs)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, writer, epoch, scheduler)\n",
        "        valid_loss, valid_acc, bal_acc, weighted_f1 = validate_one_epoch(model, valid_loader, criterion, device, writer, epoch)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {train_loss:.4f}, Validation loss: {valid_loss:.4f}, Validation acc: {valid_acc:.4f}, Balanced acc: {bal_acc:.4f}, Weighted F1-Score: {weighted_f1:.4f}\")\n",
        "        # Call early stopping\n",
        "        if CONFIG[\"early_stopping\"]:\n",
        "            early_stopping(valid_loss, model)\n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "        writer.close()\n",
        "\n",
        "        try:\n",
        "            mlflow.log_metrics({\n",
        "                'epoch': epoch,\n",
        "                'train_loss': train_loss,\n",
        "                'valid_loss': valid_loss,\n",
        "                'valid_acc': valid_acc,\n",
        "                'balanced_acc': bal_acc,\n",
        "                'weighted_f1': weighted_f1\n",
        "            }, step=epoch)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return train_loss, valid_loss, valid_acc, save_model_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b8a363e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-22T13:18:20.413475Z",
          "iopub.status.busy": "2023-11-22T13:18:20.412605Z",
          "iopub.status.idle": "2023-11-22T13:18:20.418547Z",
          "shell.execute_reply": "2023-11-22T13:18:20.417600Z"
        },
        "papermill": {
          "duration": 0.020754,
          "end_time": "2023-11-22T13:18:20.420640",
          "exception": false,
          "start_time": "2023-11-22T13:18:20.399886",
          "status": "completed"
        },
        "tags": [],
        "id": "4b8a363e"
      },
      "outputs": [],
      "source": [
        "def calc_mae_per_image(images, rec):\n",
        "    abs_diff = torch.abs(torch.from_numpy(images) - torch.from_numpy(rec))\n",
        "    # Sum differences per image\n",
        "    sum_diff_per_image = torch.sum(abs_diff.view(images.shape[0], -1), dim=1)\n",
        "\n",
        "    # Calculate mean absolute error per image\n",
        "    num_pixels_per_image = 3 * CONFIG[\"img_size\"] * CONFIG[\"img_size\"]\n",
        "    mae_per_image = sum_diff_per_image / num_pixels_per_image\n",
        "    return mae_per_image\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ead7e03e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-22T13:18:20.444625Z",
          "iopub.status.busy": "2023-11-22T13:18:20.444246Z",
          "iopub.status.idle": "2023-11-22T13:18:20.458282Z",
          "shell.execute_reply": "2023-11-22T13:18:20.457258Z"
        },
        "papermill": {
          "duration": 0.028672,
          "end_time": "2023-11-22T13:18:20.460454",
          "exception": false,
          "start_time": "2023-11-22T13:18:20.431782",
          "status": "completed"
        },
        "tags": [],
        "id": "ead7e03e"
      },
      "outputs": [],
      "source": [
        "def test_on_holdout(model, CONFIG, df_test, TRAIN_DIR=None, val_size=1.0, n_tiles=1):\n",
        "    if not CONFIG[\"is_submission\"]:\n",
        "        model.eval()\n",
        "        test_dataset = CancerTilesDataset(df_test, TRAIN_DIR, transforms=data_transforms[\"valid\"], mode=\"test\", split=1.0, n_tiles=n_tiles)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=CONFIG['valid_batch_size'],\n",
        "                                  num_workers=2, shuffle=False, pin_memory=True)\n",
        "        print(f\"Test-Dataset Size: {len(test_dataset)}\")\n",
        "\n",
        "        preds = []\n",
        "        labels_list = []\n",
        "        test_acc = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            bar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
        "            for step, data in bar:\n",
        "                # print(step)\n",
        "                images = data['image'].to(CONFIG[\"device\"], dtype=torch.float)\n",
        "                labels = data['label'].to(CONFIG[\"device\"], dtype=torch.long)\n",
        "\n",
        "                batch_size = images.size(0)\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(model.softmax(outputs), 1)\n",
        "                preds.append(predicted.detach().cpu().numpy() )\n",
        "                labels_list.append(labels.detach().cpu().numpy() )\n",
        "                acc = torch.sum(predicted == labels )\n",
        "                test_acc  += acc.item()\n",
        "        test_acc /= len(test_loader.dataset)\n",
        "        preds = np.concatenate(preds).flatten()\n",
        "        labels_list = np.concatenate(labels_list).flatten()\n",
        "        pred_labels = encoder.inverse_transform( preds )\n",
        "\n",
        "        # Calculate Balanced Accuracy\n",
        "        bal_acc = balanced_accuracy_score(labels_list, preds)\n",
        "        # Calculate Confusion Matrix\n",
        "        conf_matrix = confusion_matrix(labels_list, preds)\n",
        "        macro_f1 = f1_score(labels_list, preds, average='macro')\n",
        "\n",
        "\n",
        "        print(f\"Test Accuracy: {test_acc}\")\n",
        "        print(f\"Balanced Accuracy: {bal_acc}\")\n",
        "        print(f\"Confusion Matrix: {conf_matrix}\")\n",
        "\n",
        "        # add to validation dataframe\n",
        "        num_samples = len(df_test)\n",
        "        for i in range(0,n_tiles):\n",
        "            df_test[f\"label_tile_{str(i)}\"] = labels_list[i*num_samples:(i+1)*num_samples]\n",
        "            df_test[f\"pred_tile_{str(i)}\"] = preds[i*num_samples:(i+1)*num_samples]\n",
        "            df_test[f\"pred_label_tile_{str(i)}\"] = pred_labels[i*num_samples:(i+1)*num_samples]\n",
        "            #df_test[\"pred\"] = preds\n",
        "            #df_test[\"pred_labels\"] = pred_labels\n",
        "        try:\n",
        "            mlflow.log_metrics({\n",
        "                'test_acc': test_acc,\n",
        "                'test_balanced_acc': bal_acc,\n",
        "                'test_f1_score': macro_f1,\n",
        "            })\n",
        "        except:\n",
        "            pass\n",
        "        return df_test\n",
        "    else:\n",
        "        print(\"Skip validation on training set due to submission!\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a42547e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-22T13:18:20.544754Z",
          "iopub.status.busy": "2023-11-22T13:18:20.543855Z",
          "iopub.status.idle": "2023-11-22T13:18:25.834918Z",
          "shell.execute_reply": "2023-11-22T13:18:25.833900Z"
        },
        "papermill": {
          "duration": 5.365752,
          "end_time": "2023-11-22T13:18:25.837245",
          "exception": false,
          "start_time": "2023-11-22T13:18:20.471493",
          "status": "completed"
        },
        "tags": [],
        "id": "7a42547e",
        "outputId": "c80b49d9-2ded-4d1f-e767-7a84db725990"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class weights: tensor([0.1666, 0.1311, 0.3511, 0.3511], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "CONFIG[\"weighted_loss\"] = True\n",
        "if CONFIG[\"weighted_loss\"]:\n",
        "    class_weights = get_class_weights(df_train).to(CONFIG['device'], dtype=torch.float)\n",
        "    print(f\"Class weights: {class_weights}\")\n",
        "else:\n",
        "    class_weights=None\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bb4ef44",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-22T13:18:25.861118Z",
          "iopub.status.busy": "2023-11-22T13:18:25.860732Z",
          "iopub.status.idle": "2023-11-22T13:35:29.009220Z",
          "shell.execute_reply": "2023-11-22T13:35:29.008086Z"
        },
        "papermill": {
          "duration": 1023.163369,
          "end_time": "2023-11-22T13:35:29.011811",
          "exception": false,
          "start_time": "2023-11-22T13:18:25.848442",
          "status": "completed"
        },
        "tags": [],
        "id": "2bb4ef44",
        "outputId": "b20da570-2880-4c38-f35e-c1a80878ef0b",
        "colab": {
          "referenced_widgets": [
            "c3ca1cfdab384134a657c66826b4a8e2",
            "b5f5f7f10ba64b1291ececd05490be9b",
            "6e8450836d924720a879ecf964334d76",
            "f546f3066c5341b48a958ba25152b2c2",
            "3b8cd15ebc1048689da3f3e926d7bf22",
            "1747def8543f4d828063e248541821e4",
            "4819bd7e85be4dd68e199992825152af",
            "051bfade26e942ac9afc82ba032043d0",
            "c7bece3b6fd74f02bbdf9c0e66e76f68",
            "b24734e17cd64f01a4353d70a452f536",
            "e7636443b3244c62be6a0a43c1930367",
            "104d6c8e12a64f5cac78ef051522a64d",
            "494380b5847a4bef92f7ddac27aaf915"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape df_train: (190, 7), Shape df_test: (126, 7)\n",
            "Len Train Dataset: 1710, Len Validation Dataset: 190\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/site-packages/timm/models/_factory.py:114: UserWarning: Mapping deprecated model name tf_efficientnet_b0_ns to current tf_efficientnet_b0.ns_jft_in1k.\n",
            "  model = create_fn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model: model_epochs6_bs32_optadam_schedCosineAnnealingLR_lr0.0001_wd1e-05\n",
            "Path for saving model: best_model_checkpoint2023-11-22_13-18-28.pth\n",
            "[INFO] Using GPU: Tesla P100-PCIE-16GB\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3ca1cfdab384134a657c66826b4a8e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/54 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b5f5f7f10ba64b1291ececd05490be9b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/6 - Train loss: 1.2944, Validation loss: 1.2737, Validation acc: 0.4211, Balanced acc: 0.4156, Weighted F1-Score: 0.4317\n",
            "Validation loss decreased (inf --> 1.273675). Saving model to path best_model_checkpoint2023-11-22_13-18-28.pth\n",
            "[INFO] Using GPU: Tesla P100-PCIE-16GB\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e8450836d924720a879ecf964334d76",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/54 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f546f3066c5341b48a958ba25152b2c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/6 - Train loss: 1.0351, Validation loss: 1.1419, Validation acc: 0.4474, Balanced acc: 0.4594, Weighted F1-Score: 0.4613\n",
            "Validation loss decreased (1.273675 --> 1.141859). Saving model to path best_model_checkpoint2023-11-22_13-18-28.pth\n",
            "[INFO] Using GPU: Tesla P100-PCIE-16GB\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b8cd15ebc1048689da3f3e926d7bf22",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/54 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1747def8543f4d828063e248541821e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/6 - Train loss: 0.8565, Validation loss: 1.0570, Validation acc: 0.4895, Balanced acc: 0.5323, Weighted F1-Score: 0.4836\n",
            "Validation loss decreased (1.141859 --> 1.057028). Saving model to path best_model_checkpoint2023-11-22_13-18-28.pth\n",
            "[INFO] Using GPU: Tesla P100-PCIE-16GB\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4819bd7e85be4dd68e199992825152af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/54 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "051bfade26e942ac9afc82ba032043d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/6 - Train loss: 0.7467, Validation loss: 1.0301, Validation acc: 0.4842, Balanced acc: 0.5229, Weighted F1-Score: 0.4889\n",
            "Validation loss decreased (1.057028 --> 1.030102). Saving model to path best_model_checkpoint2023-11-22_13-18-28.pth\n",
            "[INFO] Using GPU: Tesla P100-PCIE-16GB\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c7bece3b6fd74f02bbdf9c0e66e76f68",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/54 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b24734e17cd64f01a4353d70a452f536",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/6 - Train loss: 0.6302, Validation loss: 1.0112, Validation acc: 0.4789, Balanced acc: 0.5229, Weighted F1-Score: 0.4694\n",
            "Validation loss decreased (1.030102 --> 1.011182). Saving model to path best_model_checkpoint2023-11-22_13-18-28.pth\n",
            "[INFO] Using GPU: Tesla P100-PCIE-16GB\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7636443b3244c62be6a0a43c1930367",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/54 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "104d6c8e12a64f5cac78ef051522a64d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/6 - Train loss: 0.5687, Validation loss: 1.0010, Validation acc: 0.4737, Balanced acc: 0.5281, Weighted F1-Score: 0.4615\n",
            "Validation loss decreased (1.011182 --> 1.001003). Saving model to path best_model_checkpoint2023-11-22_13-18-28.pth\n",
            "Validate on Holdout Set:\n",
            "Test-Dataset Size: 1260\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "494380b5847a4bef92f7ddac27aaf915",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.5642857142857143\n",
            "Balanced Accuracy: 0.6242239527389903\n",
            "Confusion Matrix: [[244  54  72  30]\n",
            " [ 27 187 242  34]\n",
            " [  9   4 172   5]\n",
            " [  6  38  28 108]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023/11/22 13:35:14 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpa_n2g43c/model/data, flavor: pytorch), fall back to return ['torch==2.0.0', 'cloudpickle==2.2.1']. Set logging level to DEBUG to see the full traceback.\n",
            "/opt/conda/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
            "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
          ]
        }
      ],
      "source": [
        "df_test = df_holdout.copy()\n",
        "CONFIG['num_classes'] = df_train[\"label\"].nunique()\n",
        "CONFIG['T_max'] = CONFIG['num_epochs']\n",
        "CONFIG['min_lr'] = 1e-6\n",
        "print(f\"Shape df_train: {df_train.shape}, Shape df_test: {df_test.shape}\")\n",
        "with mlflow.start_run(experiment_id=mlflow_experiment_id) as run:\n",
        "    train_loader, valid_loader, df_train_fold = get_dataloaders(df_train.copy(), n_tiles=CONFIG[\"n_tiles\"])\n",
        "\n",
        "    model = UBCOutlierModel(CONFIG['model_name'], CONFIG['num_classes'], pretrained=False , checkpoint_path=CONFIG[\"checkpoint_path\"])\n",
        "    # model.load_state_dict(torch.load(CONFIG[\"checkpoint_path\"]))\n",
        "    model.to(CONFIG['device']);\n",
        "\n",
        "    optimizer = get_optimizer(CONFIG[\"optimizer\"], model)\n",
        "    scheduler = fetch_scheduler(optimizer)\n",
        "\n",
        "    _, _, _, save_model_path = train_model(model, train_loader, valid_loader, optimizer, criterion, CONFIG[\"device\"], CONFIG[\"num_epochs\"], scheduler)\n",
        "    model.load_state_dict(torch.load(save_model_path))\n",
        "\n",
        "\n",
        "    print(\"Validate on Holdout Set:\")\n",
        "    df_test = test_on_holdout(model, CONFIG, df_test, TRAIN_DIR, val_size=1, n_tiles=CONFIG[\"n_tiles_test\"])\n",
        "    df_test_file_path = \"df_test_results.csv\"\n",
        "    df_test.to_csv(df_test_file_path, index=False)\n",
        "    try:\n",
        "        mlflow.log_params(CONFIG)\n",
        "        mlflow.pytorch.log_model(model, \"model\")\n",
        "        mlflow.log_params(save_model_path)\n",
        "        mlflow.log_artifact(df_test_file_path)\n",
        "        print_logged_info(mlflow.get_run(run_id=run.info.run_id))\n",
        "    except:\n",
        "        pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "244e82ef",
      "metadata": {
        "papermill": {
          "duration": 0.014399,
          "end_time": "2023-11-22T13:35:29.041258",
          "exception": false,
          "start_time": "2023-11-22T13:35:29.026859",
          "status": "completed"
        },
        "tags": [],
        "id": "244e82ef"
      },
      "source": [
        "# 2 .Outlier Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea9fa4b4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-22T13:35:29.073275Z",
          "iopub.status.busy": "2023-11-22T13:35:29.072887Z",
          "iopub.status.idle": "2023-11-22T13:35:29.081709Z",
          "shell.execute_reply": "2023-11-22T13:35:29.080717Z"
        },
        "papermill": {
          "duration": 0.027548,
          "end_time": "2023-11-22T13:35:29.083709",
          "exception": false,
          "start_time": "2023-11-22T13:35:29.056161",
          "status": "completed"
        },
        "tags": [],
        "id": "ea9fa4b4"
      },
      "outputs": [],
      "source": [
        "# Manipulate Encoder to returned pooled features\n",
        "class UBCOutlierModel(nn.Module):\n",
        "\n",
        "    def __init__(self, model_name, num_classes, pretrained=False, checkpoint_path=None):\n",
        "        '''\n",
        "        Fine tune for EfficientNetB0\n",
        "        Args\n",
        "            n_classes : int - Number of classification categories.\n",
        "            learnable_modules : tuple - Names of the modules to fine-tune.\n",
        "        Return\n",
        "\n",
        "        '''\n",
        "        super(UBCOutlierModel, self).__init__()\n",
        "        self.model = timm.create_model(model_name, pretrained=pretrained, checkpoint_path=checkpoint_path)\n",
        "\n",
        "        in_features = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Identity()\n",
        "        self.model.global_pool = nn.Identity()\n",
        "        self.pooling = GeM()\n",
        "        self.linear = nn.Linear(in_features, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"\n",
        "        Forward function for the fine-tuned model\n",
        "        Args\n",
        "            x:\n",
        "        Return\n",
        "            result\n",
        "        \"\"\"\n",
        "        features = self.model(images)\n",
        "        pooled_features = self.pooling(features).flatten(1)\n",
        "        output = self.linear(pooled_features)\n",
        "        return pooled_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f930266c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-22T13:35:29.115001Z",
          "iopub.status.busy": "2023-11-22T13:35:29.114570Z",
          "iopub.status.idle": "2023-11-22T13:35:29.120644Z",
          "shell.execute_reply": "2023-11-22T13:35:29.119715Z"
        },
        "papermill": {
          "duration": 0.024069,
          "end_time": "2023-11-22T13:35:29.122649",
          "exception": false,
          "start_time": "2023-11-22T13:35:29.098580",
          "status": "completed"
        },
        "tags": [],
        "id": "f930266c"
      },
      "outputs": [],
      "source": [
        "def extract_features(data_loader, model, device):\n",
        "    model.eval()\n",
        "    features = []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            images = batch['image'].to(device)  # Assuming the data loader returns a dictionary\n",
        "            batch_features = model(images)\n",
        "            features.append(batch_features.cpu().numpy())\n",
        "\n",
        "    return np.vstack(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "320e3320",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-22T13:35:29.154406Z",
          "iopub.status.busy": "2023-11-22T13:35:29.154034Z",
          "iopub.status.idle": "2023-11-22T13:35:29.160582Z",
          "shell.execute_reply": "2023-11-22T13:35:29.159628Z"
        },
        "papermill": {
          "duration": 0.02508,
          "end_time": "2023-11-22T13:35:29.162650",
          "exception": false,
          "start_time": "2023-11-22T13:35:29.137570",
          "status": "completed"
        },
        "tags": [],
        "id": "320e3320",
        "outputId": "f0f36f0d-e82b-484e-f40b-9291c4cf0a68"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'encoder = UBCOutlierModel(CONFIG[\\'model_name\\'], CONFIG[\\'num_classes\\'], pretrained=False , checkpoint_path=CONFIG[\"checkpoint_path\"])\\nencoder.load_state_dict(torch.load(save_model_path))\\nencoder.to(CONFIG[\\'device\\']);'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"encoder = UBCOutlierModel(CONFIG['model_name'], CONFIG['num_classes'], pretrained=False , checkpoint_path=CONFIG[\"checkpoint_path\"])\n",
        "encoder.load_state_dict(torch.load(save_model_path))\n",
        "encoder.to(CONFIG['device']);\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1ec1f0f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-22T13:35:29.194151Z",
          "iopub.status.busy": "2023-11-22T13:35:29.193736Z",
          "iopub.status.idle": "2023-11-22T13:35:29.201462Z",
          "shell.execute_reply": "2023-11-22T13:35:29.200589Z"
        },
        "papermill": {
          "duration": 0.025926,
          "end_time": "2023-11-22T13:35:29.203499",
          "exception": false,
          "start_time": "2023-11-22T13:35:29.177573",
          "status": "completed"
        },
        "tags": [],
        "id": "a1ec1f0f",
        "outputId": "1b1ad0cb-2892-4e36-98cd-8bf76baddf2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'from sklearn.svm import OneClassSVM\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.model_selection import GridSearchCV\\n\\n\\n# Extract features from the training dataset\\nsvm_train_loader, svm_valid_loader, _ = get_dataloaders(df_holdout.copy(), n_tiles=CONFIG[\"n_tiles\"])\\nsvm_test_dataset = CancerTilesDataset(df_anomaly.copy(), TRAIN_DIR, transforms=data_transforms[\"valid\"], mode=\"test\", split=1.0, n_tiles=CONFIG[\"n_tiles\"])\\nsvm_test_loader = DataLoader(svm_test_dataset, batch_size=CONFIG[\\'valid_batch_size\\'], \\n                          num_workers=2, shuffle=False, pin_memory=True)\\n        \\ntrain_features = extract_features(svm_train_loader, encoder, CONFIG[\"device\"])\\nvalid_features = extract_features(svm_valid_loader, encoder, CONFIG[\"device\"])\\ntest_features = extract_features(svm_test_loader, encoder, CONFIG[\"device\"])\\n\\nscaler = StandardScaler()\\ntrain_features_scaled = scaler.fit_transform(train_features)\\nvalid_features_scaled = scaler.transform(valid_features)\\ntest_features_scaled = scaler.transform(test_features)\\nprint(train_features_scaled.shape, valid_features_scaled.shape, test_features_scaled.shape)\\n\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.pipeline import Pipeline\\npca = PCA(n_components=0.95)\\ntrain_features_reduced = pca.fit_transform(train_features_scaled)\\nvalid_features_reduced = pca.transform(valid_features_scaled)\\ntest_features_reduced = pca.transform(test_features_scaled)'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"from sklearn.svm import OneClassSVM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "# Extract features from the training dataset\n",
        "svm_train_loader, svm_valid_loader, _ = get_dataloaders(df_holdout.copy(), n_tiles=CONFIG[\"n_tiles\"])\n",
        "svm_test_dataset = CancerTilesDataset(df_anomaly.copy(), TRAIN_DIR, transforms=data_transforms[\"valid\"], mode=\"test\", split=1.0, n_tiles=CONFIG[\"n_tiles\"])\n",
        "svm_test_loader = DataLoader(svm_test_dataset, batch_size=CONFIG['valid_batch_size'],\n",
        "                          num_workers=2, shuffle=False, pin_memory=True)\n",
        "\n",
        "train_features = extract_features(svm_train_loader, encoder, CONFIG[\"device\"])\n",
        "valid_features = extract_features(svm_valid_loader, encoder, CONFIG[\"device\"])\n",
        "test_features = extract_features(svm_test_loader, encoder, CONFIG[\"device\"])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train_features_scaled = scaler.fit_transform(train_features)\n",
        "valid_features_scaled = scaler.transform(valid_features)\n",
        "test_features_scaled = scaler.transform(test_features)\n",
        "print(train_features_scaled.shape, valid_features_scaled.shape, test_features_scaled.shape)\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "pca = PCA(n_components=0.95)\n",
        "train_features_reduced = pca.fit_transform(train_features_scaled)\n",
        "valid_features_reduced = pca.transform(valid_features_scaled)\n",
        "test_features_reduced = pca.transform(test_features_scaled)\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4705e733",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-22T13:35:29.235066Z",
          "iopub.status.busy": "2023-11-22T13:35:29.234677Z",
          "iopub.status.idle": "2023-11-22T13:35:29.242387Z",
          "shell.execute_reply": "2023-11-22T13:35:29.241524Z"
        },
        "papermill": {
          "duration": 0.025959,
          "end_time": "2023-11-22T13:35:29.244463",
          "exception": false,
          "start_time": "2023-11-22T13:35:29.218504",
          "status": "completed"
        },
        "tags": [],
        "id": "4705e733",
        "outputId": "23e0ce99-bf28-47c0-f4fa-cc97d22a2f05"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'# Train the one-class SVM\\nclf = OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=\"auto\")\\nclf.fit(train_features_reduced)\\ny_pred_train = clf.predict(train_features_reduced)\\ny_pred_valid = clf.predict(valid_features_reduced)\\ny_pred_outliers = clf.predict(test_features_reduced)\\nfrom numpy import quantile, where, random\\n\\ntrain_scores = clf.score_samples(train_features_reduced)\\nthresh = quantile(train_scores, 0.03)\\n\\nprint(thresh)\\n\\nn_error_train = y_pred_train[y_pred_train == -1].size\\nn_error_valid = y_pred_valid[y_pred_valid == -1].size\\nn_error_outliers = y_pred_outliers[y_pred_outliers == 1].size\\nprint(n_error_train/len(y_pred_train), n_error_valid, n_error_outliers)\\nprint(n_error_train/len(y_pred_train), n_error_valid/len(y_pred_valid), n_error_outliers/len(y_pred_outliers))'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the parameter grid\n",
        "\"\"\"param_grid = {\n",
        "    'gamma': ['scale', 'auto'] + list(np.logspace(-9, 3, 13)),\n",
        "    'nu': np.linspace(0.01, 0.5, 10)  # Assuming a smaller nu as we expect fewer outliers\n",
        "}\n",
        "\n",
        "# Create OneClassSVM object\n",
        "svm = OneClassSVM()\n",
        "\n",
        "# Grid search\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5, n_jobs=-1)\n",
        "grid_search.fit(train_features_scaled)\n",
        "\n",
        "# Best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best parameters:\", best_params)\n",
        "\n",
        "# Train the SVM with the best parameters\n",
        "optimized_svm = OneClassSVM(**best_params)\n",
        "optimized_svm.fit(train_features_scaled)\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"# Train the one-class SVM\n",
        "clf = OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=\"auto\")\n",
        "clf.fit(train_features_reduced)\n",
        "y_pred_train = clf.predict(train_features_reduced)\n",
        "y_pred_valid = clf.predict(valid_features_reduced)\n",
        "y_pred_outliers = clf.predict(test_features_reduced)\n",
        "from numpy import quantile, where, random\n",
        "\n",
        "train_scores = clf.score_samples(train_features_reduced)\n",
        "thresh = quantile(train_scores, 0.03)\n",
        "\n",
        "print(thresh)\n",
        "\n",
        "n_error_train = y_pred_train[y_pred_train == -1].size\n",
        "n_error_valid = y_pred_valid[y_pred_valid == -1].size\n",
        "n_error_outliers = y_pred_outliers[y_pred_outliers == 1].size\n",
        "print(n_error_train/len(y_pred_train), n_error_valid, n_error_outliers)\n",
        "print(n_error_train/len(y_pred_train), n_error_valid/len(y_pred_valid), n_error_outliers/len(y_pred_outliers))\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05818a58",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-22T13:14:41.812689Z",
          "iopub.status.busy": "2023-11-22T13:14:41.811972Z",
          "iopub.status.idle": "2023-11-22T13:14:42.251497Z",
          "shell.execute_reply": "2023-11-22T13:14:42.250552Z",
          "shell.execute_reply.started": "2023-11-22T13:14:41.812652Z"
        },
        "papermill": {
          "duration": 0.014807,
          "end_time": "2023-11-22T13:35:29.274486",
          "exception": false,
          "start_time": "2023-11-22T13:35:29.259679",
          "status": "completed"
        },
        "tags": [],
        "id": "05818a58"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83813bbf",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-22T13:10:11.369329Z",
          "iopub.status.busy": "2023-11-22T13:10:11.368640Z",
          "iopub.status.idle": "2023-11-22T13:10:11.375970Z",
          "shell.execute_reply": "2023-11-22T13:10:11.375057Z",
          "shell.execute_reply.started": "2023-11-22T13:10:11.369283Z"
        },
        "papermill": {
          "duration": 0.01462,
          "end_time": "2023-11-22T13:35:29.304201",
          "exception": false,
          "start_time": "2023-11-22T13:35:29.289581",
          "status": "completed"
        },
        "tags": [],
        "id": "83813bbf"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92ebe54d",
      "metadata": {
        "papermill": {
          "duration": 0.014789,
          "end_time": "2023-11-22T13:35:29.333988",
          "exception": false,
          "start_time": "2023-11-22T13:35:29.319199",
          "status": "completed"
        },
        "tags": [],
        "id": "92ebe54d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4be6dcb3",
      "metadata": {
        "papermill": {
          "duration": 0.014775,
          "end_time": "2023-11-22T13:35:29.363910",
          "exception": false,
          "start_time": "2023-11-22T13:35:29.349135",
          "status": "completed"
        },
        "tags": [],
        "id": "4be6dcb3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4237e02a",
      "metadata": {
        "papermill": {
          "duration": 0.0157,
          "end_time": "2023-11-22T13:35:29.394719",
          "exception": false,
          "start_time": "2023-11-22T13:35:29.379019",
          "status": "completed"
        },
        "tags": [],
        "id": "4237e02a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "databundleVersionId": 6924515,
          "sourceId": 45867,
          "sourceType": "competition"
        },
        {
          "datasetId": 3888697,
          "sourceId": 6755371,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 3889865,
          "sourceId": 6917177,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30559,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 1118.148838,
      "end_time": "2023-11-22T13:35:32.663386",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-11-22T13:16:54.514548",
      "version": "2.4.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}